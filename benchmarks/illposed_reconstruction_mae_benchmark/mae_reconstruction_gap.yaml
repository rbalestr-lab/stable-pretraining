seed: 42

model_name: vit_tiny
decoder_type: base-8b
mask_ratio: 0.75

alpha: 0.0

num_classes: 10

trainer:
  _target_: lightning.Trainer
  max_epochs: 100
  accelerator: auto
  devices: 1
  precision: 16-mixed
  num_sanity_val_steps: 0
  enable_checkpointing: true

  callbacks:
    - _target_: lightning.pytorch.callbacks.LearningRateMonitor
      logging_interval: step

    - _target_: lightning.pytorch.callbacks.ModelCheckpoint
      monitor: val/linear_probe/top1
      mode: max
      save_top_k: 1
      save_last: true
      filename: "epoch{epoch:03d}-acc{val/linear_probe/top1:.2f}"

    - _target_: stable_pretraining.callbacks.OnlineProbe
      name: linear_probe
      input: embedding
      target: label
      probe:
        _target_: torch.nn.Linear
        in_features: ${model_dims.${model_name}}
        out_features: ${num_classes}
      loss_fn:
        _target_: torch.nn.CrossEntropyLoss
      metrics:
        - _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${num_classes}
        - _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${num_classes}
          top_k: 5

    - _target_: stable_pretraining.callbacks.OnlineKNN
      name: knn_probe
      input: embedding
      target: label
      queue_length: 10000
      input_dim: ${model_dims.${model_name}}
      k: 10
      metrics:
        - _target_: torchmetrics.classification.MulticlassAccuracy
          num_classes: ${num_classes}

  logger:
    _target_: lightning.pytorch.loggers.WandbLogger
    entity: null
    project: mae-reconstruction-gap
    log_model: false
    name: mae-${model_name}-${decoder_type}-alpha${alpha}

model_dims:
  vit_tiny: 192
  vit_small: 384
  vit_base: 768
  vit_large: 1024

module:
  _target_: stable_pretraining.Module

  forward: benchmarks.imagenette.mae_vit_reconstruction_gap.mae_reconstruction_gap_forward

  backbone:
    _target_: benchmarks.imagenette.utils.create_mae_with_custom_decoder
    model_name: ${model_name}
    decoder_config:
      _target_: benchmarks.imagenette.utils.parse_decoder_type
      decoder_type_str: ${decoder_type}

  mask_ratio: ${mask_ratio}

  alpha: ${alpha}

  model_name: ${model_name}
  decoder_type: ${decoder_type}

  classifier:
    _target_: torch.nn.Linear
    in_features: ${model_dims.${model_name}}
    out_features: ${num_classes}

  optim:
    optimizer:
      _target_: torch.optim.AdamW
      _partial_: true
      lr: 1.5e-4
      betas: [0.9, 0.95]
      weight_decay: 0.05
    scheduler:
      _target_: stable_pretraining.optim.lr_scheduler.LinearWarmupCosineAnnealingLR
      _partial_: true
      warmup_steps: 3750
      max_steps: 37500
      min_lr: 1e-6
    interval: step

data:
  _target_: stable_pretraining.data.DataModule

  train:
    batch_size: 64
    num_workers: 4
    drop_last: true
    shuffle: false
    sampler:
      _target_: stable_pretraining.data.sampler.RepeatedRandomSampler
      n_views: 1
    dataset:
      _target_: stable_pretraining.data.HFDataset
      path: randall-lab/imagenette
      name: 320px
      split: train
      transform:
        _target_: stable_pretraining.data.transforms.Compose
        _args_:
          - _target_: stable_pretraining.data.transforms.RGB
          - _target_: stable_pretraining.data.transforms.RandomResizedCrop
            size: [224, 224]
            scale: [0.2, 1.0]
          - _target_: stable_pretraining.data.transforms.RandomHorizontalFlip
            p: 0.5
          - _target_: stable_pretraining.data.transforms.ToImage
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]

  val:
    batch_size: 64
    num_workers: 4
    dataset:
      _target_: stable_pretraining.data.HFDataset
      path: randall-lab/imagenette
      name: 320px
      split: validation
      transform:
        _target_: stable_pretraining.data.transforms.Compose
        _args_:
          - _target_: stable_pretraining.data.transforms.RGB
          - _target_: stable_pretraining.data.transforms.Resize
            size: [256, 256]
          - _target_: stable_pretraining.data.transforms.CenterCrop
            size: [224, 224]
          - _target_: stable_pretraining.data.transforms.ToImage
            mean: [0.485, 0.456, 0.406]
            std: [0.229, 0.224, 0.225]

hydra:
  run:
    dir: outputs/mae_gap/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: multirun/mae_gap/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: model=${model_name}_decoder=${decoder_type}_alpha=${alpha}
  job:
    chdir: false
