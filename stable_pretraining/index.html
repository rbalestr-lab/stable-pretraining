
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>stable_pretraining package &#8212; stable-pretraining 0.1.dev1+g148610e87.d20251117 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=07cf327b"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'stable_pretraining';</script>
    <link rel="index" title="Index" href="../genindex/" />
    <link rel="search" title="Search" href="../search/" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search/"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../">
  
  
  
  
  
  
    <p class="title logo__title">stable-pretraining 0.1.dev1+g148610e87.d20251117 documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../">stable-pretraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases/">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing/">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography/">Bibliography</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">API</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/backbone/">stable_pretraining.backbone</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.MLP/">MLP</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.Resnet9/">Resnet9</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.ConvMixer/">ConvMixer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.from_timm/">from_timm</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.from_torchvision/">from_torchvision</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.set_embedding_dim/">set_embedding_dim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.TeacherStudentWrapper/">TeacherStudentWrapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.EvalOnly/">EvalOnly</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.backbone.mae/">mae</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/data/">stable_pretraining.data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.DataModule/">DataModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Collator/">Collator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Dataset/">Dataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.FromTorchDataset/">FromTorchDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.HFDataset/">HFDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Subset/">Subset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.GMM/">GMM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.MinariStepsDataset/">MinariStepsDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.MinariEpisodeDataset/">MinariEpisodeDataset</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.swiss_roll/">swiss_roll</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.generate_perlin_noise_2d/">generate_perlin_noise_2d</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.perlin_noise_3d/">perlin_noise_3d</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.Categorical/">Categorical</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.ExponentialMixtureNoiseModel/">ExponentialMixtureNoiseModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.ExponentialNormalNoiseModel/">ExponentialNormalNoiseModel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.RepeatedRandomSampler/">RepeatedRandomSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.SupervisedBatchSampler/">SupervisedBatchSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.RandomBatchSampler/">RandomBatchSampler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.fold_views/">fold_views</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.random_split/">random_split</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.download/">download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.bulk_download/">bulk_download</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.transforms/">stable_pretraining.data.transforms</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.dataset_stats/">stable_pretraining.data.dataset_stats</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.data.synthetic_data/">stable_pretraining.data.synthetic_data</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../api/forward/">stable_pretraining.forward</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/losses/">stable_pretraining.losses</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.NTXEntLoss/">NTXEntLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.NegativeCosineSimilarity/">NegativeCosineSimilarity</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.VICRegLoss/">VICRegLoss</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.losses.BarlowTwinsLoss/">BarlowTwinsLoss</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/manager/">stable_pretraining.manager</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.manager.Manager/">Manager</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/module/">stable_pretraining.module</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.module.Module/">Module</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/monitors/">stable_pretraining.callbacks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineProbe/">OnlineProbe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineKNN/">OnlineKNN</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.OnlineWriter/">OnlineWriter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.RankMe/">RankMe</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.LiDAR/">LiDAR</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.EarlyStopping/">EarlyStopping</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.TrainerInfo/">TrainerInfo</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.LoggingCallback/">LoggingCallback</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.ModuleSummary/">ModuleSummary</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.SklearnCheckpoint/">SklearnCheckpoint</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.callbacks.ImageRetrieval/">ImageRetrieval</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../api/optim/">stable_pretraining.optim</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LARS/">LARS</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.CosineDecayer/">CosineDecayer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmup/">LinearWarmup</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupCosineAnnealing/">LinearWarmupCosineAnnealing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupCyclicAnnealing/">LinearWarmupCyclicAnnealing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api/gen_modules/stable_pretraining.optim.LinearWarmupThreeStepsAnnealing/">LinearWarmupThreeStepsAnnealing</a></li>
</ul>
</details></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/stable_pretraining.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>stable_pretraining package</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subpackages">Subpackages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submodules">Submodules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.cli">stable_pretraining.cli module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.cli.dump_csv_logs"><code class="docutils literal notranslate"><span class="pre">dump_csv_logs()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.cli.run"><code class="docutils literal notranslate"><span class="pre">run()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.config">stable_pretraining.config module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.collapse_nested_dict"><code class="docutils literal notranslate"><span class="pre">collapse_nested_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.instantiate_from_config"><code class="docutils literal notranslate"><span class="pre">instantiate_from_config()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.recursive_instantiate"><code class="docutils literal notranslate"><span class="pre">recursive_instantiate()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.forward">stable_pretraining.forward module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.barlow_twins_forward"><code class="docutils literal notranslate"><span class="pre">barlow_twins_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.byol_forward"><code class="docutils literal notranslate"><span class="pre">byol_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.dino_forward"><code class="docutils literal notranslate"><span class="pre">dino_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.dinov2_forward"><code class="docutils literal notranslate"><span class="pre">dinov2_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.nnclr_forward"><code class="docutils literal notranslate"><span class="pre">nnclr_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.simclr_forward"><code class="docutils literal notranslate"><span class="pre">simclr_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.supervised_forward"><code class="docutils literal notranslate"><span class="pre">supervised_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.swav_forward"><code class="docutils literal notranslate"><span class="pre">swav_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.vicreg_forward"><code class="docutils literal notranslate"><span class="pre">vicreg_forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.manager">stable_pretraining.manager module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager"><code class="docutils literal notranslate"><span class="pre">Manager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.init_and_sync_wandb"><code class="docutils literal notranslate"><span class="pre">Manager.init_and_sync_wandb()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.instantiated_data"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_data</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.instantiated_module"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_module</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.predict"><code class="docutils literal notranslate"><span class="pre">Manager.predict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Manager.save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.test"><code class="docutils literal notranslate"><span class="pre">Manager.test()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.validate"><code class="docutils literal notranslate"><span class="pre">Manager.validate()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.print_logger_info"><code class="docutils literal notranslate"><span class="pre">print_logger_info()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.print_signal_info"><code class="docutils literal notranslate"><span class="pre">print_signal_info()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.module">stable_pretraining.module module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.add_module"><code class="docutils literal notranslate"><span class="pre">Module.add_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.all_gather"><code class="docutils literal notranslate"><span class="pre">Module.all_gather()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.apply"><code class="docutils literal notranslate"><span class="pre">Module.apply()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.backward"><code class="docutils literal notranslate"><span class="pre">Module.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.bfloat16"><code class="docutils literal notranslate"><span class="pre">Module.bfloat16()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.buffers"><code class="docutils literal notranslate"><span class="pre">Module.buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.children"><code class="docutils literal notranslate"><span class="pre">Module.children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.clip_gradients"><code class="docutils literal notranslate"><span class="pre">Module.clip_gradients()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.compile"><code class="docutils literal notranslate"><span class="pre">Module.compile()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_callbacks"><code class="docutils literal notranslate"><span class="pre">Module.configure_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_gradient_clipping"><code class="docutils literal notranslate"><span class="pre">Module.configure_gradient_clipping()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">Module.configure_optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_sharded_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_sharded_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.cpu"><code class="docutils literal notranslate"><span class="pre">Module.cpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.cuda"><code class="docutils literal notranslate"><span class="pre">Module.cuda()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.double"><code class="docutils literal notranslate"><span class="pre">Module.double()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.eval"><code class="docutils literal notranslate"><span class="pre">Module.eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.extra_repr"><code class="docutils literal notranslate"><span class="pre">Module.extra_repr()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.float"><code class="docutils literal notranslate"><span class="pre">Module.float()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.freeze"><code class="docutils literal notranslate"><span class="pre">Module.freeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_buffer"><code class="docutils literal notranslate"><span class="pre">Module.get_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.get_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_parameter"><code class="docutils literal notranslate"><span class="pre">Module.get_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_submodule"><code class="docutils literal notranslate"><span class="pre">Module.get_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.half"><code class="docutils literal notranslate"><span class="pre">Module.half()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.ipu"><code class="docutils literal notranslate"><span class="pre">Module.ipu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.load_from_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.load_from_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.load_state_dict"><code class="docutils literal notranslate"><span class="pre">Module.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.log"><code class="docutils literal notranslate"><span class="pre">Module.log()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.log_dict"><code class="docutils literal notranslate"><span class="pre">Module.log_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.lr_scheduler_step"><code class="docutils literal notranslate"><span class="pre">Module.lr_scheduler_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.lr_schedulers"><code class="docutils literal notranslate"><span class="pre">Module.lr_schedulers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.manual_backward"><code class="docutils literal notranslate"><span class="pre">Module.manual_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.modules"><code class="docutils literal notranslate"><span class="pre">Module.modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.mtia"><code class="docutils literal notranslate"><span class="pre">Module.mtia()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_buffers"><code class="docutils literal notranslate"><span class="pre">Module.named_buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_children"><code class="docutils literal notranslate"><span class="pre">Module.named_children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_modules"><code class="docutils literal notranslate"><span class="pre">Module.named_modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_parameters"><code class="docutils literal notranslate"><span class="pre">Module.named_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_after_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_after_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_after_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_before_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_before_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.on_before_optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_before_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_fit_end"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_fit_start"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizer_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizers"><code class="docutils literal notranslate"><span class="pre">Module.optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.parameters"><code class="docutils literal notranslate"><span class="pre">Module.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.predict_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.predict_step"><code class="docutils literal notranslate"><span class="pre">Module.predict_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.prepare_data"><code class="docutils literal notranslate"><span class="pre">Module.prepare_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.print"><code class="docutils literal notranslate"><span class="pre">Module.print()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_buffer"><code class="docutils literal notranslate"><span class="pre">Module.register_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_forward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_forward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_full_backward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_load_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_load_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_module"><code class="docutils literal notranslate"><span class="pre">Module.register_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_parameter"><code class="docutils literal notranslate"><span class="pre">Module.register_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.requires_grad_"><code class="docutils literal notranslate"><span class="pre">Module.requires_grad_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.rescale_loss_for_grad_acc"><code class="docutils literal notranslate"><span class="pre">Module.rescale_loss_for_grad_acc()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.save_hyperparameters"><code class="docutils literal notranslate"><span class="pre">Module.save_hyperparameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.set_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.set_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.set_submodule"><code class="docutils literal notranslate"><span class="pre">Module.set_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.setup"><code class="docutils literal notranslate"><span class="pre">Module.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.share_memory"><code class="docutils literal notranslate"><span class="pre">Module.share_memory()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.state_dict"><code class="docutils literal notranslate"><span class="pre">Module.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.teardown"><code class="docutils literal notranslate"><span class="pre">Module.teardown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.test_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.test_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.test_step"><code class="docutils literal notranslate"><span class="pre">Module.test_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to"><code class="docutils literal notranslate"><span class="pre">Module.to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_empty"><code class="docutils literal notranslate"><span class="pre">Module.to_empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_onnx"><code class="docutils literal notranslate"><span class="pre">Module.to_onnx()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_torchscript"><code class="docutils literal notranslate"><span class="pre">Module.to_torchscript()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.toggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.toggled_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggled_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.train"><code class="docutils literal notranslate"><span class="pre">Module.train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.train_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.train_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.training_step"><code class="docutils literal notranslate"><span class="pre">Module.training_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.transfer_batch_to_device"><code class="docutils literal notranslate"><span class="pre">Module.transfer_batch_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.type"><code class="docutils literal notranslate"><span class="pre">Module.type()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.unfreeze"><code class="docutils literal notranslate"><span class="pre">Module.unfreeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.untoggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.untoggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.val_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.val_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.validation_step"><code class="docutils literal notranslate"><span class="pre">Module.validation_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.xpu"><code class="docutils literal notranslate"><span class="pre">Module.xpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.run">stable_pretraining.run module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.run.main"><code class="docutils literal notranslate"><span class="pre">main()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.run.print_config"><code class="docutils literal notranslate"><span class="pre">print_config()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.static">stable_pretraining.static module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic"><code class="docutils literal notranslate"><span class="pre">MetaStatic</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.clear"><code class="docutils literal notranslate"><span class="pre">MetaStatic.clear()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.get"><code class="docutils literal notranslate"><span class="pre">MetaStatic.get()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.items"><code class="docutils literal notranslate"><span class="pre">MetaStatic.items()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.keys"><code class="docutils literal notranslate"><span class="pre">MetaStatic.keys()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.update"><code class="docutils literal notranslate"><span class="pre">MetaStatic.update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.values"><code class="docutils literal notranslate"><span class="pre">MetaStatic.values()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_EMBEDDINGS"><code class="docutils literal notranslate"><span class="pre">TIMM_EMBEDDINGS</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_EMBEDDINGS.data"><code class="docutils literal notranslate"><span class="pre">TIMM_EMBEDDINGS.data</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_PARAMETERS"><code class="docutils literal notranslate"><span class="pre">TIMM_PARAMETERS</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_PARAMETERS.data"><code class="docutils literal notranslate"><span class="pre">TIMM_PARAMETERS.data</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.count_parameters"><code class="docutils literal notranslate"><span class="pre">count_parameters()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining">Module contents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.EarlyStopping"><code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.EarlyStopping.should_stop"><code class="docutils literal notranslate"><span class="pre">EarlyStopping.should_stop()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.NAME"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.NAME</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.state_key"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR"><code class="docutils literal notranslate"><span class="pre">LiDAR</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">LiDAR.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.setup"><code class="docutils literal notranslate"><span class="pre">LiDAR.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.state_key"><code class="docutils literal notranslate"><span class="pre">LiDAR.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LoggingCallback"><code class="docutils literal notranslate"><span class="pre">LoggingCallback</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LoggingCallback.on_validation_end"><code class="docutils literal notranslate"><span class="pre">LoggingCallback.on_validation_end()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager"><code class="docutils literal notranslate"><span class="pre">Manager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.init_and_sync_wandb"><code class="docutils literal notranslate"><span class="pre">Manager.init_and_sync_wandb()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.instantiated_data"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_data</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.instantiated_module"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_module</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.predict"><code class="docutils literal notranslate"><span class="pre">Manager.predict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Manager.save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.test"><code class="docutils literal notranslate"><span class="pre">Manager.test()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.validate"><code class="docutils literal notranslate"><span class="pre">Manager.validate()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.add_module"><code class="docutils literal notranslate"><span class="pre">Module.add_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.all_gather"><code class="docutils literal notranslate"><span class="pre">Module.all_gather()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.apply"><code class="docutils literal notranslate"><span class="pre">Module.apply()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.backward"><code class="docutils literal notranslate"><span class="pre">Module.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.bfloat16"><code class="docutils literal notranslate"><span class="pre">Module.bfloat16()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.buffers"><code class="docutils literal notranslate"><span class="pre">Module.buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.children"><code class="docutils literal notranslate"><span class="pre">Module.children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.clip_gradients"><code class="docutils literal notranslate"><span class="pre">Module.clip_gradients()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.compile"><code class="docutils literal notranslate"><span class="pre">Module.compile()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_callbacks"><code class="docutils literal notranslate"><span class="pre">Module.configure_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_gradient_clipping"><code class="docutils literal notranslate"><span class="pre">Module.configure_gradient_clipping()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">Module.configure_optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_sharded_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_sharded_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.cpu"><code class="docutils literal notranslate"><span class="pre">Module.cpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.cuda"><code class="docutils literal notranslate"><span class="pre">Module.cuda()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.double"><code class="docutils literal notranslate"><span class="pre">Module.double()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.eval"><code class="docutils literal notranslate"><span class="pre">Module.eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.extra_repr"><code class="docutils literal notranslate"><span class="pre">Module.extra_repr()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.float"><code class="docutils literal notranslate"><span class="pre">Module.float()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.freeze"><code class="docutils literal notranslate"><span class="pre">Module.freeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_buffer"><code class="docutils literal notranslate"><span class="pre">Module.get_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.get_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_parameter"><code class="docutils literal notranslate"><span class="pre">Module.get_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_submodule"><code class="docutils literal notranslate"><span class="pre">Module.get_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.half"><code class="docutils literal notranslate"><span class="pre">Module.half()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.ipu"><code class="docutils literal notranslate"><span class="pre">Module.ipu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.load_from_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.load_from_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.load_state_dict"><code class="docutils literal notranslate"><span class="pre">Module.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.log"><code class="docutils literal notranslate"><span class="pre">Module.log()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.log_dict"><code class="docutils literal notranslate"><span class="pre">Module.log_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.lr_scheduler_step"><code class="docutils literal notranslate"><span class="pre">Module.lr_scheduler_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.lr_schedulers"><code class="docutils literal notranslate"><span class="pre">Module.lr_schedulers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.manual_backward"><code class="docutils literal notranslate"><span class="pre">Module.manual_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.modules"><code class="docutils literal notranslate"><span class="pre">Module.modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.mtia"><code class="docutils literal notranslate"><span class="pre">Module.mtia()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_buffers"><code class="docutils literal notranslate"><span class="pre">Module.named_buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_children"><code class="docutils literal notranslate"><span class="pre">Module.named_children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_modules"><code class="docutils literal notranslate"><span class="pre">Module.named_modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_parameters"><code class="docutils literal notranslate"><span class="pre">Module.named_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_after_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_after_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_after_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_before_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_before_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.on_before_optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_before_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_fit_end"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_fit_start"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizer_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizers"><code class="docutils literal notranslate"><span class="pre">Module.optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.parameters"><code class="docutils literal notranslate"><span class="pre">Module.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.predict_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.predict_step"><code class="docutils literal notranslate"><span class="pre">Module.predict_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.prepare_data"><code class="docutils literal notranslate"><span class="pre">Module.prepare_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.print"><code class="docutils literal notranslate"><span class="pre">Module.print()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_buffer"><code class="docutils literal notranslate"><span class="pre">Module.register_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_forward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_forward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_full_backward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_load_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_load_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_module"><code class="docutils literal notranslate"><span class="pre">Module.register_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_parameter"><code class="docutils literal notranslate"><span class="pre">Module.register_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.requires_grad_"><code class="docutils literal notranslate"><span class="pre">Module.requires_grad_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.rescale_loss_for_grad_acc"><code class="docutils literal notranslate"><span class="pre">Module.rescale_loss_for_grad_acc()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.save_hyperparameters"><code class="docutils literal notranslate"><span class="pre">Module.save_hyperparameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.set_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.set_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.set_submodule"><code class="docutils literal notranslate"><span class="pre">Module.set_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.setup"><code class="docutils literal notranslate"><span class="pre">Module.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.share_memory"><code class="docutils literal notranslate"><span class="pre">Module.share_memory()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.state_dict"><code class="docutils literal notranslate"><span class="pre">Module.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.teardown"><code class="docutils literal notranslate"><span class="pre">Module.teardown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.test_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.test_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.test_step"><code class="docutils literal notranslate"><span class="pre">Module.test_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to"><code class="docutils literal notranslate"><span class="pre">Module.to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_empty"><code class="docutils literal notranslate"><span class="pre">Module.to_empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_onnx"><code class="docutils literal notranslate"><span class="pre">Module.to_onnx()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_torchscript"><code class="docutils literal notranslate"><span class="pre">Module.to_torchscript()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.toggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.toggled_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggled_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.train"><code class="docutils literal notranslate"><span class="pre">Module.train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.train_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.train_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.training_step"><code class="docutils literal notranslate"><span class="pre">Module.training_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.transfer_batch_to_device"><code class="docutils literal notranslate"><span class="pre">Module.transfer_batch_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.type"><code class="docutils literal notranslate"><span class="pre">Module.type()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.unfreeze"><code class="docutils literal notranslate"><span class="pre">Module.unfreeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.untoggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.untoggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.val_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.val_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.validation_step"><code class="docutils literal notranslate"><span class="pre">Module.validation_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.xpu"><code class="docutils literal notranslate"><span class="pre">Module.xpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ModuleSummary"><code class="docutils literal notranslate"><span class="pre">ModuleSummary</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ModuleSummary.setup"><code class="docutils literal notranslate"><span class="pre">ModuleSummary.setup()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN"><code class="docutils literal notranslate"><span class="pre">OnlineKNN</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.setup"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.state_key"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe"><code class="docutils literal notranslate"><span class="pre">OnlineProbe</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe.configure_model"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe.wrap_forward"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.wrap_forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter"><code class="docutils literal notranslate"><span class="pre">OnlineWriter</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.dump"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.dump()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.is_writing_epoch"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.is_writing_epoch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_sanity_check_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_sanity_check_start"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.write_at_phase"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.write_at_phase()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe"><code class="docutils literal notranslate"><span class="pre">RankMe</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">RankMe.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.setup"><code class="docutils literal notranslate"><span class="pre">RankMe.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.state_key"><code class="docutils literal notranslate"><span class="pre">RankMe.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.setup"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.setup()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_after_backward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_fit_start"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_train_batch_end()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward_student"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_student()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.update_ema_coefficient"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_ema_coefficient()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.update_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_teacher()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TrainerInfo"><code class="docutils literal notranslate"><span class="pre">TrainerInfo</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TrainerInfo.setup"><code class="docutils literal notranslate"><span class="pre">TrainerInfo.setup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="stable-pretraining-package">
<h1>stable_pretraining package<a class="headerlink" href="#stable-pretraining-package" title="Link to this heading">#</a></h1>
<section id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Link to this heading">#</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.backbone/">stable_pretraining.backbone package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.aggregator">stable_pretraining.backbone.aggregator module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.aggregator.TensorAggregator"><code class="docutils literal notranslate"><span class="pre">TensorAggregator</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.aggregator.TensorAggregator.compute_output_dim"><code class="docutils literal notranslate"><span class="pre">TensorAggregator.compute_output_dim()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.aggregator.TensorAggregator.forward"><code class="docutils literal notranslate"><span class="pre">TensorAggregator.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.convmixer">stable_pretraining.backbone.convmixer module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.convmixer.ConvMixer"><code class="docutils literal notranslate"><span class="pre">ConvMixer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.convmixer.ConvMixer.forward"><code class="docutils literal notranslate"><span class="pre">ConvMixer.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.mae">stable_pretraining.backbone.mae module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.forward"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.forward_decoder"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.forward_decoder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.forward_encoder"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.forward_encoder()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.initialize_weights"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.initialize_weights()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.patchify"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.patchify()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.random_masking"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.random_masking()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.MaskedAutoencoderViT.unpatchify"><code class="docutils literal notranslate"><span class="pre">MaskedAutoencoderViT.unpatchify()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.get_1d_sincos_pos_embed_from_grid"><code class="docutils literal notranslate"><span class="pre">get_1d_sincos_pos_embed_from_grid()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.get_2d_sincos_pos_embed"><code class="docutils literal notranslate"><span class="pre">get_2d_sincos_pos_embed()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.get_2d_sincos_pos_embed_from_grid"><code class="docutils literal notranslate"><span class="pre">get_2d_sincos_pos_embed_from_grid()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_base_patch16"><code class="docutils literal notranslate"><span class="pre">vit_base_patch16()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_base_patch16_dec512d8b"><code class="docutils literal notranslate"><span class="pre">vit_base_patch16_dec512d8b()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_huge_patch14"><code class="docutils literal notranslate"><span class="pre">vit_huge_patch14()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_huge_patch14_dec512d8b"><code class="docutils literal notranslate"><span class="pre">vit_huge_patch14_dec512d8b()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_large_patch16"><code class="docutils literal notranslate"><span class="pre">vit_large_patch16()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mae.vit_large_patch16_dec512d8b"><code class="docutils literal notranslate"><span class="pre">vit_large_patch16_dec512d8b()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.mlp">stable_pretraining.backbone.mlp module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.mlp.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.probe">stable_pretraining.backbone.probe module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoLinearClassifier"><code class="docutils literal notranslate"><span class="pre">AutoLinearClassifier</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoLinearClassifier.norm"><code class="docutils literal notranslate"><span class="pre">AutoLinearClassifier.norm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoLinearClassifier.fc"><code class="docutils literal notranslate"><span class="pre">AutoLinearClassifier.fc</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoLinearClassifier.forward"><code class="docutils literal notranslate"><span class="pre">AutoLinearClassifier.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP.forward"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP.get_best_variant"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP.get_best_variant()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP.get_variant"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP.get_variant()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP.keys"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP.keys()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.AutoTuneMLP.num_variants"><code class="docutils literal notranslate"><span class="pre">AutoTuneMLP.num_variants()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.LinearProbe"><code class="docutils literal notranslate"><span class="pre">LinearProbe</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.LinearProbe.norm"><code class="docutils literal notranslate"><span class="pre">LinearProbe.norm</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.LinearProbe.fc"><code class="docutils literal notranslate"><span class="pre">LinearProbe.fc</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.LinearProbe.forward"><code class="docutils literal notranslate"><span class="pre">LinearProbe.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.MultiHeadAttentiveProbe"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttentiveProbe</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.MultiHeadAttentiveProbe.ln"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttentiveProbe.ln</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.MultiHeadAttentiveProbe.attn_vectors"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttentiveProbe.attn_vectors</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.MultiHeadAttentiveProbe.fc"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttentiveProbe.fc</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.probe.MultiHeadAttentiveProbe.forward"><code class="docutils literal notranslate"><span class="pre">MultiHeadAttentiveProbe.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.resnet9">stable_pretraining.backbone.resnet9 module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.resnet9.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.resnet9.ResidualBlock"><code class="docutils literal notranslate"><span class="pre">ResidualBlock</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.resnet9.ResidualBlock.forward"><code class="docutils literal notranslate"><span class="pre">ResidualBlock.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.resnet9.Resnet9"><code class="docutils literal notranslate"><span class="pre">Resnet9</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.resnet9.Resnet9.forward"><code class="docutils literal notranslate"><span class="pre">Resnet9.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone.utils">stable_pretraining.backbone.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EfficientMaskedTimmViT"><code class="docutils literal notranslate"><span class="pre">EfficientMaskedTimmViT</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EfficientMaskedTimmViT.clear_cache"><code class="docutils literal notranslate"><span class="pre">EfficientMaskedTimmViT.clear_cache()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EfficientMaskedTimmViT.forward"><code class="docutils literal notranslate"><span class="pre">EfficientMaskedTimmViT.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EvalOnly"><code class="docutils literal notranslate"><span class="pre">EvalOnly</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EvalOnly.forward"><code class="docutils literal notranslate"><span class="pre">EvalOnly.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.EvalOnly.train"><code class="docutils literal notranslate"><span class="pre">EvalOnly.train()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.FeaturesConcat"><code class="docutils literal notranslate"><span class="pre">FeaturesConcat</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.FeaturesConcat.forward"><code class="docutils literal notranslate"><span class="pre">FeaturesConcat.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.FeaturesConcat.get_output_shape"><code class="docutils literal notranslate"><span class="pre">FeaturesConcat.get_output_shape()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.ReturnEmbedding"><code class="docutils literal notranslate"><span class="pre">ReturnEmbedding</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.ReturnEmbedding.forward"><code class="docutils literal notranslate"><span class="pre">ReturnEmbedding.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper.forward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper.forward_student"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_student()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper.forward_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_teacher()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper.update_ema_coefficient"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_ema_coefficient()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.TeacherStudentWrapper.update_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_teacher()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.from_huggingface"><code class="docutils literal notranslate"><span class="pre">from_huggingface()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.from_timm"><code class="docutils literal notranslate"><span class="pre">from_timm()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.from_torchvision"><code class="docutils literal notranslate"><span class="pre">from_torchvision()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.get_children_modules"><code class="docutils literal notranslate"><span class="pre">get_children_modules()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.get_output_shape"><code class="docutils literal notranslate"><span class="pre">get_output_shape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.register_lr_scale_hook"><code class="docutils literal notranslate"><span class="pre">register_lr_scale_hook()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.set_embedding_dim"><code class="docutils literal notranslate"><span class="pre">set_embedding_dim()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.backbone/#stable_pretraining.backbone.utils.vit_hf"><code class="docutils literal notranslate"><span class="pre">vit_hf()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.backbone/#module-stable_pretraining.backbone">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.callbacks/">stable_pretraining.callbacks package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.checkpoint_sklearn">stable_pretraining.callbacks.checkpoint_sklearn module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.SklearnCheckpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.SklearnCheckpoint.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_load_checkpoint()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.SklearnCheckpoint.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_save_checkpoint()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.SklearnCheckpoint.setup"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.setup()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.StrictCheckpointCallback"><code class="docutils literal notranslate"><span class="pre">StrictCheckpointCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.StrictCheckpointCallback.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">StrictCheckpointCallback.on_load_checkpoint()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.WandbCheckpoint"><code class="docutils literal notranslate"><span class="pre">WandbCheckpoint</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.WandbCheckpoint.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">WandbCheckpoint.on_load_checkpoint()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.WandbCheckpoint.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">WandbCheckpoint.on_save_checkpoint()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.checkpoint_sklearn.WandbCheckpoint.setup"><code class="docutils literal notranslate"><span class="pre">WandbCheckpoint.setup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.cleanup">stable_pretraining.callbacks.cleanup module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cleanup.CleanUpCallback"><code class="docutils literal notranslate"><span class="pre">CleanUpCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cleanup.CleanUpCallback.on_exception"><code class="docutils literal notranslate"><span class="pre">CleanUpCallback.on_exception()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cleanup.CleanUpCallback.on_fit_end"><code class="docutils literal notranslate"><span class="pre">CleanUpCallback.on_fit_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cleanup.CleanUpCallback.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">CleanUpCallback.on_train_epoch_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cleanup.human_size"><code class="docutils literal notranslate"><span class="pre">human_size()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.clip_zero_shot">stable_pretraining.callbacks.clip_zero_shot module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.clip_zero_shot.CLIPZeroShot"><code class="docutils literal notranslate"><span class="pre">CLIPZeroShot</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.clip_zero_shot.CLIPZeroShot.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">CLIPZeroShot.on_validation_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.clip_zero_shot.CLIPZeroShot.setup"><code class="docutils literal notranslate"><span class="pre">CLIPZeroShot.setup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.cpu_offload">stable_pretraining.callbacks.cpu_offload module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.load_state_dict"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.load_state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.on_exception"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.on_exception()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.on_save_checkpoint()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.on_train_end"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.on_train_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.on_train_start"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.on_train_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.setup"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.state_dict"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.cpu_offload.CPUOffloadCallback.teardown"><code class="docutils literal notranslate"><span class="pre">CPUOffloadCallback.teardown()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.earlystop">stable_pretraining.callbacks.earlystop module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.earlystop.EpochMilestones"><code class="docutils literal notranslate"><span class="pre">EpochMilestones</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.earlystop.EpochMilestones.on_training_epoch_end"><code class="docutils literal notranslate"><span class="pre">EpochMilestones.on_training_epoch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.earlystop.EpochMilestones.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">EpochMilestones.on_validation_epoch_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.earlystop.to_scalar"><code class="docutils literal notranslate"><span class="pre">to_scalar()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.embedding_cache">stable_pretraining.callbacks.embedding_cache module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.forward_hook_fn"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.forward_hook_fn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.on_test_batch_start"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.on_test_batch_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.on_train_batch_start"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.on_train_batch_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.on_validation_batch_start"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.on_validation_batch_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.setup"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.embedding_cache.EmbeddingCache.teardown"><code class="docutils literal notranslate"><span class="pre">EmbeddingCache.teardown()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.env_info">stable_pretraining.callbacks.env_info module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.env_info.EnvironmentDumpCallback"><code class="docutils literal notranslate"><span class="pre">EnvironmentDumpCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.env_info.EnvironmentDumpCallback.setup"><code class="docutils literal notranslate"><span class="pre">EnvironmentDumpCallback.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.env_info.EnvironmentDumpCallback.teardown"><code class="docutils literal notranslate"><span class="pre">EnvironmentDumpCallback.teardown()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.factories">stable_pretraining.callbacks.factories module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.factories.default"><code class="docutils literal notranslate"><span class="pre">default()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.image_retrieval">stable_pretraining.callbacks.image_retrieval module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.ImageRetrieval"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.ImageRetrieval.NAME"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.NAME</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.ImageRetrieval.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.ImageRetrieval.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.ImageRetrieval.state_key"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.state_key</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.image_retrieval.wrap_validation_step"><code class="docutils literal notranslate"><span class="pre">wrap_validation_step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.knn">stable_pretraining.callbacks.knn module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.knn.OnlineKNN"><code class="docutils literal notranslate"><span class="pre">OnlineKNN</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.knn.OnlineKNN.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.on_validation_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.knn.OnlineKNN.setup"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.knn.OnlineKNN.state_key"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.state_key</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.latent_viz">stable_pretraining.callbacks.latent_viz module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz"><code class="docutils literal notranslate"><span class="pre">LatentViz</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">LatentViz.on_train_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">LatentViz.on_validation_epoch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz.projection_module"><code class="docutils literal notranslate"><span class="pre">LatentViz.projection_module</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz.setup"><code class="docutils literal notranslate"><span class="pre">LatentViz.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.latent_viz.LatentViz.setup_optimizer"><code class="docutils literal notranslate"><span class="pre">LatentViz.setup_optimizer()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.lidar">stable_pretraining.callbacks.lidar module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.lidar.LiDAR"><code class="docutils literal notranslate"><span class="pre">LiDAR</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.lidar.LiDAR.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">LiDAR.on_validation_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.lidar.LiDAR.setup"><code class="docutils literal notranslate"><span class="pre">LiDAR.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.lidar.LiDAR.state_key"><code class="docutils literal notranslate"><span class="pre">LiDAR.state_key</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.probe">stable_pretraining.callbacks.probe module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.probe.OnlineProbe"><code class="docutils literal notranslate"><span class="pre">OnlineProbe</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.probe.OnlineProbe.configure_model"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.configure_model()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.probe.OnlineProbe.wrap_forward"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.wrap_forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.queue">stable_pretraining.callbacks.queue module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue"><code class="docutils literal notranslate"><span class="pre">OnlineQueue</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.data"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.data</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.actual_queue_length"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.actual_queue_length</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#id0"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.actual_queue_length</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#id21"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.data</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.on_train_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.on_validation_epoch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.on_validation_epoch_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.OnlineQueue.setup"><code class="docutils literal notranslate"><span class="pre">OnlineQueue.setup()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.queue.find_or_create_queue_callback"><code class="docutils literal notranslate"><span class="pre">find_or_create_queue_callback()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.rankme">stable_pretraining.callbacks.rankme module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.rankme.RankMe"><code class="docutils literal notranslate"><span class="pre">RankMe</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.rankme.RankMe.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">RankMe.on_validation_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.rankme.RankMe.setup"><code class="docutils literal notranslate"><span class="pre">RankMe.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.rankme.RankMe.state_key"><code class="docutils literal notranslate"><span class="pre">RankMe.state_key</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.teacher_student">stable_pretraining.callbacks.teacher_student module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.teacher_student.TeacherStudentCallback"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.teacher_student.TeacherStudentCallback.on_after_backward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_after_backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.teacher_student.TeacherStudentCallback.on_fit_start"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_fit_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.teacher_student.TeacherStudentCallback.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_train_batch_end()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.trainer_info">stable_pretraining.callbacks.trainer_info module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.LoggingCallback"><code class="docutils literal notranslate"><span class="pre">LoggingCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.LoggingCallback.on_validation_end"><code class="docutils literal notranslate"><span class="pre">LoggingCallback.on_validation_end()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.ModuleSummary"><code class="docutils literal notranslate"><span class="pre">ModuleSummary</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.ModuleSummary.setup"><code class="docutils literal notranslate"><span class="pre">ModuleSummary.setup()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.SLURMInfo"><code class="docutils literal notranslate"><span class="pre">SLURMInfo</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.SLURMInfo.setup"><code class="docutils literal notranslate"><span class="pre">SLURMInfo.setup()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.TrainerInfo"><code class="docutils literal notranslate"><span class="pre">TrainerInfo</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.trainer_info.TrainerInfo.setup"><code class="docutils literal notranslate"><span class="pre">TrainerInfo.setup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.utils">stable_pretraining.callbacks.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.EarlyStopping"><code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.EarlyStopping.should_stop"><code class="docutils literal notranslate"><span class="pre">EarlyStopping.should_stop()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback"><code class="docutils literal notranslate"><span class="pre">TrainableCallback</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.configure_model"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.configure_model()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.module"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.module</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.setup_optimizer"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.setup_optimizer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.setup_scheduler"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.setup_scheduler()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.state_key"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.state_key</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.wrap_configure_model"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.wrap_configure_model()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback.wrap_configure_optimizers"><code class="docutils literal notranslate"><span class="pre">TrainableCallback.wrap_configure_optimizers()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.format_metrics_as_dict"><code class="docutils literal notranslate"><span class="pre">format_metrics_as_dict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.wd_schedule">stable_pretraining.callbacks.wd_schedule module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.wd_schedule.WeightDecayUpdater"><code class="docutils literal notranslate"><span class="pre">WeightDecayUpdater</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.wd_schedule.WeightDecayUpdater.load_state_dict"><code class="docutils literal notranslate"><span class="pre">WeightDecayUpdater.load_state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.wd_schedule.WeightDecayUpdater.on_before_optimizer_step"><code class="docutils literal notranslate"><span class="pre">WeightDecayUpdater.on_before_optimizer_step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.wd_schedule.WeightDecayUpdater.on_fit_start"><code class="docutils literal notranslate"><span class="pre">WeightDecayUpdater.on_fit_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.wd_schedule.WeightDecayUpdater.state_dict"><code class="docutils literal notranslate"><span class="pre">WeightDecayUpdater.state_dict()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks.writer">stable_pretraining.callbacks.writer module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter"><code class="docutils literal notranslate"><span class="pre">OnlineWriter</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.dump"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.dump()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.is_writing_epoch"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.is_writing_epoch()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_predict_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_sanity_check_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_sanity_check_start"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_start()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_test_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_train_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_validation_batch_end()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.writer.OnlineWriter.write_at_phase"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.write_at_phase()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.callbacks/#module-stable_pretraining.callbacks">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.data/">stable_pretraining.data package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.collate">stable_pretraining.data.collate module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.collate.Collator"><code class="docutils literal notranslate"><span class="pre">Collator</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.dataset_stats">stable_pretraining.data.dataset_stats module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.datasets">stable_pretraining.data.datasets module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Dataset"><code class="docutils literal notranslate"><span class="pre">Dataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Dataset.process_sample"><code class="docutils literal notranslate"><span class="pre">Dataset.process_sample()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Dataset.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">Dataset.set_pl_trainer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.FromTorchDataset"><code class="docutils literal notranslate"><span class="pre">FromTorchDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.FromTorchDataset.column_names"><code class="docutils literal notranslate"><span class="pre">FromTorchDataset.column_names</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.HFDataset"><code class="docutils literal notranslate"><span class="pre">HFDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.HFDataset.column_names"><code class="docutils literal notranslate"><span class="pre">HFDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.HFDataset.is_saved_with_save_to_disk"><code class="docutils literal notranslate"><span class="pre">HFDataset.is_saved_with_save_to_disk()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Subset"><code class="docutils literal notranslate"><span class="pre">Subset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Subset.column_names"><code class="docutils literal notranslate"><span class="pre">Subset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Subset.dataset"><code class="docutils literal notranslate"><span class="pre">Subset.dataset</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.datasets.Subset.indices"><code class="docutils literal notranslate"><span class="pre">Subset.indices</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.download">stable_pretraining.data.download module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.download.bulk_download"><code class="docutils literal notranslate"><span class="pre">bulk_download()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.download.download"><code class="docutils literal notranslate"><span class="pre">download()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.masking">stable_pretraining.data.masking module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.masking.multi_block_mask"><code class="docutils literal notranslate"><span class="pre">multi_block_mask()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.module">stable_pretraining.data.module module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule"><code class="docutils literal notranslate"><span class="pre">DataModule</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.load_state_dict"><code class="docutils literal notranslate"><span class="pre">DataModule.load_state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.predict_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">DataModule.set_pl_trainer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.setup"><code class="docutils literal notranslate"><span class="pre">DataModule.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.state_dict"><code class="docutils literal notranslate"><span class="pre">DataModule.state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.teardown"><code class="docutils literal notranslate"><span class="pre">DataModule.teardown()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.test_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.test_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.train_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.train_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DataModule.val_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.val_dataloader()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.module.DictFormat"><code class="docutils literal notranslate"><span class="pre">DictFormat</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.sampler">stable_pretraining.data.sampler module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.sampler.RandomBatchSampler"><code class="docutils literal notranslate"><span class="pre">RandomBatchSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.sampler.RepeatedRandomSampler"><code class="docutils literal notranslate"><span class="pre">RepeatedRandomSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.sampler.SupervisedBatchSampler"><code class="docutils literal notranslate"><span class="pre">SupervisedBatchSampler</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.synthetic_data">stable_pretraining.data.synthetic_data module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.Categorical"><code class="docutils literal notranslate"><span class="pre">Categorical</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.Categorical.sample"><code class="docutils literal notranslate"><span class="pre">Categorical.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.ExponentialMixtureNoiseModel"><code class="docutils literal notranslate"><span class="pre">ExponentialMixtureNoiseModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.ExponentialMixtureNoiseModel.sample"><code class="docutils literal notranslate"><span class="pre">ExponentialMixtureNoiseModel.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.ExponentialNormalNoiseModel"><code class="docutils literal notranslate"><span class="pre">ExponentialNormalNoiseModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.ExponentialNormalNoiseModel.sample"><code class="docutils literal notranslate"><span class="pre">ExponentialNormalNoiseModel.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.GMM"><code class="docutils literal notranslate"><span class="pre">GMM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.GMM.score"><code class="docutils literal notranslate"><span class="pre">GMM.score()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariEpisodeDataset"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariEpisodeDataset.NAMES"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.NAMES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariEpisodeDataset.column_names"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariEpisodeDataset.nested_step"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.nested_step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariEpisodeDataset.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.set_pl_trainer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariStepsDataset"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariStepsDataset.NAMES"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.NAMES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariStepsDataset.column_names"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.MinariStepsDataset.nested_step"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.nested_step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.generate_perlin_noise_2d"><code class="docutils literal notranslate"><span class="pre">generate_perlin_noise_2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.perlin_noise_3d"><code class="docutils literal notranslate"><span class="pre">perlin_noise_3d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.synthetic_data.swiss_roll"><code class="docutils literal notranslate"><span class="pre">swiss_roll()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.transforms">stable_pretraining.data.transforms module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.AdditiveGaussian"><code class="docutils literal notranslate"><span class="pre">AdditiveGaussian</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.AdditiveGaussian.BYPASS_VALUE"><code class="docutils literal notranslate"><span class="pre">AdditiveGaussian.BYPASS_VALUE</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.CenterCrop"><code class="docutils literal notranslate"><span class="pre">CenterCrop</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.ColorJitter"><code class="docutils literal notranslate"><span class="pre">ColorJitter</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.ColorJitter.get_params"><code class="docutils literal notranslate"><span class="pre">ColorJitter.get_params()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Compose"><code class="docutils literal notranslate"><span class="pre">Compose</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Conditional"><code class="docutils literal notranslate"><span class="pre">Conditional</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.ContextTargetsMultiBlockMask"><code class="docutils literal notranslate"><span class="pre">ContextTargetsMultiBlockMask</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.ControlledTransform"><code class="docutils literal notranslate"><span class="pre">ControlledTransform</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.GaussianBlur"><code class="docutils literal notranslate"><span class="pre">GaussianBlur</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.GaussianBlur.get_params"><code class="docutils literal notranslate"><span class="pre">GaussianBlur.get_params()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Lambda"><code class="docutils literal notranslate"><span class="pre">Lambda</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.MultiViewTransform"><code class="docutils literal notranslate"><span class="pre">MultiViewTransform</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.PILGaussianBlur"><code class="docutils literal notranslate"><span class="pre">PILGaussianBlur</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.PatchMasking"><code class="docutils literal notranslate"><span class="pre">PatchMasking</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RGB"><code class="docutils literal notranslate"><span class="pre">RGB</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomChannelPermutation"><code class="docutils literal notranslate"><span class="pre">RandomChannelPermutation</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomContiguousTemporalSampler"><code class="docutils literal notranslate"><span class="pre">RandomContiguousTemporalSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomCrop"><code class="docutils literal notranslate"><span class="pre">RandomCrop</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomCrop.get_params"><code class="docutils literal notranslate"><span class="pre">RandomCrop.get_params()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomGrayscale"><code class="docutils literal notranslate"><span class="pre">RandomGrayscale</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomHorizontalFlip"><code class="docutils literal notranslate"><span class="pre">RandomHorizontalFlip</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomMask"><code class="docutils literal notranslate"><span class="pre">RandomMask</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomResizedCrop"><code class="docutils literal notranslate"><span class="pre">RandomResizedCrop</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomResizedCrop.get_params"><code class="docutils literal notranslate"><span class="pre">RandomResizedCrop.get_params()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomRotation"><code class="docutils literal notranslate"><span class="pre">RandomRotation</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomRotation.get_params"><code class="docutils literal notranslate"><span class="pre">RandomRotation.get_params()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RandomSolarize"><code class="docutils literal notranslate"><span class="pre">RandomSolarize</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Resize"><code class="docutils literal notranslate"><span class="pre">Resize</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RoundRobinMultiViewTransform"><code class="docutils literal notranslate"><span class="pre">RoundRobinMultiViewTransform</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.RoutingTransform"><code class="docutils literal notranslate"><span class="pre">RoutingTransform</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.ToImage"><code class="docutils literal notranslate"><span class="pre">ToImage</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform"><code class="docutils literal notranslate"><span class="pre">Transform</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.get_name"><code class="docutils literal notranslate"><span class="pre">Transform.get_name()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.name"><code class="docutils literal notranslate"><span class="pre">Transform.name</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.nested_get"><code class="docutils literal notranslate"><span class="pre">Transform.nested_get()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.nested_set"><code class="docutils literal notranslate"><span class="pre">Transform.nested_set()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.single_nested_get"><code class="docutils literal notranslate"><span class="pre">Transform.single_nested_get()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.Transform.single_nested_set"><code class="docutils literal notranslate"><span class="pre">Transform.single_nested_set()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.UniformTemporalSubsample"><code class="docutils literal notranslate"><span class="pre">UniformTemporalSubsample</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.UniformTemporalSubsample.forward"><code class="docutils literal notranslate"><span class="pre">UniformTemporalSubsample.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.WrapTorchTransform"><code class="docutils literal notranslate"><span class="pre">WrapTorchTransform</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.random_seed"><code class="docutils literal notranslate"><span class="pre">random_seed()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.set_seed"><code class="docutils literal notranslate"><span class="pre">set_seed()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.transforms.to_image"><code class="docutils literal notranslate"><span class="pre">to_image()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data.utils">stable_pretraining.data.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.utils.apply_masks"><code class="docutils literal notranslate"><span class="pre">apply_masks()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.utils.fold_views"><code class="docutils literal notranslate"><span class="pre">fold_views()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.utils.get_num_workers"><code class="docutils literal notranslate"><span class="pre">get_num_workers()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.utils.random_split"><code class="docutils literal notranslate"><span class="pre">random_split()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.data/#module-stable_pretraining.data">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Categorical"><code class="docutils literal notranslate"><span class="pre">Categorical</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Categorical.sample"><code class="docutils literal notranslate"><span class="pre">Categorical.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Collator"><code class="docutils literal notranslate"><span class="pre">Collator</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule"><code class="docutils literal notranslate"><span class="pre">DataModule</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.load_state_dict"><code class="docutils literal notranslate"><span class="pre">DataModule.load_state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.predict_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">DataModule.set_pl_trainer()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.setup"><code class="docutils literal notranslate"><span class="pre">DataModule.setup()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.state_dict"><code class="docutils literal notranslate"><span class="pre">DataModule.state_dict()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.teardown"><code class="docutils literal notranslate"><span class="pre">DataModule.teardown()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.test_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.test_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.train_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.train_dataloader()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.DataModule.val_dataloader"><code class="docutils literal notranslate"><span class="pre">DataModule.val_dataloader()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Dataset"><code class="docutils literal notranslate"><span class="pre">Dataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Dataset.process_sample"><code class="docutils literal notranslate"><span class="pre">Dataset.process_sample()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Dataset.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">Dataset.set_pl_trainer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.ExponentialMixtureNoiseModel"><code class="docutils literal notranslate"><span class="pre">ExponentialMixtureNoiseModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.ExponentialMixtureNoiseModel.sample"><code class="docutils literal notranslate"><span class="pre">ExponentialMixtureNoiseModel.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.ExponentialNormalNoiseModel"><code class="docutils literal notranslate"><span class="pre">ExponentialNormalNoiseModel</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.ExponentialNormalNoiseModel.sample"><code class="docutils literal notranslate"><span class="pre">ExponentialNormalNoiseModel.sample()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.FromTorchDataset"><code class="docutils literal notranslate"><span class="pre">FromTorchDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.FromTorchDataset.column_names"><code class="docutils literal notranslate"><span class="pre">FromTorchDataset.column_names</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.GMM"><code class="docutils literal notranslate"><span class="pre">GMM</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.GMM.score"><code class="docutils literal notranslate"><span class="pre">GMM.score()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.HFDataset"><code class="docutils literal notranslate"><span class="pre">HFDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.HFDataset.column_names"><code class="docutils literal notranslate"><span class="pre">HFDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.HFDataset.is_saved_with_save_to_disk"><code class="docutils literal notranslate"><span class="pre">HFDataset.is_saved_with_save_to_disk()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariEpisodeDataset"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariEpisodeDataset.NAMES"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.NAMES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariEpisodeDataset.column_names"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariEpisodeDataset.nested_step"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.nested_step()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariEpisodeDataset.set_pl_trainer"><code class="docutils literal notranslate"><span class="pre">MinariEpisodeDataset.set_pl_trainer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariStepsDataset"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariStepsDataset.NAMES"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.NAMES</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariStepsDataset.column_names"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.MinariStepsDataset.nested_step"><code class="docutils literal notranslate"><span class="pre">MinariStepsDataset.nested_step()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.RandomBatchSampler"><code class="docutils literal notranslate"><span class="pre">RandomBatchSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.RepeatedRandomSampler"><code class="docutils literal notranslate"><span class="pre">RepeatedRandomSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Subset"><code class="docutils literal notranslate"><span class="pre">Subset</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Subset.column_names"><code class="docutils literal notranslate"><span class="pre">Subset.column_names</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Subset.dataset"><code class="docutils literal notranslate"><span class="pre">Subset.dataset</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.Subset.indices"><code class="docutils literal notranslate"><span class="pre">Subset.indices</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.SupervisedBatchSampler"><code class="docutils literal notranslate"><span class="pre">SupervisedBatchSampler</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.bulk_download"><code class="docutils literal notranslate"><span class="pre">bulk_download()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.download"><code class="docutils literal notranslate"><span class="pre">download()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.fold_views"><code class="docutils literal notranslate"><span class="pre">fold_views()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.generate_perlin_noise_2d"><code class="docutils literal notranslate"><span class="pre">generate_perlin_noise_2d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.perlin_noise_3d"><code class="docutils literal notranslate"><span class="pre">perlin_noise_3d()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.random_split"><code class="docutils literal notranslate"><span class="pre">random_split()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.data/#stable_pretraining.data.swiss_roll"><code class="docutils literal notranslate"><span class="pre">swiss_roll()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.losses/">stable_pretraining.losses package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses.dino">stable_pretraining.losses.dino module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.cross_entropy_loss"><code class="docutils literal notranslate"><span class="pre">cross_entropy_loss()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.dino.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses.joint_embedding">stable_pretraining.losses.joint_embedding module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.InfoNCELoss"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.InfoNCELoss.forward"><code class="docutils literal notranslate"><span class="pre">InfoNCELoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.SwAVLoss"><code class="docutils literal notranslate"><span class="pre">SwAVLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.SwAVLoss.forward"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.SwAVLoss.sinkhorn"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.sinkhorn()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.SwAVLoss.swapped_prediction"><code class="docutils literal notranslate"><span class="pre">SwAVLoss.swapped_prediction()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.joint_embedding.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses.multimodal">stable_pretraining.losses.multimodal module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.multimodal.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.multimodal.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses.reconstruction">stable_pretraining.losses.reconstruction module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.reconstruction.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses.utils">stable_pretraining.losses.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.utils.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.utils.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.utils.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.utils.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.losses/#module-stable_pretraining.losses">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.BYOLLoss"><code class="docutils literal notranslate"><span class="pre">BYOLLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.BYOLLoss.forward"><code class="docutils literal notranslate"><span class="pre">BYOLLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.BarlowTwinsLoss"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.BarlowTwinsLoss.forward"><code class="docutils literal notranslate"><span class="pre">BarlowTwinsLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.CLIPLoss"><code class="docutils literal notranslate"><span class="pre">CLIPLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.CLIPLoss.forward"><code class="docutils literal notranslate"><span class="pre">CLIPLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss.apply_center_update"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.apply_center_update()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.sinkhorn_knopp_teacher()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss.softmax_center_teacher"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.softmax_center_teacher()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv1Loss.update_center"><code class="docutils literal notranslate"><span class="pre">DINOv1Loss.update_center()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv2Loss"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.DINOv2Loss.forward"><code class="docutils literal notranslate"><span class="pre">DINOv2Loss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.NTXEntLoss"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.NTXEntLoss.forward"><code class="docutils literal notranslate"><span class="pre">NTXEntLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.NegativeCosineSimilarity"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.NegativeCosineSimilarity.forward"><code class="docutils literal notranslate"><span class="pre">NegativeCosineSimilarity.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.VICRegLoss"><code class="docutils literal notranslate"><span class="pre">VICRegLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.VICRegLoss.forward"><code class="docutils literal notranslate"><span class="pre">VICRegLoss.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.iBOTPatchLoss"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.iBOTPatchLoss.forward"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.forward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.iBOTPatchLoss.sinkhorn_knopp_teacher"><code class="docutils literal notranslate"><span class="pre">iBOTPatchLoss.sinkhorn_knopp_teacher()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.mae"><code class="docutils literal notranslate"><span class="pre">mae()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.off_diagonal"><code class="docutils literal notranslate"><span class="pre">off_diagonal()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.losses/#stable_pretraining.losses.sinkhorn_knopp"><code class="docutils literal notranslate"><span class="pre">sinkhorn_knopp()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.optim/">stable_pretraining.optim package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.optim/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.optim/#module-stable_pretraining.optim.lars">stable_pretraining.optim.lars module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lars.LARS"><code class="docutils literal notranslate"><span class="pre">LARS</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lars.LARS.step"><code class="docutils literal notranslate"><span class="pre">LARS.step()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.optim/#module-stable_pretraining.optim.lr_scheduler">stable_pretraining.optim.lr_scheduler module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.CosineDecayer"><code class="docutils literal notranslate"><span class="pre">CosineDecayer</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmup"><code class="docutils literal notranslate"><span class="pre">LinearWarmup()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmupCosineAnnealing"><code class="docutils literal notranslate"><span class="pre">LinearWarmupCosineAnnealing()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmupCosineAnnealingLR"><code class="docutils literal notranslate"><span class="pre">LinearWarmupCosineAnnealingLR</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmupCosineAnnealingLR.get_lr"><code class="docutils literal notranslate"><span class="pre">LinearWarmupCosineAnnealingLR.get_lr()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmupCyclicAnnealing"><code class="docutils literal notranslate"><span class="pre">LinearWarmupCyclicAnnealing()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.LinearWarmupThreeStepsAnnealing"><code class="docutils literal notranslate"><span class="pre">LinearWarmupThreeStepsAnnealing()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.lr_scheduler.create_scheduler"><code class="docutils literal notranslate"><span class="pre">create_scheduler()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.optim/#module-stable_pretraining.optim.utils">stable_pretraining.optim.utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.optim/#stable_pretraining.optim.utils.create_optimizer"><code class="docutils literal notranslate"><span class="pre">create_optimizer()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.optim/#module-stable_pretraining.optim">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../stable_pretraining.utils/">stable_pretraining.utils package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.batch_utils">stable_pretraining.utils.batch_utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.batch_utils.detach_tensors"><code class="docutils literal notranslate"><span class="pre">detach_tensors()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.batch_utils.get_data_from_batch_or_outputs"><code class="docutils literal notranslate"><span class="pre">get_data_from_batch_or_outputs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.config">stable_pretraining.utils.config module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.adapt_resnet_for_lowres"><code class="docutils literal notranslate"><span class="pre">adapt_resnet_for_lowres()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.execute_from_config"><code class="docutils literal notranslate"><span class="pre">execute_from_config()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.find_module"><code class="docutils literal notranslate"><span class="pre">find_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.is_dist"><code class="docutils literal notranslate"><span class="pre">is_dist()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.load_hparams_from_ckpt"><code class="docutils literal notranslate"><span class="pre">load_hparams_from_ckpt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.replace_module"><code class="docutils literal notranslate"><span class="pre">replace_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.rgetattr"><code class="docutils literal notranslate"><span class="pre">rgetattr()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.config.rsetattr"><code class="docutils literal notranslate"><span class="pre">rsetattr()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.data_generation">stable_pretraining.utils.data_generation module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.data_generation.generate_dae_samples"><code class="docutils literal notranslate"><span class="pre">generate_dae_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.data_generation.generate_dm_samples"><code class="docutils literal notranslate"><span class="pre">generate_dm_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.data_generation.generate_ssl_samples"><code class="docutils literal notranslate"><span class="pre">generate_ssl_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.data_generation.generate_sup_samples"><code class="docutils literal notranslate"><span class="pre">generate_sup_samples()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.distance_metrics">stable_pretraining.utils.distance_metrics module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distance_metrics.compute_pairwise_distances"><code class="docutils literal notranslate"><span class="pre">compute_pairwise_distances()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distance_metrics.compute_pairwise_distances_chunked"><code class="docutils literal notranslate"><span class="pre">compute_pairwise_distances_chunked()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.distributed">stable_pretraining.utils.distributed module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.FullGatherLayer"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.FullGatherLayer.backward"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.FullGatherLayer.forward"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.all_reduce"><code class="docutils literal notranslate"><span class="pre">all_reduce()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.distributed.is_dist_avail_and_initialized"><code class="docutils literal notranslate"><span class="pre">is_dist_avail_and_initialized()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.error_handling">stable_pretraining.utils.error_handling module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.catch_errors"><code class="docutils literal notranslate"><span class="pre">catch_errors()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.catch_errors_class"><code class="docutils literal notranslate"><span class="pre">catch_errors_class()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.catch_errors_decorator"><code class="docutils literal notranslate"><span class="pre">catch_errors_decorator()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.get_rank"><code class="docutils literal notranslate"><span class="pre">get_rank()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.is_main_process"><code class="docutils literal notranslate"><span class="pre">is_main_process()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.error_handling.with_hf_retry_ratelimit"><code class="docutils literal notranslate"><span class="pre">with_hf_retry_ratelimit()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.gdrive_utils">stable_pretraining.utils.gdrive_utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader"><code class="docutils literal notranslate"><span class="pre">GDriveUploader</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader.get_folder_id"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_folder_id()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader.get_folder_url"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_folder_url()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader.get_queue_size"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_queue_size()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader.upload_file"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.upload_file()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.gdrive_utils.GDriveUploader.wait_for_uploads"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.wait_for_uploads()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.inspection_utils">stable_pretraining.utils.inspection_utils module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.inspection_utils.broadcast_param_to_list"><code class="docutils literal notranslate"><span class="pre">broadcast_param_to_list()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.inspection_utils.dict_values"><code class="docutils literal notranslate"><span class="pre">dict_values()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.inspection_utils.get_required_fn_parameters"><code class="docutils literal notranslate"><span class="pre">get_required_fn_parameters()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.lightning_patch">stable_pretraining.utils.lightning_patch module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.lightning_patch.apply_manual_optimization_patch"><code class="docutils literal notranslate"><span class="pre">apply_manual_optimization_patch()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.lightning_patch.restore_original_validation"><code class="docutils literal notranslate"><span class="pre">restore_original_validation()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.log_reader">stable_pretraining.utils.log_reader module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LocalLogReader"><code class="docutils literal notranslate"><span class="pre">LocalLogReader</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LocalLogReader.read"><code class="docutils literal notranslate"><span class="pre">LocalLogReader.read()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LocalLogReader.read_config"><code class="docutils literal notranslate"><span class="pre">LocalLogReader.read_config()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LocalLogReader.read_project"><code class="docutils literal notranslate"><span class="pre">LocalLogReader.read_project()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LogReader"><code class="docutils literal notranslate"><span class="pre">LogReader</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.LogReader.read"><code class="docutils literal notranslate"><span class="pre">LogReader.read()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.TableFormatter"><code class="docutils literal notranslate"><span class="pre">TableFormatter</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.TableFormatter.create_table"><code class="docutils literal notranslate"><span class="pre">TableFormatter.create_table()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.TableFormatter.tabulate_runs"><code class="docutils literal notranslate"><span class="pre">TableFormatter.tabulate_runs()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.WandbLogReader"><code class="docutils literal notranslate"><span class="pre">WandbLogReader</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.WandbLogReader.read"><code class="docutils literal notranslate"><span class="pre">WandbLogReader.read()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.WandbLogReader.read_project"><code class="docutils literal notranslate"><span class="pre">WandbLogReader.read_project()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.alphanum_key"><code class="docutils literal notranslate"><span class="pre">alphanum_key()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.create_results_table"><code class="docutils literal notranslate"><span class="pre">create_results_table()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.flatten_config"><code class="docutils literal notranslate"><span class="pre">flatten_config()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.natural_sort"><code class="docutils literal notranslate"><span class="pre">natural_sort()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.read_local_logs"><code class="docutils literal notranslate"><span class="pre">read_local_logs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.read_local_project"><code class="docutils literal notranslate"><span class="pre">read_local_project()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.read_wandb_project"><code class="docutils literal notranslate"><span class="pre">read_wandb_project()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.log_reader.read_wandb_run"><code class="docutils literal notranslate"><span class="pre">read_wandb_run()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.nn_modules">stable_pretraining.utils.nn_modules module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.BatchNorm1dNoBias"><code class="docutils literal notranslate"><span class="pre">BatchNorm1dNoBias</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.EMA"><code class="docutils literal notranslate"><span class="pre">EMA</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.EMA.forward"><code class="docutils literal notranslate"><span class="pre">EMA.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.ImageToVideoEncoder"><code class="docutils literal notranslate"><span class="pre">ImageToVideoEncoder</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.ImageToVideoEncoder.forward"><code class="docutils literal notranslate"><span class="pre">ImageToVideoEncoder.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.L2Norm"><code class="docutils literal notranslate"><span class="pre">L2Norm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.L2Norm.forward"><code class="docutils literal notranslate"><span class="pre">L2Norm.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.Normalize"><code class="docutils literal notranslate"><span class="pre">Normalize</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.Normalize.forward"><code class="docutils literal notranslate"><span class="pre">Normalize.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.OrderedQueue"><code class="docutils literal notranslate"><span class="pre">OrderedQueue</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.OrderedQueue.append"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.append()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.OrderedQueue.get"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.get()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.OrderedQueue.get_unsorted"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.get_unsorted()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.OrderedQueue.load_state_dict"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.load_state_dict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.UnsortedQueue"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.UnsortedQueue.append"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue.append()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.nn_modules.UnsortedQueue.get"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue.get()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.read_csv_logger">stable_pretraining.utils.read_csv_logger module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.read_csv_logger.CSVLogAutoSummarizer"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.read_csv_logger.CSVLogAutoSummarizer.METRICS_PATTERNS"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer.METRICS_PATTERNS</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.read_csv_logger.CSVLogAutoSummarizer.collect"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer.collect()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.read_csv_logger.load_best_compressed"><code class="docutils literal notranslate"><span class="pre">load_best_compressed()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.read_csv_logger.save_best_compressed"><code class="docutils literal notranslate"><span class="pre">save_best_compressed()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.timm_to_hf_hub">stable_pretraining.utils.timm_to_hf_hub module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.timm_to_hf_hub.push_timm_to_hf"><code class="docutils literal notranslate"><span class="pre">push_timm_to_hf()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils.visualization">stable_pretraining.utils.visualization module</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.visualization.escape_labels"><code class="docutils literal notranslate"><span class="pre">escape_labels()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.visualization.format_df_to_latex"><code class="docutils literal notranslate"><span class="pre">format_df_to_latex()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.visualization.imshow_with_grid"><code class="docutils literal notranslate"><span class="pre">imshow_with_grid()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.visualization.latex_escape"><code class="docutils literal notranslate"><span class="pre">latex_escape()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.visualization.visualize_images_graph"><code class="docutils literal notranslate"><span class="pre">visualize_images_graph()</span></code></a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../stable_pretraining.utils/#module-stable_pretraining.utils">Module contents</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.BatchNorm1dNoBias"><code class="docutils literal notranslate"><span class="pre">BatchNorm1dNoBias</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.CSVLogAutoSummarizer"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.CSVLogAutoSummarizer.METRICS_PATTERNS"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer.METRICS_PATTERNS</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.CSVLogAutoSummarizer.collect"><code class="docutils literal notranslate"><span class="pre">CSVLogAutoSummarizer.collect()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.EMA"><code class="docutils literal notranslate"><span class="pre">EMA</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.EMA.forward"><code class="docutils literal notranslate"><span class="pre">EMA.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.FullGatherLayer"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.FullGatherLayer.backward"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer.backward()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.FullGatherLayer.forward"><code class="docutils literal notranslate"><span class="pre">FullGatherLayer.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader"><code class="docutils literal notranslate"><span class="pre">GDriveUploader</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader.get_folder_id"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_folder_id()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader.get_folder_url"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_folder_url()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader.get_queue_size"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.get_queue_size()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader.upload_file"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.upload_file()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.GDriveUploader.wait_for_uploads"><code class="docutils literal notranslate"><span class="pre">GDriveUploader.wait_for_uploads()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.ImageToVideoEncoder"><code class="docutils literal notranslate"><span class="pre">ImageToVideoEncoder</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.ImageToVideoEncoder.forward"><code class="docutils literal notranslate"><span class="pre">ImageToVideoEncoder.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.L2Norm"><code class="docutils literal notranslate"><span class="pre">L2Norm</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.L2Norm.forward"><code class="docutils literal notranslate"><span class="pre">L2Norm.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.Normalize"><code class="docutils literal notranslate"><span class="pre">Normalize</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.Normalize.forward"><code class="docutils literal notranslate"><span class="pre">Normalize.forward()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.OrderedQueue"><code class="docutils literal notranslate"><span class="pre">OrderedQueue</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.OrderedQueue.append"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.append()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.OrderedQueue.get"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.get()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.OrderedQueue.get_unsorted"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.get_unsorted()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.OrderedQueue.load_state_dict"><code class="docutils literal notranslate"><span class="pre">OrderedQueue.load_state_dict()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.UnsortedQueue"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.UnsortedQueue.append"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue.append()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.UnsortedQueue.get"><code class="docutils literal notranslate"><span class="pre">UnsortedQueue.get()</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.adapt_resnet_for_lowres"><code class="docutils literal notranslate"><span class="pre">adapt_resnet_for_lowres()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.all_gather"><code class="docutils literal notranslate"><span class="pre">all_gather()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.all_reduce"><code class="docutils literal notranslate"><span class="pre">all_reduce()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.broadcast_param_to_list"><code class="docutils literal notranslate"><span class="pre">broadcast_param_to_list()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.compute_pairwise_distances"><code class="docutils literal notranslate"><span class="pre">compute_pairwise_distances()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.compute_pairwise_distances_chunked"><code class="docutils literal notranslate"><span class="pre">compute_pairwise_distances_chunked()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.detach_tensors"><code class="docutils literal notranslate"><span class="pre">detach_tensors()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.dict_values"><code class="docutils literal notranslate"><span class="pre">dict_values()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.execute_from_config"><code class="docutils literal notranslate"><span class="pre">execute_from_config()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.find_module"><code class="docutils literal notranslate"><span class="pre">find_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.format_df_to_latex"><code class="docutils literal notranslate"><span class="pre">format_df_to_latex()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.generate_dae_samples"><code class="docutils literal notranslate"><span class="pre">generate_dae_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.generate_dm_samples"><code class="docutils literal notranslate"><span class="pre">generate_dm_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.generate_ssl_samples"><code class="docutils literal notranslate"><span class="pre">generate_ssl_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.generate_sup_samples"><code class="docutils literal notranslate"><span class="pre">generate_sup_samples()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.get_data_from_batch_or_outputs"><code class="docutils literal notranslate"><span class="pre">get_data_from_batch_or_outputs()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.get_required_fn_parameters"><code class="docutils literal notranslate"><span class="pre">get_required_fn_parameters()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.is_dist_avail_and_initialized"><code class="docutils literal notranslate"><span class="pre">is_dist_avail_and_initialized()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.load_hparams_from_ckpt"><code class="docutils literal notranslate"><span class="pre">load_hparams_from_ckpt()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.replace_module"><code class="docutils literal notranslate"><span class="pre">replace_module()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.rgetattr"><code class="docutils literal notranslate"><span class="pre">rgetattr()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.rsetattr"><code class="docutils literal notranslate"><span class="pre">rsetattr()</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../stable_pretraining.utils/#stable_pretraining.utils.with_hf_retry_ratelimit"><code class="docutils literal notranslate"><span class="pre">with_hf_retry_ratelimit()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</section>
<section id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Link to this heading">#</a></h2>
</section>
<section id="module-stable_pretraining.cli">
<span id="stable-pretraining-cli-module"></span><h2>stable_pretraining.cli module<a class="headerlink" href="#module-stable_pretraining.cli" title="Link to this heading">#</a></h2>
<p>Command-line interface for Stable SSL training.</p>
<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.cli.dump_csv_logs">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.cli.</span></span><span class="sig-name descname"><span class="pre">dump_csv_logs</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dir:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&lt;typer.models.ArgumentInfo</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_name:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&lt;typer.models.ArgumentInfo</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">agg:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&lt;typer.models.ArgumentInfo</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/cli/#dump_csv_logs"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.cli.dump_csv_logs" title="Link to this definition">#</a></dt>
<dd><p>Compress CSV logs to the smallest possible format with aggregation.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.cli.run">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.cli.</span></span><span class="sig-name descname"><span class="pre">run</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config:</span> <span class="pre">str</span> <span class="pre">=</span> <span class="pre">&lt;typer.models.ArgumentInfo</span> <span class="pre">object&gt;</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overrides:</span> <span class="pre">~typing.List[str]</span> <span class="pre">|</span> <span class="pre">None</span> <span class="pre">=</span> <span class="pre">&lt;typer.models.ArgumentInfo</span> <span class="pre">object&gt;</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/cli/#run"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.cli.run" title="Link to this definition">#</a></dt>
<dd><p>Execute experiment with the specified config.</p>
<p class="rubric">Examples</p>
<p>spt run config.yaml</p>
<p>spt run config.yaml -m</p>
<p>spt run config.yaml trainer.max_epochs=100</p>
</dd></dl>

</section>
<section id="module-stable_pretraining.config">
<span id="stable-pretraining-config-module"></span><h2>stable_pretraining.config module<a class="headerlink" href="#module-stable_pretraining.config" title="Link to this heading">#</a></h2>
<p>Configuration classes specifying default parameters for stable-SSL.</p>
<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.config.collapse_nested_dict">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.config.</span></span><span class="sig-name descname"><span class="pre">collapse_nested_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><span class="pre">object</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">level_separator</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'.'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_base_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">_flat_cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/config/#collapse_nested_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.config.collapse_nested_dict" title="Link to this definition">#</a></dt>
<dd><p>Parse a Hydra config and make it readable for wandb (flatten).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cfg</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><em>object</em></a><em>]</em>)  The original (Hydra) nested dict.</p></li>
<li><p><strong>level_separator</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  The string to separate level names. Defaults to ..</p></li>
<li><p><strong>_base_name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  The parent string, used for recursion only, users should ignore.
Defaults to None.</p></li>
<li><p><strong>_flat_cfg</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>optional</em>)  The flattened config, used for recursion only, users should ignore.
Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Flat config.</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.config.instantiate_from_config">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.config.</span></span><span class="sig-name descname"><span class="pre">instantiate_from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DictConfig</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/config/#instantiate_from_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.config.instantiate_from_config" title="Link to this definition">#</a></dt>
<dd><p>Main entry point for config-based training.</p>
<p>This function handles the complete instantiation of a training setup from config:
- Recursively instantiates all components
- Creates Manager if trainer/module/data are present
- Returns appropriate object based on config structure</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cfg</strong>  Complete configuration dictionary or DictConfig</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Manager instance if config contains trainer/module/data,
otherwise returns instantiated config dict</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.config.recursive_instantiate">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.config.</span></span><span class="sig-name descname"><span class="pre">recursive_instantiate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">DictConfig</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">parent_objects</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/config/#recursive_instantiate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.config.recursive_instantiate" title="Link to this definition">#</a></dt>
<dd><p>Recursively instantiate all components in config with dependency resolution.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>cfg</strong>  Configuration dictionary or DictConfig with _target_ fields</p></li>
<li><p><strong>parent_objects</strong>  Optional dict of already instantiated objects for dependencies</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dictionary of instantiated components</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="module-stable_pretraining.forward">
<span id="stable-pretraining-forward-module"></span><h2>stable_pretraining.forward module<a class="headerlink" href="#module-stable_pretraining.forward" title="Link to this heading">#</a></h2>
<p>Forward functions for self-supervised learning methods.</p>
<p>This module provides pre-defined forward functions for various SSL methods
that can be used with the Module class. These functions define the training
logic for each method and can be specified in YAML configs or Python code.</p>
<p class="rubric">Example</p>
<p>Using in a YAML config:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">module</span><span class="p">:</span>
  <span class="n">_target_</span><span class="p">:</span> <span class="n">stable_pretraining</span><span class="o">.</span><span class="n">Module</span>
  <span class="n">forward</span><span class="p">:</span> <span class="n">stable_pretraining</span><span class="o">.</span><span class="n">forward</span><span class="o">.</span><span class="n">simclr_forward</span>
  <span class="n">backbone</span><span class="p">:</span> <span class="o">...</span>
  <span class="n">projector</span><span class="p">:</span> <span class="o">...</span>
</pre></div>
</div>
<p>Using in Python code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">stable_pretraining</span><span class="w"> </span><span class="kn">import</span> <span class="n">Module</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">stable_pretraining.forward</span><span class="w"> </span><span class="kn">import</span> <span class="n">simclr_forward</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">Module</span><span class="p">(</span><span class="n">forward</span><span class="o">=</span><span class="n">simclr_forward</span><span class="p">,</span> <span class="n">backbone</span><span class="o">=</span><span class="n">backbone</span><span class="p">,</span> <span class="n">projector</span><span class="o">=</span><span class="n">projector</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.barlow_twins_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">barlow_twins_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#barlow_twins_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.barlow_twins_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for Barlow Twins.</p>
<p>Barlow Twins learns representations by making the cross-correlation matrix
between embeddings of augmented views as close to the identity matrix as
possible, reducing redundancy while maintaining invariance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: Feature extraction network
- projector: Projection head (typically with BN and high dimension)
- barlow_loss: Barlow Twins loss function</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from backbone</p></li>
<li><p>loss: Barlow Twins loss (during training only)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the Barlow Twins paper <span id="id1">[<a class="reference internal" href="../bibliography/#id4" title="Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stphane Deny. Barlow twins: self-supervised learning via redundancy reduction. In International conference on machine learning, 1231012320. PMLR, 2021.">Zbontar <em>et al.</em>, 2021</a>]</span>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.byol_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">byol_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#byol_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.byol_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for BYOL (Bootstrap Your Own Latent).</p>
<p>BYOL learns representations without negative pairs by using a momentum-based
target network and predicting target projections from online projections.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance with required attributes:
- backbone: TeacherStudentWrapper for feature extraction
- projector: TeacherStudentWrapper for projection head
- predictor: Online network predictor
- byol_loss: BYOL loss function (optional, uses MSE if not provided)</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from teacher backbone (EMA target)</p></li>
<li><p>loss: BYOL loss between predictions and targets (during training)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the BYOL paper <span id="id2">[<a class="reference internal" href="../bibliography/#id3" title="Jean-Bastien Grill, Florian Strub, Florent Altch, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, and others. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:2127121284, 2020.">Grill <em>et al.</em>, 2020</a>]</span>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.dino_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">dino_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#dino_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.dino_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for DINO (self-DIstillation with NO labels).</p>
<p>DINO learns representations through self-distillation where a student network
is trained to match the output of a teacher network (EMA of student) on
different augmented views. Global views are processed by both networks while
local views are only processed by the student.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: TeacherStudentWrapper for feature extraction
- projector: TeacherStudentWrapper for projection head
- dino_loss: DINOv1Loss instance (required, pass spt.losses.DINOv1Loss())
- warmup_temperature_teacher (float): Starting teacher temperature
- temperature_teacher (float): Final teacher temperature
- warmup_epochs_temperature_teacher (int): Epochs to warm up temperature</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view).
For multi-crop: First 2 views should be global crops, rest are local crops</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from teacher backbone</p></li>
<li><p>loss: DINO distillation loss (during training only)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the DINO paper <span id="id3">[<a class="reference internal" href="../bibliography/#id9" title="Mathilde Caron, Hugo Touvron, Ishan Misra, Herv Jgou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, 96509660. 2021.">Caron <em>et al.</em>, 2021</a>]</span>.
Requires TeacherStudentWrapper for both backbone and projector,
and assumes first 2 views in batch are global views.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.dinov2_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">dinov2_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#dinov2_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.dinov2_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for DINOv2 with iBOT.</p>
<p>DINOv2 combines two self-supervised losses:
- DINO: CLS token distillation between global views
- iBOT: Masked patch prediction</p>
<p>Both losses use Sinkhorn-Knopp normalization for optimal transport.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: TeacherStudentWrapper for feature extraction (ViT)
- projector: TeacherStudentWrapper for CLS token projection head
- patch_projector: TeacherStudentWrapper for patch projection head
- dinov2_loss: DINOv2Loss instance combining DINO + iBOT
- warmup_temperature_teacher (float): Starting teacher temperature
- temperature_teacher (float): Final teacher temperature
- warmup_epochs_temperature_teacher (int): Epochs to warm up temperature
- mask_ratio (float): Ratio of patches to mask for iBOT (default: 0.3)</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view).
For multi-crop: First 2 views should be global crops, rest are local crops</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from teacher backbone</p></li>
<li><p>loss: Combined DINOv2 loss (DINO + iBOT)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the DINOv2 paper.
Requires TeacherStudentWrapper for backbone, projector, and patch_projector.
Assumes first 2 views in batch are global views.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.nnclr_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">nnclr_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#nnclr_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.nnclr_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for NNCLR (Nearest-Neighbor Contrastive Learning).</p>
<p>NNCLR learns representations by using the nearest neighbor of an augmented
view from a support set of past embeddings as a positive pair. This encourages
the model to learn representations that are similar for semantically similar
instances, not just for different augmentations of the same instance.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: Feature extraction network
- projector: Projection head for embedding transformation
- predictor: Prediction head used for the online view
- nnclr_loss: NTXent contrastive loss function</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from backbone</p></li>
<li><p>loss: NTXent contrastive loss (during training only)</p></li>
<li><p>nnclr_support_set: Projections to be added to the support set queue</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the NNCLR paper <span id="id4">[<a class="reference internal" href="../bibliography/#id12" title="Debidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With a little help from my friends: nearest-neighbor contrastive learning of visual representations. In Proceedings of the IEEE/CVF international conference on computer vision, 95889597. 2021.">Dwibedi <em>et al.</em>, 2021</a>]</span>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.simclr_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">simclr_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#simclr_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.simclr_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for SimCLR (Simple Contrastive Learning of Representations).</p>
<p>SimCLR learns representations by maximizing agreement between differently
augmented views of the same image via a contrastive loss in the latent space.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: Feature extraction network
- projector: Projection head mapping features to latent space
- simclr_loss: NT-Xent contrastive loss function</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from backbone</p></li>
<li><p>loss: NT-Xent contrastive loss (during training only)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the SimCLR paper <span id="id5">[<a class="reference internal" href="../bibliography/#id2" title="Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on machine learning, 15971607. PMLR, 2020.">Chen <em>et al.</em>, 2020</a>]</span>.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.supervised_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">supervised_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#supervised_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.supervised_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for standard supervised training.</p>
<p>This function implements traditional supervised learning with labels,
useful for baseline comparisons and fine-tuning pre-trained models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: Feature extraction network
- classifier: Classification head (e.g., Linear layer)
- supervised_loss: Loss function for supervised learning</p></li>
<li><p><strong>batch</strong>  Input batch dictionary containing:
- image: Tensor of images [N, C, H, W]
- label: Ground truth labels [N] (optional, for loss computation)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from backbone</p></li>
<li><p>logits: Classification predictions</p></li>
<li><p>loss: Supervised loss (if labels provided)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Unlike SSL methods, this function uses actual labels for training
and is primarily used for evaluation or supervised baselines.</p>
</div>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.swav_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">swav_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#swav_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.swav_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for SwAV (Swapping Assignments between Views).</p>
<p>SwAV learns representations by predicting the cluster assignment (code) of one
view from the representation of another view. For small-batch training, this function
manages a feature queue to stabilize the training process.</p>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.forward.vicreg_forward">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.forward.</span></span><span class="sig-name descname"><span class="pre">vicreg_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">self</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/forward/#vicreg_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.forward.vicreg_forward" title="Link to this definition">#</a></dt>
<dd><p>Forward function for VICReg (Variance-Invariance-Covariance Regularization).</p>
<p>VICReg learns representations using three criteria: variance (maintaining
information), invariance (to augmentations), and covariance (decorrelating
features).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>self</strong>  Module instance (automatically bound) with required attributes:
- backbone: Feature extraction network
- projector: Projection head for embedding transformation
- vicreg_loss: VICReg loss with variance, invariance, covariance terms</p></li>
<li><p><strong>batch</strong>  Either a list of view dicts (from MultiViewTransform) or
a single dict (for validation/single-view)</p></li>
<li><p><strong>stage</strong>  Training stage (train, val, or test)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p>embedding: Feature representations from backbone</p></li>
<li><p>loss: Combined VICReg loss (during training only)</p></li>
<li><p>label: Labels if present (for probes/callbacks)</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Dictionary containing</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Introduced in the VICReg paper <span id="id6">[]</span>.</p>
</div>
</dd></dl>

</section>
<section id="module-stable_pretraining.manager">
<span id="stable-pretraining-manager-module"></span><h2>stable_pretraining.manager module<a class="headerlink" href="#module-stable_pretraining.manager" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.manager.</span></span><span class="sig-name descname"><span class="pre">Manager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpointable</span></code></p>
<p>Manages training with logging, scheduling, and checkpointing support.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.Trainer</em><em>]</em>)  PyTorch Lightning trainer configuration or instance.</p></li>
<li><p><strong>module</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.LightningModule</em><em>]</em>)  Lightning module configuration or instance.</p></li>
<li><p><strong>data</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.LightningDataModule</em><em>]</em>)  Data module configuration or instance.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  Random seed for reproducibility. Defaults to None.</p></li>
<li><p><strong>ckpt_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  Path to checkpoint for resuming training. Defaults to last.</p></li>
<li><p><strong>compile</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  Should we compile the given module. Defaults to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.init_and_sync_wandb">
<span class="sig-name descname"><span class="pre">init_and_sync_wandb</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.init_and_sync_wandb"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager.init_and_sync_wandb" title="Link to this definition">#</a></dt>
<dd><p>Handles some utilities for WandB.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.instantiated_data">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">instantiated_data</span></span><a class="headerlink" href="#stable_pretraining.manager.Manager.instantiated_data" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.instantiated_module">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">instantiated_module</span></span><a class="headerlink" href="#stable_pretraining.manager.Manager.instantiated_module" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager.predict" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.save_checkpoint">
<span class="sig-name descname"><span class="pre">save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upload_wandb</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager.save_checkpoint" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager.test" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.manager.Manager.validate">
<span class="sig-name descname"><span class="pre">validate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.validate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.Manager.validate" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.manager.print_logger_info">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.manager.</span></span><span class="sig-name descname"><span class="pre">print_logger_info</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">logger</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#print_logger_info"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.print_logger_info" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.manager.print_signal_info">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.manager.</span></span><span class="sig-name descname"><span class="pre">print_signal_info</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#print_signal_info"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.manager.print_signal_info" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</section>
<section id="module-stable_pretraining.module">
<span id="stable-pretraining-module-module"></span><h2>stable_pretraining.module module<a class="headerlink" href="#module-stable_pretraining.module" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.module.Module">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.module.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></p>
<p>PyTorch Lightning module using manual optimization with multi-optimizer support.</p>
<p>Core usage
- Provide a custom <cite>forward(self, batch, stage)</cite> via the <cite>forward</cite> argument at init.
- During training, <cite>forward</cite> must return a dict with <cite>state[loss]</cite> (a single joint loss).</p>
<blockquote>
<div><p>When multiple optimizers are configured, this joint loss is used for all optimizers.</p>
</div></blockquote>
<p>Optimizer configuration (<cite>self.optim</cite>)
- Single optimizer:</p>
<blockquote>
<div><p>{optimizer: str|dict|partial|Class, scheduler: &lt;see below&gt;, interval: step|epoch, frequency: int}
- Optimizer accepted forms:</p>
<blockquote>
<div><ul class="simple">
<li><p>string name (e.g., AdamW, SGD) from torch.optim</p></li>
<li><p>dict: {type: AdamW, lr: 1e-3, }</p></li>
<li><p>functools.partial: partial(torch.optim.AdamW, lr=1e-3)</p></li>
<li><p>optimizer class: torch.optim.AdamW</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<ul>
<li><p>Multiple optimizers:
{</p>
<blockquote>
<div><dl class="simple">
<dt>name: {</dt><dd><p>modules: regex,                # assign params by module-name pattern (children inherit)
optimizer: str|dict|partial|Class, # optimizer factory (same accepted forms as above)
scheduler: str|dict|partial|Class, # flexible scheduler config (see below)
interval: step|epoch,       # scheduler interval
frequency: int,                   # optimizer step frequency
monitor: str                      # (optional) for ReduceLROnPlateau; alternatively set inside scheduler dict</p>
</dd>
</dl>
<p>}, </p>
</div></blockquote>
<p>}</p>
</li>
</ul>
<p>Parameter assignment (multi-optimizer)
- Modules are matched by regex on their qualified name. Children inherit the parents assignment</p>
<blockquote>
<div><p>unless they match a more specific pattern. Only direct parameters of each module are collected
to avoid duplication.</p>
</div></blockquote>
<p>Schedulers (flexible)
- Accepted forms: string name (e.g., CosineAnnealingLR, StepLR), dict with {type: , },</p>
<blockquote>
<div><p>functools.partial, or a scheduler class. Smart defaults are applied when params are omitted for
common schedulers (CosineAnnealingLR, OneCycleLR, StepLR, ExponentialLR, ReduceLROnPlateau,
LinearLR, ConstantLR). For ReduceLROnPlateau, a <cite>monitor</cite> key is added (default: val_loss).
You may specify <cite>monitor</cite> either alongside the optimizer config (top level) or inside the
scheduler dict itself.</p>
</div></blockquote>
<ul class="simple">
<li><p>The resulting Lightning scheduler dict includes <cite>interval</cite> and <cite>frequency</cite> (or <cite>scheduler_frequency</cite>).</p></li>
</ul>
<p>Training loop behavior
- Manual optimization (<cite>automatic_optimization = False</cite>).
- Gradient accumulation: scales loss by 1/N where N = Trainer.accumulate_grad_batches and steps on the boundary.
- Per-optimizer step frequency: each optimizer steps only when its frequency boundary is met (in addition to accumulation boundary).
- Gradient clipping: uses Trainers <cite>gradient_clip_val</cite> and <cite>gradient_clip_algorithm</cite> before each step.
- Returns the <cite>state</cite> dict from <cite>forward</cite> unchanged for logging/inspection.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.add_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.add_module" title="Link to this definition">#</a></dt>
<dd><p>Add a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module"><em>Module</em></a>)  child module to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.all_gather" title="Link to this definition">#</a></dt>
<dd><p>Gather tensors or collections of tensors from multiple processes.</p>
<p>This method needs to be called on all processes and the tensors need to have the same shape across all
processes, otherwise your program will stall forever.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong>  int, float, tensor of shape (batch, ), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong>  the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong>  flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, ), or if the input was a collection
the output will also be a collection with tensors of this shape. For the special case where
world_size is 1, no additional dimension is added to the tensor(s).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.apply" title="Link to this definition">#</a></dt>
<dd><p>Apply <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p>
<p>Typical use includes initializing the parameters of a model
(see also <a class="reference external" href="https://docs.pytorch.org/docs/stable/nn.init.html#nn-init-doc" title="(in PyTorch v2.9)"><span>torch.nn.init</span></a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>fn</strong> (<a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None)  function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[1., 1.],</span>
<span class="go">        [1., 1.]], requires_grad=True)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[1., 1.],</span>
<span class="go">        [1., 1.]], requires_grad=True)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.backward" title="Link to this definition">#</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#stable_pretraining.module.Module.training_step" title="stable_pretraining.module.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your own
implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>loss</strong>  The loss tensor returned by <a class="reference internal" href="#stable_pretraining.module.Module.training_step" title="stable_pretraining.module.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.bfloat16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.bfloat16" title="Link to this definition">#</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.buffers" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>torch.Tensor</em>  module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.children"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.children" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Module</em>  a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.clip_gradients" title="Link to this definition">#</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Do not override this method. If you want to customize gradient clipping, consider using
<a class="reference internal" href="#stable_pretraining.module.Module.configure_gradient_clipping" title="stable_pretraining.module.Module.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p></li>
<li><p>For manual optimization (<code class="docutils literal notranslate"><span class="pre">self.automatic_optimization</span> <span class="pre">=</span> <span class="pre">False</span></code>), if you want to use
gradient clipping, consider calling
<code class="docutils literal notranslate"><span class="pre">self.clip_gradients(opt,</span> <span class="pre">gradient_clip_val=0.5,</span> <span class="pre">gradient_clip_algorithm=&quot;norm&quot;)</span></code>
manually in the training step.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong>  Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong>  The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong>  The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.compile" title="Link to this definition">#</a></dt>
<dd><p>Compile this Modules forward using <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.</p>
<p>This Modules <cite>__call__</cite> method is compiled and all arguments are passed as-is
to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> for details on the arguments for this function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence" title="(in Python v3.14)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Callback</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callback</span></span></span><a class="headerlink" href="#stable_pretraining.module.Module.configure_callbacks" title="Link to this definition">#</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code> gets
called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainers
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainers callbacks list, it will take priority and replace them. In addition, Lightning will
make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.configure_gradient_clipping" title="Link to this definition">#</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#stable_pretraining.module.Module.optimizer_step" title="stable_pretraining.module.Module.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong>  Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong>  The value at which to clip gradients. By default, value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong>  The gradient clipping algorithm to use. By default, value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="c1"># Implement your own custom logic to clip gradients</span>
    <span class="c1"># You can call `self.clip_gradients` with your settings:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
    <span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.configure_model">
<span class="sig-name descname"><span class="pre">configure_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.configure_model" title="Link to this definition">#</a></dt>
<dd><p>Hook to create modules in a strategy and precision aware context.</p>
<p>This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where wed like to shard
the model instantly to save memory and initialization time.
For non-sharded strategies, you can choose to override this hook or to initialize your model under the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">init_module()</span></code> context manager.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is <strong>idempotent</strong>, i.e., after the first time the hook is called, subsequent calls
to it should be a no-op.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.configure_optimizers" title="Link to this definition">#</a></dt>
<dd><p>Configure optimizers and schedulers for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optimizer configuration with optional learning rate scheduler.
For single optimizer: Returns a dict with optimizer and lr_scheduler.
For multiple optimizers: Returns a tuple of (optimizers, schedulers).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a> or <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>Multi-optimizer configuration with module pattern matching and schedulers:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simple single optimizer with scheduler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="s2">&quot;CosineAnnealingLR&quot;</span><span class="p">,</span>  <span class="c1"># Uses smart defaults</span>
<span class="gp">... </span>    <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Multi-optimizer with custom scheduler configs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_opt&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span>  <span class="c1"># Matches &#39;encoder&#39; and all children</span>
<span class="gp">... </span>        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;OneCycleLR&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;max_lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
<span class="gp">... </span>        <span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span>    <span class="s2">&quot;head_opt&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="s2">&quot;.*head$&quot;</span><span class="p">,</span>  <span class="c1"># Matches modules ending with &#39;head&#39;</span>
<span class="gp">... </span>        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;SGD&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;ReduceLROnPlateau&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;patience&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;factor&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_accuracy&quot;</span><span class="p">,</span>  <span class="c1"># Required for ReduceLROnPlateau</span>
<span class="gp">... </span>        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<p>With model structure:
- encoder                 -&gt; encoder_opt (matches encoder)
- encoder.layer1          -&gt; encoder_opt (inherits from parent)
- encoder.layer1.conv     -&gt; encoder_opt (inherits from encoder.layer1)
- classifier_head         -&gt; head_opt (matches .*head$)
- classifier_head.linear  -&gt; head_opt (inherits from parent)
- decoder                 -&gt; None (no match, no parameters collected)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.configure_sharded_model" title="Link to this definition">#</a></dt>
<dd><p>Deprecated.</p>
<p>Use <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_model()</span></code> instead.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.cpu" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.cpu()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.cuda" title="Link to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU while
being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong>  If specified, all parameters will be copied to that device. If <cite>None</cite>, the current CUDA device
index will be used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.double" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.double()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.eval" title="Link to this definition">#</a></dt>
<dd><p>Set the module in evaluation mode.</p>
<p>This has an effect only on certain modules. See the documentation of
particular modules for details of their behaviors in training/evaluation
mode, i.e. whether they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code></a>.</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc" title="(in PyTorch v2.9)"><span>Locally disabling gradient computation</span></a> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.extra_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.extra_repr" title="Link to this definition">#</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.float" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.float()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.forward" title="Link to this definition">#</a></dt>
<dd><p>Same as <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong>  Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong>  Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Your models output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.freeze" title="Link to this definition">#</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.get_buffer" title="Link to this definition">#</a></dt>
<dd><p>Return the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this methods functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)">torch.Tensor</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If the target string references an invalid
    path or resolves to something that is not a
    buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_extra_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.get_extra_state" title="Link to this definition">#</a></dt>
<dd><p>Return any extra state to include in the modules state_dict.</p>
<p>Implement this and a corresponding <a class="reference internal" href="#stable_pretraining.module.Module.set_extra_state" title="stable_pretraining.module.Module.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
modules <cite>state_dict()</cite>.</p>
<p>Note that extra state should be picklable to ensure working serialization
of the state_dict. We only provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any extra state to store in the modules state_dict</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)">object</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="(in PyTorch v2.9)"><span class="pre">Parameter</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.get_parameter" title="Link to this definition">#</a></dt>
<dd><p>Return the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this methods functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If the target string references an invalid
    path or resolves to something that is not an
    <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_submodule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.get_submodule" title="Link to this definition">#</a></dt>
<dd><p>Return the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>For example, lets say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> which has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)">torch.nn.Module</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If at any point along the path resulting from
    the target string the (sub)path resolves to a non-existent
    attribute name or an object that is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.half" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.half()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.ipu">
<span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.ipu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.ipu" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the IPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing the optimizer if the module will
live on IPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.load_from_checkpoint">
<span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.IO" title="(in Python v3.14)"><span class="pre">IO</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/storage.html#torch.UntypedStorage" title="(in PyTorch v2.9)"><span class="pre">UntypedStorage</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/storage.html#torch.UntypedStorage" title="(in PyTorch v2.9)"><span class="pre">UntypedStorage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.load_from_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments
passed to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>  in the checkpoint under <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<p>Any arguments specified through **kwargs will override args stored in <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong>  Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong>  If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>.</p></li>
<li><p><strong>hparams_file</strong>  <p>Optional path to a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> or <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely wont need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights dont have the hyperparameters saved,
use this method to pass in a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file with the hparams youd like to use.
These will be converted into a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your models <code class="docutils literal notranslate"><span class="pre">hparams</span></code> argument is <a class="reference external" href="https://docs.python.org/3/library/argparse.html#argparse.Namespace" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code></a>
and <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file has hierarchical structure, you need to refactor your model to treat
<code class="docutils literal notranslate"><span class="pre">hparams</span></code> as <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
</p></li>
<li><p><strong>strict</strong>  Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this modules state dict. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> unless <code class="docutils literal notranslate"><span class="pre">LightningModule.strict_loading</span></code> is
set, in which case it defaults to the value of <code class="docutils literal notranslate"><span class="pre">LightningModule.strict_loading</span></code>.</p></li>
<li><p><strong>**kwargs</strong>  Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> is a <strong>class</strong> method. You should use your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>
<strong>class</strong> to call it instead of the <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance, or a
<code class="docutils literal notranslate"><span class="pre">TypeError</span></code> will be raised.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To ensure all layers can be loaded from the checkpoint, this function will call
<code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_model()</span></code> directly after instantiating the
model if this hook is overridden in your LightningModule. However, note that <code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> does
not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this
case, consider loading through the Trainer via <code class="docutils literal notranslate"><span class="pre">.fit(ckpt_path=...)</span></code>.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assign</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.load_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Copy parameters and buffers from <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this modules <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">assign</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> the optimizer must be created after
the call to <a class="reference internal" href="#stable_pretraining.module.Module.load_state_dict" title="stable_pretraining.module.Module.load_state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">load_state_dict</span></code></a> unless
<a class="reference external" href="https://docs.pytorch.org/docs/stable/future_mod.html#torch.__future__.get_swap_module_params_on_conversion" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_swap_module_params_on_conversion()</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  whether to strictly enforce that the keys
in <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this modules
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>assign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the properties of the tensors
in the current module are preserved whereas setting it to <code class="docutils literal notranslate"><span class="pre">True</span></code> preserves
properties of the Tensors in the state dict. The only
exception is the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> field of <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>
for which the value from the module is preserved. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> is a list of str containing any keys that are expected</dt><dd><p>by this module but missing from the provided <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> is a list of str containing the keys that are not</dt><dd><p>expected by this module but present in the provided <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#stable_pretraining.module.Module.load_state_dict" title="stable_pretraining.module.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Metric</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.log" title="Link to this definition">#</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is documented here: <span class="xref std std-ref">extensions/logging:Automatic Logging</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong>  key to log. Must be identical across all processes if using DDP or any other distributed strategy.</p></li>
<li><p><strong>value</strong>  value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, or a <code class="docutils literal notranslate"><span class="pre">Metric</span></code>.</p></li>
<li><p><strong>prog_bar</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress bar.</p></li>
<li><p><strong>logger</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><strong>on_step</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step. The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>on_epoch</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics. The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>reduce_fx</strong>  reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto detach the graph.</p></li>
<li><p><strong>sync_dist</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across devices. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong>  the DDP group to sync across.</p></li>
<li><p><strong>add_dataloader_idx</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple dataloaders). If False, user needs to give unique names for
each dataloader to not mix the values.</p></li>
<li><p><strong>batch_size</strong>  Current batch_size. This will be directly inferred from the loaded batch,
but for some data structures you might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong>  To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong>  Tells Lightning if you are calling <code class="docutils literal notranslate"><span class="pre">self.log</span></code> from every process (default) or only from
rank 0. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, you wont be able to use this metric as a monitor in callbacks
(e.g., early stopping). Warning: Improper use can lead to deadlocks! See
<span class="xref std std-ref">Advanced Logging</span> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Metric</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">MetricCollection</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.log_dict" title="Link to this definition">#</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong>  key value pairs.
Keys must be identical across all processes if using DDP or any other distributed strategy.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or <code class="docutils literal notranslate"><span class="pre">MetricCollection</span></code>.</p></li>
<li><p><strong>prog_bar</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress base.</p></li>
<li><p><strong>logger</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><strong>on_step</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for training_step but not validation/test_step.
The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>on_epoch</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for val/test step but not <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.
The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>reduce_fx</strong>  reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto-detach the graph</p></li>
<li><p><strong>sync_dist</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong>  the ddp group to sync across.</p></li>
<li><p><strong>add_dataloader_idx</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple). If <code class="docutils literal notranslate"><span class="pre">False</span></code>, user needs to give unique names for
each dataloader to not mix values.</p></li>
<li><p><strong>batch_size</strong>  Current batch size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong>  Tells Lightning if you are calling <code class="docutils literal notranslate"><span class="pre">self.log</span></code> from every process (default) or only from
rank 0. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, you wont be able to use this metric as a monitor in callbacks
(e.g., early stopping). Warning: Improper use can lead to deadlocks! See
<span class="xref std std-ref">Advanced Logging</span> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.lr_scheduler_step">
<span class="sig-name descname"><span class="pre">lr_scheduler_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.lr_scheduler_step" title="Link to this definition">#</a></dt>
<dd><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls
each scheduler. By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and as shown in the example for each scheduler based on
its <code class="docutils literal notranslate"><span class="pre">interval</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scheduler</strong>  Learning rate scheduler.</p></li>
<li><p><strong>metric</strong>  Value of the monitor used for schedulers like <code class="docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lr_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">metric</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>

<span class="c1"># Alternative way to update schedulers if it requires an epoch value</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lr_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">metric</span><span class="p">):</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.lr_schedulers" title="Link to this definition">#</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.manual_backward" title="Link to this definition">#</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#stable_pretraining.module.Module.training_step" title="stable_pretraining.module.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong>  The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong>  Additional positional arguments to be forwarded to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments to be forwarded to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.modules" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Module</em>  a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.mtia">
<span class="sig-name descname"><span class="pre">mtia</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.mtia"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.mtia" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the MTIA.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing the optimizer if the module will
live on MTIA while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.named_buffers" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module. Defaults to True.</p></li>
<li><p><strong>remove_duplicate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  whether to remove the duplicated buffers in the result. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>(str, torch.Tensor)</em>  Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_children"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.named_children" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>(str, Module)</em>  Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#set" title="(in Python v3.14)"><span class="pre">set</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.named_modules" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong>  a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong>  a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong>  whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>(str, Module)</em>  Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.named_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.named_parameters" title="Link to this definition">#</a></dt>
<dd><p>Override to globally exclude callback-related parameters.</p>
<p>Excludes parameters that belong to <code class="docutils literal notranslate"><span class="pre">self.callbacks_modules</span></code> or <code class="docutils literal notranslate"><span class="pre">self.callbacks_metrics</span></code>.
This prevents accidental optimization of callback/metric internals, even if external code
calls <code class="docutils literal notranslate"><span class="pre">self.parameters()</span></code> or <code class="docutils literal notranslate"><span class="pre">self.named_parameters()</span></code> directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>with_callbacks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If False, excludes callback parameters. Defaults to True.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  Prefix to prepend to parameter names. Defaults to .</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If True, yields parameters of this module and all submodules.
If False, yields only direct parameters. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>tuple[str, torch.nn.Parameter]</em>  Name and parameter pairs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_after_backward" title="Link to this definition">#</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_after_batch_transfer" title="Link to this definition">#</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.on_before_batch_transfer" title="stable_pretraining.module.Module.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.transfer_batch_to_device" title="stable_pretraining.module.Module.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_before_backward" title="Link to this definition">#</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>loss</strong>  Loss divided by number of batches for gradient accumulation and scaled if using AMP.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_before_batch_transfer" title="Link to this definition">#</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.on_after_batch_transfer" title="stable_pretraining.module.Module.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.transfer_batch_to_device" title="stable_pretraining.module.Module.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_before_optimizer_step" title="Link to this definition">#</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>If using gradient accumulation, the hook is called once the gradients have been accumulated.
See: <a href="#id7"><span class="problematic" id="id8">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  Current optimizer being used.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_before_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer for which grads should be zeroed.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_fit_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_fit_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_load_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called by Lightning to restore your model. If you saved something with <a class="reference internal" href="#stable_pretraining.module.Module.on_save_checkpoint" title="stable_pretraining.module.Module.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is
your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong>  Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of predict_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the prediction DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the predict loop starts.</p>
<p>The predict loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_predict_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.on_save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.on_save_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Offload checkpoint tensors to CPU to reduce GPU memory usage during save.</p>
<p>This method intercepts the checkpoint saving process and recursively moves all
PyTorch tensors (model weights, optimizer states, scheduler states) from GPU
to CPU before writing to disk. This prevents GPU OOM issues when checkpointing
large models (e.g., 2B+ parameters with optimizer states).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  Lightning checkpoint dictionary containing:
- state_dict: Model parameters (moved to CPU)
- optimizer_states: Optimizer state dicts (moved to CPU)
- lr_schedulers: LR scheduler states (moved to CPU)
- Other keys: Custom objects, metadata (left unchanged)</p>
</dd>
</dl>
<dl class="simple">
<dt>Behavior:</dt><dd><ul class="simple">
<li><p>Processes standard Lightning checkpoint keys (state_dict, optimizer_states, lr_schedulers)</p></li>
<li><p>Recursively traverses dicts, lists, and tuples to find tensors</p></li>
<li><p>Moves all torch.Tensor objects to CPU</p></li>
<li><p>Skips custom objects (returns unchanged)</p></li>
<li><p>Logs GPU memory freed and processing time</p></li>
<li><p>Non-destructive: Checkpoint loading/resuming works normally</p></li>
</ul>
</dd>
<dt>Side Effects:</dt><dd><ul class="simple">
<li><p>Modifies checkpoint dict in-place (tensors moved to CPU)</p></li>
<li><p>Temporarily increases CPU memory during offload</p></li>
<li><p>Adds ~2-5 seconds to checkpoint save time for 2B models</p></li>
<li><p>Frees ~8-12GB GPU memory for 2B model + optimizer states</p></li>
</ul>
</dd>
<dt>Custom Objects:</dt><dd><p>Custom objects in the checkpoint are NOT modified and will be logged as
warnings. These include: custom classes, numpy arrays, primitives, etc.
They are safely skipped and preserved in the checkpoint.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.14)"><strong>Exception</strong></a>  If tensor offload fails for any checkpoint key, logs error
    but allows checkpoint save to proceed (non-fatal).</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>For a 2B parameter model with AdamW optimizer:
- Before: ~12GB GPU memory spike on rank 0 during checkpoint save
- After: ~0.2GB GPU memory spike, ~10-12GB freed
- Checkpoint save time: +2-3 seconds
- Resume from checkpoint: Works normally, tensors auto-loaded to GPU</p>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>Only rank 0 saves checkpoints in DDP, so only rank 0 sees memory benefit</p></li>
<li><p>Does not affect checkpoint contents or ability to resume training</p></li>
<li><p>Safe for standard PyTorch/Lightning use cases</p></li>
<li><p>If using FSDP/DeepSpeed, consider strategy-specific checkpointing instead</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p>PyTorch Lightning ModelCheckpoint callback</p></li>
<li><p>torch.Tensor.cpu() for device transfer behavior</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of test_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the test loop starts.</p>
<p>The test loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior. See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_model_train()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_model_train" title="Link to this definition">#</a></dt>
<dd><p>Called when the test loop ends.</p>
<p>The test loop by default restores the <cite>training</cite> mode of the LightningModule to what it was before
starting testing. Override this hook to change the behavior. See also
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_model_eval()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_test_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_train_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of training_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">outputs[&quot;loss&quot;]</span></code> here will be the normalized value w.r.t <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> of the
loss returned from <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_train_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_train_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_train_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> and access them in this hook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># do something with all training_step outputs, for example:</span>
        <span class="n">epoch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;training_epoch_mean&quot;</span><span class="p">,</span> <span class="n">epoch_mean</span><span class="p">)</span>
        <span class="c1"># free up the memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_train_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.on_train_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.on_train_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of validation_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of validation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation loop starts.</p>
<p>The validation loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior. See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_validation_model_train()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_model_train" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation loop ends.</p>
<p>The validation loop by default restores the <cite>training</cite> mode of the LightningModule to what it was before
starting validation. Override this hook to change the behavior. See also
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_validation_model_eval()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_model_zero_grad">
<span class="sig-name descname"><span class="pre">on_validation_model_zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_model_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Called by the training loop to release gradients before entering the validation loop.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.on_validation_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of validation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.optimizer_step" title="Link to this definition">#</a></dt>
<dd><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls
the optimizer.</p>
<p>By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example.
This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) wont be called during the accumulation phase when
<code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>. Overriding this hook has no benefit with manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong>  Current epoch</p></li>
<li><p><strong>batch_idx</strong>  Index of current batch</p></li>
<li><p><strong>optimizer</strong>  A PyTorch optimizer</p></li>
<li><p><strong>optimizer_closure</strong>  The optimizer closure. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
    <span class="c1"># Add your custom logic to run directly before `optimizer.step()`</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># Add your custom logic to run directly after `optimizer.step()`</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.optimizer_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong>  Current epoch</p></li>
<li><p><strong>batch_idx</strong>  Index of current batch</p></li>
<li><p><strong>optimizer</strong>  A PyTorch optimizer</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance (not required on `torch&gt;=2.0.0`).</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code></a> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">_FabricOptimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><span class="pre">LightningOptimizer</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_FabricOptimizer</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#stable_pretraining.module.Module.optimizers" title="Link to this definition">#</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong>  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision,
profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the
<code class="docutils literal notranslate"><span class="pre">step</span></code> method and custom optimizers that dont have this method are not supported.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.parameters" title="Link to this definition">#</a></dt>
<dd><p>Override to route through the filtered <code class="docutils literal notranslate"><span class="pre">named_parameters</span></code> implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>with_callbacks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If False, excludes callback parameters. Defaults to True.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If True, yields parameters of this module and all submodules.
If False, yields only direct parameters. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>torch.nn.Parameter</em>  Module parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.predict_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying prediction samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>Its recommended that all data downloads and preparation happen in <a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.predict_step" title="Link to this definition">#</a></dt>
<dd><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>. By default, it calls
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>. Override to add any processing logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(strategy=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;tpu&quot;,</span> <span class="pre">devices=8)</span></code> as predictions wont be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Predicted output (optional).</p>
</dd>
</dl>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.prepare_data" title="Link to this definition">#</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.print" title="Link to this definition">#</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong>  The thing to print. The same as for Pythons built-in print function.</p></li>
<li><p><strong>**kwargs</strong>  The same as for Pythons built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_backward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward hook on the module.</p>
<p>This function is deprecated in favor of <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_buffer" title="Link to this definition">#</a></dt>
<dd><p>Add a buffer to the module.</p>
<p>This is typically used to register a buffer that should not be
considered a model parameter. For example, BatchNorms <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the modules state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this modules
<a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>)  buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#stable_pretraining.module.Module.cuda" title="stable_pretraining.module.Module.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the modules <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether the buffer is part of this modules
<a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">always_call</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_forward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_forward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#stable_pretraining.module.Module.forward" title="stable_pretraining.module.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> or not specified, the input contains only
the positional arguments given to the module. Keyword arguments wont be
passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>. The hook can modify the
output. It can modify the input inplace but it will not have effect on
forward since this is called after <a class="reference internal" href="#stable_pretraining.module.Module.forward" title="stable_pretraining.module.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called. The hook
should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the forward hook will be passed the
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> given to the forward function and be expected to return the
output possibly modified. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired
before all existing <code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks on
this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_hook()</span></code> will fire before all hooks
registered by this method.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>with_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be passed the
kwargs given to the forward function.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>always_call</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code> the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be run regardless of
whether an exception is raised while calling the Module.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_forward_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_forward_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#stable_pretraining.module.Module.forward" title="stable_pretraining.module.Module.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is false or not specified, the input contains only
the positional arguments given to the module. Keyword arguments wont be
passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>. The hook can modify the
input. User can either return a tuple or a single modified value in the
hook. We will wrap the value into a tuple if a single value is returned
(unless that value is already a tuple). The hook should have the
following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is true, the forward pre-hook will be passed the
kwargs given to the forward function. And if the hook modifies the
input, both the args and kwargs should be returned. The hook should have
the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">a</span> <span class="nb">tuple</span> <span class="n">of</span> <span class="n">modified</span> <span class="nb">input</span> <span class="ow">and</span> <span class="n">kwargs</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks
on this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_pre_hook()</span></code> will fire before all
hooks registered by this method.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>with_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be passed the kwargs
given to the forward function.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_full_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_full_backward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.</p></li>
<li><p>If none of the module inputs require gradients, the hook will fire when the gradients are computed
with respect to module outputs.</p></li>
<li><p>If none of the module outputs require gradients, then the hooks will not fire.</p></li>
</ol>
</div></blockquote>
<p>The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Modules forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user-defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks on
this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_hook()</span></code> will fire before
all hooks registered by this method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_full_backward_pre_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_full_backward_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_full_backward_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward pre-hook on the module.</p>
<p>The hook will be called every time the gradients for the module are computed.
The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> is a tuple. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the output that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> in
subsequent computations. Entries in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for
all non-Tensor arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Modules forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user-defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks
on this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_pre_hook()</span></code> will fire before
all hooks registered by this method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_load_state_dict_post_hook">
<span class="sig-name descname"><span class="pre">register_load_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_load_state_dict_post_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_load_state_dict_post_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a post-hook to be run after modules <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> is called.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">module</span></code> argument is the current module that this hook is registered
on, and the <code class="docutils literal notranslate"><span class="pre">incompatible_keys</span></code> argument is a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> consisting
of attributes <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>. <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code>
is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the missing keys and
<code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the unexpected keys.</p>
<p>The given incompatible_keys can be modified inplace if needed.</p>
<p>Note that the checks performed when calling <a class="reference internal" href="#stable_pretraining.module.Module.load_state_dict" title="stable_pretraining.module.Module.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> with
<code class="docutils literal notranslate"><span class="pre">strict=True</span></code> are affected by modifications the hook makes to
<code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> or <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>, as expected. Additions to either
set of keys will result in an error being thrown when <code class="docutils literal notranslate"><span class="pre">strict=True</span></code>, and
clearing out both missing and unexpected keys will avoid an error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_load_state_dict_pre_hook">
<span class="sig-name descname"><span class="pre">register_load_state_dict_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_load_state_dict_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_load_state_dict_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a pre-hook to be run before modules <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> is called.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hook</strong> (<em>Callable</em>)  Callable hook that will be invoked before
loading the state dict.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_module" title="Link to this definition">#</a></dt>
<dd><p>Alias for <a class="reference internal" href="#stable_pretraining.module.Module.add_module" title="stable_pretraining.module.Module.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="(in PyTorch v2.9)"><span class="pre">Parameter</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_parameter" title="Link to this definition">#</a></dt>
<dd><p>Add a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>)  parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#stable_pretraining.module.Module.cuda" title="stable_pretraining.module.Module.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
modules <a class="reference internal" href="#stable_pretraining.module.Module.state_dict" title="stable_pretraining.module.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_state_dict_post_hook">
<span class="sig-name descname"><span class="pre">register_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_state_dict_post_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_state_dict_post_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a post-hook for the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> method.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata) -&gt; None</p>
</dd>
</dl>
<p>The registered hooks can modify the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> inplace.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.register_state_dict_pre_hook">
<span class="sig-name descname"><span class="pre">register_state_dict_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_state_dict_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.register_state_dict_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a pre-hook for the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> method.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, prefix, keep_vars) -&gt; None</p>
</dd>
</dl>
<p>The registered hooks can be used to perform pre-processing before the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>
call is made.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.requires_grad_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.requires_grad_" title="Link to this definition">#</a></dt>
<dd><p>Change if autograd should record operations on parameters in this module.</p>
<p>This method sets the parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc" title="(in PyTorch v2.9)"><span>Locally disabling gradient computation</span></a> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.rescale_loss_for_grad_acc">
<span class="sig-name descname"><span class="pre">rescale_loss_for_grad_acc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.rescale_loss_for_grad_acc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.rescale_loss_for_grad_acc" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence" title="(in Python v3.14)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FrameType</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.save_hyperparameters" title="Link to this definition">#</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong>  single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong>  an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong>  a frame object. Default is None</p></li>
<li><p><strong>logger</strong>  Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.set_extra_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.set_extra_state" title="Link to this definition">#</a></dt>
<dd><p>Set extra state contained in the loaded <cite>state_dict</cite>.</p>
<p>This function is called from <a class="reference internal" href="#stable_pretraining.module.Module.load_state_dict" title="stable_pretraining.module.Module.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#stable_pretraining.module.Module.get_extra_state" title="stable_pretraining.module.Module.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.set_submodule">
<span class="sig-name descname"><span class="pre">set_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.set_submodule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.set_submodule" title="Link to this definition">#</a></dt>
<dd><p>Set the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> (default), the method will replace an existing submodule
or create a new submodule if the parent module exists. If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the method will only attempt to replace an existing submodule and throw an error if
the submodule does not exist.</p>
</div>
<p>For example, lets say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(3, 3, 3)
        )
        (linear): Linear(3, 3)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To override the <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> with a new submodule <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, you
could call <code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.net_c.conv&quot;,</span> <span class="pre">nn.Linear(1,</span> <span class="pre">1))</span></code>
where <code class="docutils literal notranslate"><span class="pre">strict</span></code> could be <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>To add a new submodule <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> to the existing <code class="docutils literal notranslate"><span class="pre">net_b</span></code> module,
you would call <code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.conv&quot;,</span> <span class="pre">nn.Conv2d(1,</span> <span class="pre">1,</span> <span class="pre">1))</span></code>.</p>
<p>In the above if you set <code class="docutils literal notranslate"><span class="pre">strict=True</span></code> and call
<code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.conv&quot;,</span> <span class="pre">nn.Conv2d(1,</span> <span class="pre">1,</span> <span class="pre">1),</span> <span class="pre">strict=True)</span></code>, an AttributeError
will be raised because <code class="docutils literal notranslate"><span class="pre">net_b</span></code> does not have a submodule named <code class="docutils literal notranslate"><span class="pre">conv</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong>  The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p></li>
<li><p><strong>module</strong>  The module to set the submodule to.</p></li>
<li><p><strong>strict</strong>  If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the method will replace an existing submodule
or create a new submodule if the parent module exists. If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the method will only attempt to replace an existing submodule and throw an error
if the submodule doesnt already exist.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a>  If the <code class="docutils literal notranslate"><span class="pre">target</span></code> string is empty or if <code class="docutils literal notranslate"><span class="pre">module</span></code> is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If at any point along the path resulting from
    the <code class="docutils literal notranslate"><span class="pre">target</span></code> string the (sub)path resolves to a non-existent
    attribute name or an object that is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.setup" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you
need to build models dynamically or adjust something about them. This hook is called on every process when
using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong>  either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.share_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.share_memory" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.state_dict" title="Link to this definition">#</a></dt>
<dd><p>Return a dictionary containing references to the whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned object is a shallow copy. It contains references
to the modules parameters and buffers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> also accepts positional arguments for
<code class="docutils literal notranslate"><span class="pre">destination</span></code>, <code class="docutils literal notranslate"><span class="pre">prefix</span></code> and <code class="docutils literal notranslate"><span class="pre">keep_vars</span></code> in order. However,
this is being deprecated and keyword arguments will be enforced in
future releases.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please avoid the use of argument <code class="docutils literal notranslate"><span class="pre">destination</span></code> as it is not
designed for end-users.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>optional</em>)  If provided, the state of module will
be updated into the dict and the same object is returned.
Otherwise, an <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code> will be created and returned.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  a prefix added to parameter and buffer
names to compose the keys in state_dict. Default: <code class="docutils literal notranslate"><span class="pre">''</span></code>.</p></li>
<li><p><strong>keep_vars</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  by default the <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> s
returned in the state dict are detached from autograd. If its
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, detaching will not be performed.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.teardown" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, or predict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong>  either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.test_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying test samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need a test dataset and a <a class="reference internal" href="#stable_pretraining.module.Module.test_step" title="stable_pretraining.module.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you dont need to implement
this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.test_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the test set. In this step youd normally generate examples or
calculate anything of interest such as accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#stable_pretraining.module.Module.test_step" title="stable_pretraining.module.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;test_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;test_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need to test you dont need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#stable_pretraining.module.Module.test_step" title="stable_pretraining.module.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.to" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.to()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.to_empty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.to_empty" title="Link to this definition">#</a></dt>
<dd><p>Move the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>)  The desired device of the parameters
and buffers in this module.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  Whether parameters and buffers of submodules should
be recursively moved to the specified device.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/io.html#io.BytesIO" title="(in Python v3.14)"><span class="pre">BytesIO</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ONNXProgram</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.to_onnx" title="Link to this definition">#</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong>  The path of the file the onnx model should be saved to. Default: None (no file saved).</p></li>
<li><p><strong>input_sample</strong>  An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong>  Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="s2">&quot;export.onnx&quot;</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ScriptModule</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ScriptModule</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#stable_pretraining.module.Module.to_torchscript" title="Link to this definition">#</a></dt>
<dd><p>By default compiles the whole model to a <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code></a>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code> set. If you would like to customize the modules that are
scripted you should override this method. In case you want to return multiple modules, we recommend using a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong>  Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong>  Whether to use TorchScripts script or trace method. Default: script</p></li>
<li><p><strong>example_inputs</strong>  An input to be used to do tracing when method is set to trace.
Default: None (uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code>)</p></li>
<li><p><strong>**kwargs</strong>  Additional arguments that will be passed to the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code></a> or
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code></a> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <a class="reference external" href="https://docs.pytorch.org/docs/stable/jit.html#module-torch.jit" title="(in PyTorch v2.9)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code></a>
documentation for supported features.</p></li>
</ul>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span>
    <span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.toggle_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Makes sure only the gradients of the current optimizers parameters are calculated in the training step to
prevent dangling gradients in multiple-optimizer setup.</p>
<p>It works with <a class="reference internal" href="#stable_pretraining.module.Module.untoggle_optimizer" title="stable_pretraining.module.Module.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to toggle.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.toggled_optimizer">
<span class="sig-name descname"><span class="pre">toggled_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Generator" title="(in Python v3.14)"><span class="pre">Generator</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.toggled_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Makes sure only the gradients of the current optimizers parameters are calculated in the training step to
prevent dangling gradients in multiple-optimizer setup. Combines <a class="reference internal" href="#stable_pretraining.module.Module.toggle_optimizer" title="stable_pretraining.module.Module.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a> and
<a class="reference internal" href="#stable_pretraining.module.Module.untoggle_optimizer" title="stable_pretraining.module.Module.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> into context manager.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to toggle.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">toggled_optimizer</span><span class="p">(</span><span class="n">opt</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.train" title="Link to this definition">#</a></dt>
<dd><p>Set the module in training mode.</p>
<p>This has an effect only on certain modules. See the documentation of
particular modules for details of their behaviors in training/evaluation
mode, i.e., whether they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.train_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id9"><span class="problematic" id="id10">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.training_step" title="Link to this definition">#</a></dt>
<dd><p>Manual optimization training step with support for multiple optimizers.</p>
<p>Expected output from forward during training (stage=fit):
- state[loss]: torch.Tensor - Single joint loss for all optimizers</p>
<p>When multiple optimizers are configured, the same loss is used for all of them.
Each optimizer updates its assigned parameters based on gradients from this joint loss.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.transfer_batch_to_device" title="Link to this definition">#</a></dt>
<dd><p>Override this hook if your <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> returns tensors wrapped in a custom data
structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> or anything that implements <cite>.to()</cite></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong>  The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.9)"><span class="pre">dtype</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.type" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.type()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.unfreeze" title="Link to this definition">#</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.untoggle_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#stable_pretraining.module.Module.toggle_optimizer" title="stable_pretraining.module.Module.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to untoggle.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.module.Module.val_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id11"><span class="problematic" id="id12">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>Its recommended that all data downloads and preparation happen in <a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.prepare_data" title="stable_pretraining.module.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.module.Module.setup" title="stable_pretraining.module.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need a validation dataset and a <a class="reference internal" href="#stable_pretraining.module.Module.validation_step" title="stable_pretraining.module.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you dont need to
implement this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.validation_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the validation set. In this step youd might generate examples or
calculate anything of interest like accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#stable_pretraining.module.Module.validation_step" title="stable_pretraining.module.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;val_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;val_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need to validate you dont need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#stable_pretraining.module.Module.validation_step" title="stable_pretraining.module.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.xpu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.xpu" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.module.Module" title="stable_pretraining.module.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.module.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.module.Module.zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Reset gradients of all model parameters.</p>
<p>See similar function under <a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  instead of setting to zero, set the grads to None.
See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code></a> for details.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>
<section id="module-stable_pretraining.run">
<span id="stable-pretraining-run-module"></span><h2>stable_pretraining.run module<a class="headerlink" href="#module-stable_pretraining.run" title="Link to this heading">#</a></h2>
<p>Universal experiment runner for stable-pretraining using Hydra configs.</p>
<p>This script provides a unified entry point for all experiments (training, evaluation, etc.)
via configuration files. It supports both single-file configs and modular Hydra composition.</p>
<dl>
<dt>Usage:</dt><dd><p># Run with a config file
python -m stable_pretraining.run config-path ../examples config-name simclr_cifar10</p>
<p># Run with config and override parameters
python -m stable_pretraining.run config-path ../examples config-name simclr_cifar10         module.optimizer.lr=0.01         trainer.max_epochs=200</p>
<p># Run hyperparameter sweep
python -m stable_pretraining.run multirun         config-path ../examples config-name simclr_cifar10         module.optimizer.lr=0.001,0.01,0.1</p>
</dd>
</dl>
<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.run.main">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.run.</span></span><span class="sig-name descname"><span class="pre">main</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DictConfig</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/run/#main"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.run.main" title="Link to this definition">#</a></dt>
<dd><p>Main execution function that instantiates components from config and runs the experiment.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>cfg</strong>  Hydra configuration dictionary containing all experiment parameters</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.run.print_config">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.run.</span></span><span class="sig-name descname"><span class="pre">print_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">cfg</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">DictConfig</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/run/#print_config"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.run.print_config" title="Link to this definition">#</a></dt>
<dd><p>Print configuration only on rank 0.</p>
</dd></dl>

</section>
<section id="module-stable_pretraining.static">
<span id="stable-pretraining-static-module"></span><h2>stable_pretraining.static module<a class="headerlink" href="#module-stable_pretraining.static" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.static.</span></span><span class="sig-name descname"><span class="pre">MetaStatic</span></span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#type" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">type</span></code></a></p>
<p>Metaclass that enables dict-like behavior on the TIMM_PARAMETERS class.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.clear">
<span class="sig-name descname"><span class="pre">clear</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.clear"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.clear" title="Link to this definition">#</a></dt>
<dd><p>Clear all data.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.get">
<span class="sig-name descname"><span class="pre">get</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">key</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">default</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.get"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.get" title="Link to this definition">#</a></dt>
<dd><p>Get value with optional default.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.items">
<span class="sig-name descname"><span class="pre">items</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.items"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.items" title="Link to this definition">#</a></dt>
<dd><p>Return a view of the items (with copied values).</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.keys">
<span class="sig-name descname"><span class="pre">keys</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.keys"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.keys" title="Link to this definition">#</a></dt>
<dd><p>Return a view of the keys.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.update">
<span class="sig-name descname"><span class="pre">update</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">other</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.update"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.update" title="Link to this definition">#</a></dt>
<dd><p>Update from another dict or iterable of key-value pairs.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.static.MetaStatic.values">
<span class="sig-name descname"><span class="pre">values</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#MetaStatic.values"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.MetaStatic.values" title="Link to this definition">#</a></dt>
<dd><p>Return a view of the values (as copies to prevent mutation).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.static.TIMM_EMBEDDINGS">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.static.</span></span><span class="sig-name descname"><span class="pre">TIMM_EMBEDDINGS</span></span><a class="reference internal" href="../_modules/stable_pretraining/static/#TIMM_EMBEDDINGS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.TIMM_EMBEDDINGS" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Thread-safe, lazy-loaded registry for TIMM (PyTorch Image Models) embedding names, accessed via class-level indexing.</p>
<p>This class provides a mapping from string keys to lists of embedding names, loaded on first access from a
JSON file located at assets/static_timm.json relative to the source file. The data is cached as a class
attribute after the first load, and subsequent accesses are served from memory.
The class is intended to be used as a static registry, e.g.:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">names</span> <span class="o">=</span> <span class="n">TIMM_EMBEDDINGS</span><span class="p">[</span><span class="s2">&quot;resnet50&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">names</span><span class="p">)</span>  <span class="c1"># List of embedding names for &#39;resnet50&#39;</span>
</pre></div>
</div>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>The data is loaded only once per process and is shared across all uses of the class.</p></li>
<li><p>Thread-safe: concurrent first-time access is protected by a class-level lock.</p></li>
<li><p>The class depends on the presence of the assets/static_timm.json file two directories above the source file.</p></li>
<li><p>The class assumes the <cite>__file__</cite> attribute is available and points to the current file.</p></li>
<li><p>The class attribute <cite>_data</cite> is private and shared.</p></li>
<li><p>Logging and printing occur on first load for debugging.</p></li>
<li><p>File system access and JSON parsing are required at runtime.</p></li>
</ul>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.14)"><strong>RuntimeError</strong></a>  If the assets file is missing.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#OSError" title="(in Python v3.14)"><strong>OSError</strong></a><strong>, </strong><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#IOError" title="(in Python v3.14)"><strong>IOError</strong></a>  If the file cannot be read.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/json.html#json.JSONDecodeError" title="(in Python v3.14)"><strong>json.JSONDecodeError</strong></a>  If the file is not valid JSON.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.14)"><strong>KeyError</strong></a>  If the requested key is not present in the data.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">TIMM_EMBEDDINGS</span><span class="p">[</span><span class="s2">&quot;vit_base_patch16_224&quot;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="stable_pretraining.static.TIMM_EMBEDDINGS.data">
<span class="sig-name descname"><span class="pre">data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#stable_pretraining.static.TIMM_EMBEDDINGS.data" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.static.TIMM_PARAMETERS">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.static.</span></span><span class="sig-name descname"><span class="pre">TIMM_PARAMETERS</span></span><a class="reference internal" href="../_modules/stable_pretraining/static/#TIMM_PARAMETERS"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.TIMM_PARAMETERS" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></a></p>
<p>Thread-safe singleton class for accessing TIMM (Timm Image Models) parameters.</p>
<p>This class provides lazy-loaded, cached access to TIMM model parameters stored
in a static JSON file. It implements a dict-like interface with thread-safe
initialization and defensive copying to prevent mutation of cached data.</p>
<dl>
<dt>Usage:</dt><dd><p># Access parameters by key
params = TIMM_PARAMETERS[model_name]</p>
<p># Iterate over keys
for key in TIMM_PARAMETERS.keys():</p>
<blockquote>
<div><p>print(key)</p>
</div></blockquote>
<p># Iterate over values
for values in TIMM_PARAMETERS.values():</p>
<blockquote>
<div><p>print(values)</p>
</div></blockquote>
<p># Iterate over items
for key, values in TIMM_PARAMETERS.items():</p>
<blockquote>
<div><p>print(f{key}: {values})</p>
</div></blockquote>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All methods return copies of the data to prevent accidental mutation
of the internal cache.</p>
</div>
<dl class="py attribute">
<dt class="sig sig-object py" id="stable_pretraining.static.TIMM_PARAMETERS.data">
<span class="sig-name descname"><span class="pre">data</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></em><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">None</span></em><a class="headerlink" href="#stable_pretraining.static.TIMM_PARAMETERS.data" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py function">
<dt class="sig sig-object py" id="stable_pretraining.static.count_parameters">
<span class="sig-prename descclassname"><span class="pre">stable_pretraining.static.</span></span><span class="sig-name descname"><span class="pre">count_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">model</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/static/#count_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.static.count_parameters" title="Link to this definition">#</a></dt>
<dd><p>Count trainable parameters efficiently.</p>
</dd></dl>

</section>
<section id="module-stable_pretraining">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-stable_pretraining" title="Link to this heading">#</a></h2>
<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.EarlyStopping">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">EarlyStopping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'min'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">milestones</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">patience</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/utils/#EarlyStopping"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.EarlyStopping" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Early stopping mechanism with support for metric milestones and patience.</p>
<p>This module provides flexible early stopping capabilities that can halt training
based on metric performance. It supports both milestone-based stopping (stop if
metric doesnt reach target by specific epochs) and patience-based stopping
(stop if metric doesnt improve for N epochs).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>mode</strong>  Optimization direction - min for metrics to minimize (e.g., loss),
max for metrics to maximize (e.g., accuracy).</p></li>
<li><p><strong>milestones</strong>  Dict mapping epoch numbers to target metric values. Training
stops if targets are not met at specified epochs.</p></li>
<li><p><strong>metric_name</strong>  Name of the metric to monitor if metric is a dict.</p></li>
<li><p><strong>patience</strong>  Number of epochs with no improvement before stopping.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">,</span> <span class="n">milestones</span><span class="o">=</span><span class="p">{</span><span class="mi">10</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">20</span><span class="p">:</span> <span class="mf">0.9</span><span class="p">})</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Stops if accuracy &lt; 0.8 at epoch 10 or &lt; 0.9 at epoch 20</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.EarlyStopping.should_stop">
<span class="sig-name descname"><span class="pre">should_stop</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">metric</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">step</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/utils/#EarlyStopping.should_stop"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.EarlyStopping.should_stop" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.ImageRetrieval">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">ImageRetrieval</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">query_col</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">retrieval_col</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.14)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">features_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">normalizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/image_retrieval/#ImageRetrieval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.ImageRetrieval" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Image Retrieval evaluator for self-supervised learning.</p>
<dl class="simple">
<dt>The implementation follows:</dt><dd><ol class="arabic simple">
<li><p><a class="github reference external" href="https://github.com/facebookresearch/dino/blob/main/eval_image_retrieval.py">facebookresearch/dino</a></p></li>
</ol>
</dd>
</dl>
<dl class="py attribute">
<dt class="sig sig-object py" id="stable_pretraining.ImageRetrieval.NAME">
<span class="sig-name descname"><span class="pre">NAME</span></span><em class="property"><span class="w"> </span><span class="p"><span class="pre">=</span></span><span class="w"> </span><span class="pre">'ImageRetrieval'</span></em><a class="headerlink" href="#stable_pretraining.ImageRetrieval.NAME" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.ImageRetrieval.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/image_retrieval/#ImageRetrieval.on_validation_epoch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the val epoch ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.ImageRetrieval.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/image_retrieval/#ImageRetrieval.on_validation_epoch_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called when the val epoch begins.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.ImageRetrieval.state_key">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">state_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#stable_pretraining.ImageRetrieval.state_key" title="Link to this definition">#</a></dt>
<dd><p>Unique identifier for this callbacks state during checkpointing.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.LiDAR">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">LiDAR</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.14)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_classes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">samples_per_class</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">10</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delta</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.0001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1e-08</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/lidar/#LiDAR"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.LiDAR" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>LiDAR (Linear Discriminant Analysis Rank) monitor using queue discovery.</p>
<p>LiDAR measures the effective rank of learned representations using Linear Discriminant
Analysis (LDA). It computes the exponential of the entropy of the eigenvalue distribution
from the LDA transformation, providing a metric between 1 and min(d, n_classes - 1) where
d is the feature dimension, indicating how many dimensions are effectively being used.</p>
<p>This implementation is based on Thilak et al. LiDAR: Sensing Linear Probing Performance
in Joint Embedding SSL Architectures (arXiv:2312.04000).</p>
<section id="important-surrogate-class-formation-requirement">
<h3>IMPORTANT: Surrogate Class Formation Requirement<a class="headerlink" href="#important-surrogate-class-formation-requirement" title="Link to this heading">#</a></h3>
<p>The LiDAR paper requires that each surrogate class consists of q augmented views
of the same clean sample. The current implementation chunks the queue sequentially
into groups of size samples_per_class. For faithful reproduction of the paper:</p>
<ul class="simple">
<li><p>Ensure the upstream queue pushes q contiguous augmentations of each clean sample</p></li>
<li><p>OR implement ID-based grouping to ensure each group contains views of the same sample</p></li>
</ul>
<p>Without proper grouping, the metric may not accurately reflect the papers methodology.</p>
<p>The metric helps detect:
- Dimensional collapse in self-supervised learning
- Loss of representational capacity
- Over-regularization effects</p>
<dl class="field-list simple">
<dt class="field-odd">param name<span class="colon">:</span></dt>
<dd class="field-odd"><p>Unique identifier for this callback instance</p>
</dd>
<dt class="field-even">param target<span class="colon">:</span></dt>
<dd class="field-even"><p>Key in batch dict containing the feature embeddings to monitor</p>
</dd>
<dt class="field-odd">param queue_length<span class="colon">:</span></dt>
<dd class="field-odd"><p>Size of the circular buffer for caching embeddings</p>
</dd>
<dt class="field-even">param target_shape<span class="colon">:</span></dt>
<dd class="field-even"><p>Shape of the target embeddings (e.g., 768 for 768-dim features)</p>
</dd>
<dt class="field-odd">param n_classes<span class="colon">:</span></dt>
<dd class="field-odd"><p>Number of surrogate classes (clean samples) for LDA computation</p>
</dd>
<dt class="field-even">param samples_per_class<span class="colon">:</span></dt>
<dd class="field-even"><p>Number of augmented samples per class</p>
</dd>
<dt class="field-odd">param delta<span class="colon">:</span></dt>
<dd class="field-odd"><p>Regularization constant added to within-class covariance (default: 1e-4)</p>
</dd>
<dt class="field-even">param epsilon<span class="colon">:</span></dt>
<dd class="field-even"><p>Small constant for numerical stability (default: 1e-8)</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.LiDAR.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/lidar/#LiDAR.on_validation_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.LiDAR.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Compute LiDAR metric on the first validation batch only.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.LiDAR.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/lidar/#LiDAR.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.LiDAR.setup" title="Link to this definition">#</a></dt>
<dd><p>Find or create the queue callback for target features.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.LiDAR.state_key">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">state_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#stable_pretraining.LiDAR.state_key" title="Link to this definition">#</a></dt>
<dd><p>Unique identifier for this callbacks state during checkpointing.</p>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.LoggingCallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">LoggingCallback</span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#LoggingCallback"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.LoggingCallback" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Displays validation metrics in a color-coded formatted table.</p>
<p>This callback creates a visually appealing table of all validation metrics
at the end of each validation epoch. Metrics are color-coded for better
readability in terminal outputs.</p>
<p>Features:
- Automatic sorting of metrics by name
- Color coding: blue for metric names, green for values
- Filters out internal metrics (log, progress_bar)</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.LoggingCallback.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#LoggingCallback.on_validation_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.LoggingCallback.on_validation_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation loop ends.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.Manager">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">Manager</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Checkpointable</span></code></p>
<p>Manages training with logging, scheduling, and checkpointing support.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.Trainer</em><em>]</em>)  PyTorch Lightning trainer configuration or instance.</p></li>
<li><p><strong>module</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.LightningModule</em><em>]</em>)  Lightning module configuration or instance.</p></li>
<li><p><strong>data</strong> (<em>Union</em><em>[</em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>DictConfig</em><em>, </em><em>pl.LightningDataModule</em><em>]</em>)  Data module configuration or instance.</p></li>
<li><p><strong>seed</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  Random seed for reproducibility. Defaults to None.</p></li>
<li><p><strong>ckpt_path</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  Path to checkpoint for resuming training. Defaults to last.</p></li>
<li><p><strong>compile</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  Should we compile the given module. Defaults to False.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Manager.init_and_sync_wandb">
<span class="sig-name descname"><span class="pre">init_and_sync_wandb</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.init_and_sync_wandb"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager.init_and_sync_wandb" title="Link to this definition">#</a></dt>
<dd><p>Handles some utilities for WandB.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.Manager.instantiated_data">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">instantiated_data</span></span><a class="headerlink" href="#stable_pretraining.Manager.instantiated_data" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.Manager.instantiated_module">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">instantiated_module</span></span><a class="headerlink" href="#stable_pretraining.Manager.instantiated_module" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Manager.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.predict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager.predict" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Manager.save_checkpoint">
<span class="sig-name descname"><span class="pre">save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">upload_wandb</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">verbose</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager.save_checkpoint" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Manager.test">
<span class="sig-name descname"><span class="pre">test</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.test"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager.test" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Manager.validate">
<span class="sig-name descname"><span class="pre">validate</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/manager/#Manager.validate"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Manager.validate" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.Module">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">Module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forward</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code></p>
<p>PyTorch Lightning module using manual optimization with multi-optimizer support.</p>
<p>Core usage
- Provide a custom <cite>forward(self, batch, stage)</cite> via the <cite>forward</cite> argument at init.
- During training, <cite>forward</cite> must return a dict with <cite>state[loss]</cite> (a single joint loss).</p>
<blockquote>
<div><p>When multiple optimizers are configured, this joint loss is used for all optimizers.</p>
</div></blockquote>
<p>Optimizer configuration (<cite>self.optim</cite>)
- Single optimizer:</p>
<blockquote>
<div><p>{optimizer: str|dict|partial|Class, scheduler: &lt;see below&gt;, interval: step|epoch, frequency: int}
- Optimizer accepted forms:</p>
<blockquote>
<div><ul class="simple">
<li><p>string name (e.g., AdamW, SGD) from torch.optim</p></li>
<li><p>dict: {type: AdamW, lr: 1e-3, }</p></li>
<li><p>functools.partial: partial(torch.optim.AdamW, lr=1e-3)</p></li>
<li><p>optimizer class: torch.optim.AdamW</p></li>
</ul>
</div></blockquote>
</div></blockquote>
<ul>
<li><p>Multiple optimizers:
{</p>
<blockquote>
<div><dl class="simple">
<dt>name: {</dt><dd><p>modules: regex,                # assign params by module-name pattern (children inherit)
optimizer: str|dict|partial|Class, # optimizer factory (same accepted forms as above)
scheduler: str|dict|partial|Class, # flexible scheduler config (see below)
interval: step|epoch,       # scheduler interval
frequency: int,                   # optimizer step frequency
monitor: str                      # (optional) for ReduceLROnPlateau; alternatively set inside scheduler dict</p>
</dd>
</dl>
<p>}, </p>
</div></blockquote>
<p>}</p>
</li>
</ul>
<p>Parameter assignment (multi-optimizer)
- Modules are matched by regex on their qualified name. Children inherit the parents assignment</p>
<blockquote>
<div><p>unless they match a more specific pattern. Only direct parameters of each module are collected
to avoid duplication.</p>
</div></blockquote>
<p>Schedulers (flexible)
- Accepted forms: string name (e.g., CosineAnnealingLR, StepLR), dict with {type: , },</p>
<blockquote>
<div><p>functools.partial, or a scheduler class. Smart defaults are applied when params are omitted for
common schedulers (CosineAnnealingLR, OneCycleLR, StepLR, ExponentialLR, ReduceLROnPlateau,
LinearLR, ConstantLR). For ReduceLROnPlateau, a <cite>monitor</cite> key is added (default: val_loss).
You may specify <cite>monitor</cite> either alongside the optimizer config (top level) or inside the
scheduler dict itself.</p>
</div></blockquote>
<ul class="simple">
<li><p>The resulting Lightning scheduler dict includes <cite>interval</cite> and <cite>frequency</cite> (or <cite>scheduler_frequency</cite>).</p></li>
</ul>
<p>Training loop behavior
- Manual optimization (<cite>automatic_optimization = False</cite>).
- Gradient accumulation: scales loss by 1/N where N = Trainer.accumulate_grad_batches and steps on the boundary.
- Per-optimizer step frequency: each optimizer steps only when its frequency boundary is met (in addition to accumulation boundary).
- Gradient clipping: uses Trainers <cite>gradient_clip_val</cite> and <cite>gradient_clip_algorithm</cite> before each step.
- Returns the <cite>state</cite> dict from <cite>forward</cite> unchanged for logging/inspection.</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.add_module">
<span class="sig-name descname"><span class="pre">add_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.add_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.add_module" title="Link to this definition">#</a></dt>
<dd><p>Add a child module to the current module.</p>
<p>The module can be accessed as an attribute using the given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the child module. The child module can be
accessed from this module using the given name</p></li>
<li><p><strong>module</strong> (<a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module"><em>Module</em></a>)  child module to be added to the module.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.all_gather">
<span class="sig-name descname"><span class="pre">all_gather</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">data</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_grads</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.all_gather" title="Link to this definition">#</a></dt>
<dd><p>Gather tensors or collections of tensors from multiple processes.</p>
<p>This method needs to be called on all processes and the tensors need to have the same shape across all
processes, otherwise your program will stall forever.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>data</strong>  int, float, tensor of shape (batch, ), or a (possibly nested) collection thereof.</p></li>
<li><p><strong>group</strong>  the process group to gather results from. Defaults to all processes (world)</p></li>
<li><p><strong>sync_grads</strong>  flag that allows users to synchronize gradients for the all_gather operation</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A tensor of shape (world_size, batch, ), or if the input was a collection
the output will also be a collection with tensors of this shape. For the special case where
world_size is 1, no additional dimension is added to the tensor(s).</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.apply">
<span class="sig-name descname"><span class="pre">apply</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.apply"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.apply" title="Link to this definition">#</a></dt>
<dd><p>Apply <code class="docutils literal notranslate"><span class="pre">fn</span></code> recursively to every submodule (as returned by <code class="docutils literal notranslate"><span class="pre">.children()</span></code>) as well as self.</p>
<p>Typical use includes initializing the parameters of a model
(see also <a class="reference external" href="https://docs.pytorch.org/docs/stable/nn.init.html#nn-init-doc" title="(in PyTorch v2.9)"><span>torch.nn.init</span></a>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>fn</strong> (<a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a> -&gt; None)  function to be applied to each submodule</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span><span class="w"> </span><span class="nf">init_weights</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">m</span><span class="p">)</span> <span class="o">==</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">init_weights</span><span class="p">)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[1., 1.],</span>
<span class="go">        [1., 1.]], requires_grad=True)</span>
<span class="go">Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">Parameter containing:</span>
<span class="go">tensor([[1., 1.],</span>
<span class="go">        [1., 1.]], requires_grad=True)</span>
<span class="go">Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.backward">
<span class="sig-name descname"><span class="pre">backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.backward" title="Link to this definition">#</a></dt>
<dd><p>Called to perform backward on the loss returned in <a class="reference internal" href="#stable_pretraining.Module.training_step" title="stable_pretraining.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. Override this hook with your own
implementation if you need to.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>loss</strong>  The loss tensor returned by <a class="reference internal" href="#stable_pretraining.Module.training_step" title="stable_pretraining.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a>. If gradient accumulation is used, the loss here
holds the normalized value (scaled by 1 / accumulation steps).</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.bfloat16">
<span class="sig-name descname"><span class="pre">bfloat16</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.bfloat16"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.bfloat16" title="Link to this definition">#</a></dt>
<dd><p>Casts all floating point parameters and buffers to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> datatype.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.buffers">
<span class="sig-name descname"><span class="pre">buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.buffers" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over module buffers.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module.</p>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>torch.Tensor</em>  module buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">buf</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">buf</span><span class="p">),</span> <span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L,)</span>
<span class="go">&lt;class &#39;torch.Tensor&#39;&gt; (20L, 1L, 5L, 5L)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.children">
<span class="sig-name descname"><span class="pre">children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.children"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.children" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over immediate children modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Module</em>  a child module</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.clip_gradients">
<span class="sig-name descname"><span class="pre">clip_gradients</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.clip_gradients" title="Link to this definition">#</a></dt>
<dd><p>Handles gradient clipping internally.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Do not override this method. If you want to customize gradient clipping, consider using
<a class="reference internal" href="#stable_pretraining.Module.configure_gradient_clipping" title="stable_pretraining.Module.configure_gradient_clipping"><code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_gradient_clipping()</span></code></a> method.</p></li>
<li><p>For manual optimization (<code class="docutils literal notranslate"><span class="pre">self.automatic_optimization</span> <span class="pre">=</span> <span class="pre">False</span></code>), if you want to use
gradient clipping, consider calling
<code class="docutils literal notranslate"><span class="pre">self.clip_gradients(opt,</span> <span class="pre">gradient_clip_val=0.5,</span> <span class="pre">gradient_clip_algorithm=&quot;norm&quot;)</span></code>
manually in the training step.</p></li>
</ul>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong>  Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong>  The value at which to clip gradients.</p></li>
<li><p><strong>gradient_clip_algorithm</strong>  The gradient clipping algorithm to use. Pass <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;value&quot;</span></code>
to clip by value, and <code class="docutils literal notranslate"><span class="pre">gradient_clip_algorithm=&quot;norm&quot;</span></code> to clip by norm.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.compile">
<span class="sig-name descname"><span class="pre">compile</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.compile"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.compile" title="Link to this definition">#</a></dt>
<dd><p>Compile this Modules forward using <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.</p>
<p>This Modules <cite>__call__</cite> method is compiled and all arguments are passed as-is
to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a>.</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.compile.html#torch.compile" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.compile()</span></code></a> for details on the arguments for this function.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.configure_callbacks">
<span class="sig-name descname"><span class="pre">configure_callbacks</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence" title="(in Python v3.14)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><span class="pre">Callback</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callback</span></span></span><a class="headerlink" href="#stable_pretraining.Module.configure_callbacks" title="Link to this definition">#</a></dt>
<dd><p>Configure model-specific callbacks. When the model gets attached, e.g., when <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> or <code class="docutils literal notranslate"><span class="pre">.test()</span></code> gets
called, the list or a callback returned here will be merged with the list of callbacks passed to the Trainers
<code class="docutils literal notranslate"><span class="pre">callbacks</span></code> argument. If a callback returned here has the same type as one or several callbacks already
present in the Trainers callbacks list, it will take priority and replace them. In addition, Lightning will
make sure <code class="xref py py-class docutils literal notranslate"><span class="pre">ModelCheckpoint</span></code> callbacks run last.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A callback or a list of callbacks which will extend the list of callbacks in the Trainer.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">configure_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">early_stop</span> <span class="o">=</span> <span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;max&quot;</span><span class="p">)</span>
    <span class="n">checkpoint</span> <span class="o">=</span> <span class="n">ModelCheckpoint</span><span class="p">(</span><span class="n">monitor</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">early_stop</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.configure_gradient_clipping">
<span class="sig-name descname"><span class="pre">configure_gradient_clipping</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.configure_gradient_clipping" title="Link to this definition">#</a></dt>
<dd><p>Perform gradient clipping for the optimizer parameters. Called before <a class="reference internal" href="#stable_pretraining.Module.optimizer_step" title="stable_pretraining.Module.optimizer_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">optimizer_step()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizer</strong>  Current optimizer being used.</p></li>
<li><p><strong>gradient_clip_val</strong>  The value at which to clip gradients. By default, value passed in Trainer
will be available here.</p></li>
<li><p><strong>gradient_clip_algorithm</strong>  The gradient clipping algorithm to use. By default, value
passed in Trainer will be available here.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">configure_gradient_clipping</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="p">,</span> <span class="n">gradient_clip_algorithm</span><span class="p">):</span>
    <span class="c1"># Implement your own custom logic to clip gradients</span>
    <span class="c1"># You can call `self.clip_gradients` with your settings:</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clip_gradients</span><span class="p">(</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
        <span class="n">gradient_clip_algorithm</span><span class="o">=</span><span class="n">gradient_clip_algorithm</span>
    <span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.configure_model">
<span class="sig-name descname"><span class="pre">configure_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.configure_model" title="Link to this definition">#</a></dt>
<dd><p>Hook to create modules in a strategy and precision aware context.</p>
<p>This is particularly useful for when using sharded strategies (FSDP and DeepSpeed), where wed like to shard
the model instantly to save memory and initialization time.
For non-sharded strategies, you can choose to override this hook or to initialize your model under the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">init_module()</span></code> context manager.</p>
<p>This hook is called during each of fit/val/test/predict stages in the same process, so ensure that
implementation of this hook is <strong>idempotent</strong>, i.e., after the first time the hook is called, subsequent calls
to it should be a no-op.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.configure_optimizers">
<span class="sig-name descname"><span class="pre">configure_optimizers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.configure_optimizers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.configure_optimizers" title="Link to this definition">#</a></dt>
<dd><p>Configure optimizers and schedulers for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Optimizer configuration with optional learning rate scheduler.
For single optimizer: Returns a dict with optimizer and lr_scheduler.
For multiple optimizers: Returns a tuple of (optimizers, schedulers).</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a> or <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)">tuple</a></p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>Multi-optimizer configuration with module pattern matching and schedulers:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Simple single optimizer with scheduler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="n">partial</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">),</span>
<span class="gp">... </span>    <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="s2">&quot;CosineAnnealingLR&quot;</span><span class="p">,</span>  <span class="c1"># Uses smart defaults</span>
<span class="gp">... </span>    <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># Multi-optimizer with custom scheduler configs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">optim</span> <span class="o">=</span> <span class="p">{</span>
<span class="gp">... </span>    <span class="s2">&quot;encoder_opt&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="s2">&quot;encoder&quot;</span><span class="p">,</span>  <span class="c1"># Matches &#39;encoder&#39; and all children</span>
<span class="gp">... </span>        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span> <span class="s2">&quot;lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;OneCycleLR&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;max_lr&quot;</span><span class="p">:</span> <span class="mf">1e-3</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;total_steps&quot;</span><span class="p">:</span> <span class="mi">10000</span><span class="p">,</span>
<span class="gp">... </span>        <span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;step&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span>    <span class="s2">&quot;head_opt&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>        <span class="s2">&quot;modules&quot;</span><span class="p">:</span> <span class="s2">&quot;.*head$&quot;</span><span class="p">,</span>  <span class="c1"># Matches modules ending with &#39;head&#39;</span>
<span class="gp">... </span>        <span class="s2">&quot;optimizer&quot;</span><span class="p">:</span> <span class="s2">&quot;SGD&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="p">{</span>
<span class="gp">... </span>            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;ReduceLROnPlateau&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;mode&quot;</span><span class="p">:</span> <span class="s2">&quot;max&quot;</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;patience&quot;</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span>
<span class="gp">... </span>            <span class="s2">&quot;factor&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
<span class="gp">... </span>        <span class="p">},</span>
<span class="gp">... </span>        <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_accuracy&quot;</span><span class="p">,</span>  <span class="c1"># Required for ReduceLROnPlateau</span>
<span class="gp">... </span>        <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
<span class="gp">... </span>        <span class="s2">&quot;frequency&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="p">},</span>
<span class="gp">... </span><span class="p">}</span>
</pre></div>
</div>
<p>With model structure:
- encoder                 -&gt; encoder_opt (matches encoder)
- encoder.layer1          -&gt; encoder_opt (inherits from parent)
- encoder.layer1.conv     -&gt; encoder_opt (inherits from encoder.layer1)
- classifier_head         -&gt; head_opt (matches .*head$)
- classifier_head.linear  -&gt; head_opt (inherits from parent)
- decoder                 -&gt; None (no match, no parameters collected)</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.configure_sharded_model">
<span class="sig-name descname"><span class="pre">configure_sharded_model</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.configure_sharded_model" title="Link to this definition">#</a></dt>
<dd><p>Deprecated.</p>
<p>Use <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_model()</span></code> instead.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.cpu">
<span class="sig-name descname"><span class="pre">cpu</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.cpu" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.cpu" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.cpu()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.cuda">
<span class="sig-name descname"><span class="pre">cuda</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.cuda" title="Link to this definition">#</a></dt>
<dd><p>Moves all model parameters and buffers to the GPU. This also makes associated parameters and buffers
different objects. So it should be called before constructing optimizer if the module will live on GPU while
being optimized.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong>  If specified, all parameters will be copied to that device. If <cite>None</cite>, the current CUDA device
index will be used.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.double">
<span class="sig-name descname"><span class="pre">double</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.double" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.double" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.double()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.eval">
<span class="sig-name descname"><span class="pre">eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.eval"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.eval" title="Link to this definition">#</a></dt>
<dd><p>Set the module in evaluation mode.</p>
<p>This has an effect only on certain modules. See the documentation of
particular modules for details of their behaviors in training/evaluation
mode, i.e. whether they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<p>This is equivalent with <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.train" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">self.train(False)</span></code></a>.</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc" title="(in PyTorch v2.9)"><span>Locally disabling gradient computation</span></a> for a comparison between
<cite>.eval()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.extra_repr">
<span class="sig-name descname"><span class="pre">extra_repr</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.extra_repr"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.extra_repr" title="Link to this definition">#</a></dt>
<dd><p>Return the extra representation of the module.</p>
<p>To print customized extra information, you should re-implement
this method in your own modules. Both single-line and multi-line
strings are acceptable.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.float">
<span class="sig-name descname"><span class="pre">float</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.float" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.float" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.float()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.forward" title="Link to this definition">#</a></dt>
<dd><p>Same as <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.forward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.forward()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong>  Whatever you decide to pass into the forward method.</p></li>
<li><p><strong>**kwargs</strong>  Keyword arguments are also possible.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Your models output</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.freeze">
<span class="sig-name descname"><span class="pre">freeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.freeze" title="Link to this definition">#</a></dt>
<dd><p>Freeze all params for inference.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.get_buffer">
<span class="sig-name descname"><span class="pre">get_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.get_buffer" title="Link to this definition">#</a></dt>
<dd><p>Return the buffer given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this methods functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the buffer
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The buffer referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)">torch.Tensor</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If the target string references an invalid
    path or resolves to something that is not a
    buffer</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.get_extra_state">
<span class="sig-name descname"><span class="pre">get_extra_state</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_extra_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.get_extra_state" title="Link to this definition">#</a></dt>
<dd><p>Return any extra state to include in the modules state_dict.</p>
<p>Implement this and a corresponding <a class="reference internal" href="#stable_pretraining.Module.set_extra_state" title="stable_pretraining.Module.set_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">set_extra_state()</span></code></a> for your module
if you need to store extra state. This function is called when building the
modules <cite>state_dict()</cite>.</p>
<p>Note that extra state should be picklable to ensure working serialization
of the state_dict. We only provide backwards compatibility guarantees
for serializing Tensors; other objects may break backwards compatibility if
their serialized pickled form changes.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>Any extra state to store in the modules state_dict</p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.14)">object</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.get_parameter">
<span class="sig-name descname"><span class="pre">get_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="(in PyTorch v2.9)"><span class="pre">Parameter</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.get_parameter" title="Link to this definition">#</a></dt>
<dd><p>Return the parameter given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>See the docstring for <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for a more detailed
explanation of this methods functionality as well as how to
correctly specify <code class="docutils literal notranslate"><span class="pre">target</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the Parameter
to look for. (See <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The Parameter referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p>torch.nn.Parameter</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If the target string references an invalid
    path or resolves to something that is not an
    <code class="docutils literal notranslate"><span class="pre">nn.Parameter</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.get_submodule">
<span class="sig-name descname"><span class="pre">get_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.get_submodule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.get_submodule" title="Link to this definition">#</a></dt>
<dd><p>Return the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<p>For example, lets say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
        )
        (linear): Linear(in_features=100, out_features=200, bias=True)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> which has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To check whether or not we have the <code class="docutils literal notranslate"><span class="pre">linear</span></code> submodule, we
would call <code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.linear&quot;)</span></code>. To check whether
we have the <code class="docutils literal notranslate"><span class="pre">conv</span></code> submodule, we would call
<code class="docutils literal notranslate"><span class="pre">get_submodule(&quot;net_b.net_c.conv&quot;)</span></code>.</p>
<p>The runtime of <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> is bounded by the degree
of module nesting in <code class="docutils literal notranslate"><span class="pre">target</span></code>. A query against
<code class="docutils literal notranslate"><span class="pre">named_modules</span></code> achieves the same result, but it is O(N) in
the number of transitive modules. So, for a simple check to see
if some submodule exists, <code class="docutils literal notranslate"><span class="pre">get_submodule</span></code> should always be
used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>target</strong>  The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>The submodule referenced by <code class="docutils literal notranslate"><span class="pre">target</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)">torch.nn.Module</a></p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If at any point along the path resulting from
    the target string the (sub)path resolves to a non-existent
    attribute name or an object that is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.half">
<span class="sig-name descname"><span class="pre">half</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.half" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.half" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.half()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.ipu">
<span class="sig-name descname"><span class="pre">ipu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.ipu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.ipu" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the IPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing the optimizer if the module will
live on IPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.load_from_checkpoint">
<span class="sig-name descname"><span class="pre">load_from_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.IO" title="(in Python v3.14)"><span class="pre">IO</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">map_location</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/storage.html#torch.UntypedStorage" title="(in PyTorch v2.9)"><span class="pre">UntypedStorage</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/storage.html#torch.UntypedStorage" title="(in PyTorch v2.9)"><span class="pre">UntypedStorage</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">hparams_file</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.load_from_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments
passed to <code class="docutils literal notranslate"><span class="pre">__init__</span></code>  in the checkpoint under <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<p>Any arguments specified through **kwargs will override args stored in <code class="docutils literal notranslate"><span class="pre">&quot;hyper_parameters&quot;</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>checkpoint_path</strong>  Path to checkpoint. This can also be a URL, or file-like object</p></li>
<li><p><strong>map_location</strong>  If your checkpoint saved a GPU model and you now load on CPUs
or a different number of GPUs, use this to map to the new setup.
The behaviour is the same as in <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.load.html#torch.load" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.load()</span></code></a>.</p></li>
<li><p><strong>hparams_file</strong>  <p>Optional path to a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> or <code class="docutils literal notranslate"><span class="pre">.csv</span></code> file with hierarchical structure
as in this example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">drop_prob</span><span class="p">:</span> <span class="mf">0.2</span>
<span class="n">dataloader</span><span class="p">:</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="mi">32</span>
</pre></div>
</div>
<p>You most likely wont need this since Lightning will always save the hyperparameters
to the checkpoint.
However, if your checkpoint weights dont have the hyperparameters saved,
use this method to pass in a <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file with the hparams youd like to use.
These will be converted into a <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a> and passed into your
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> for use.</p>
<p>If your models <code class="docutils literal notranslate"><span class="pre">hparams</span></code> argument is <a class="reference external" href="https://docs.python.org/3/library/argparse.html#argparse.Namespace" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Namespace</span></code></a>
and <code class="docutils literal notranslate"><span class="pre">.yaml</span></code> file has hierarchical structure, you need to refactor your model to treat
<code class="docutils literal notranslate"><span class="pre">hparams</span></code> as <a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a>.</p>
</p></li>
<li><p><strong>strict</strong>  Whether to strictly enforce that the keys in <code class="xref py py-attr docutils literal notranslate"><span class="pre">checkpoint_path</span></code> match the keys
returned by this modules state dict. Defaults to <code class="docutils literal notranslate"><span class="pre">True</span></code> unless <code class="docutils literal notranslate"><span class="pre">LightningModule.strict_loading</span></code> is
set, in which case it defaults to the value of <code class="docutils literal notranslate"><span class="pre">LightningModule.strict_loading</span></code>.</p></li>
<li><p><strong>**kwargs</strong>  Any extra keyword args needed to init the model. Can also be used to override saved
hyperparameter values.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance with loaded weights and hyperparameters (if available).</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> is a <strong>class</strong> method. You should use your <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code>
<strong>class</strong> to call it instead of the <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance, or a
<code class="docutils literal notranslate"><span class="pre">TypeError</span></code> will be raised.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To ensure all layers can be loaded from the checkpoint, this function will call
<code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_model()</span></code> directly after instantiating the
model if this hook is overridden in your LightningModule. However, note that <code class="docutils literal notranslate"><span class="pre">load_from_checkpoint</span></code> does
not support loading sharded checkpoints, and you may run out of memory if the model is too large. In this
case, consider loading through the Trainer via <code class="docutils literal notranslate"><span class="pre">.fit(ckpt_path=...)</span></code>.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># load weights without mapping ...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span><span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">)</span>

<span class="c1"># or load weights mapping all weights from GPU 1 to GPU 0 ...</span>
<span class="n">map_location</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;cuda:1&#39;</span><span class="p">:</span><span class="s1">&#39;cuda:0&#39;</span><span class="p">}</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span>
<span class="p">)</span>

<span class="c1"># or load weights and hyperparameters from separate files.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="s1">&#39;path/to/checkpoint.ckpt&#39;</span><span class="p">,</span>
    <span class="n">hparams_file</span><span class="o">=</span><span class="s1">&#39;/path/to/hparams_file.yaml&#39;</span>
<span class="p">)</span>

<span class="c1"># override some of the params with new values</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="o">.</span><span class="n">load_from_checkpoint</span><span class="p">(</span>
    <span class="n">PATH</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
    <span class="n">pretrained_ckpt_path</span><span class="o">=</span><span class="n">NEW_PATH</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># predict</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">pretrained_model</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
<span class="n">y_hat</span> <span class="o">=</span> <span class="n">pretrained_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.load_state_dict">
<span class="sig-name descname"><span class="pre">load_state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state_dict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">assign</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.load_state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.load_state_dict" title="Link to this definition">#</a></dt>
<dd><p>Copy parameters and buffers from <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into this module and its descendants.</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this modules <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If <code class="xref py py-attr docutils literal notranslate"><span class="pre">assign</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code> the optimizer must be created after
the call to <a class="reference internal" href="#stable_pretraining.Module.load_state_dict" title="stable_pretraining.Module.load_state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">load_state_dict</span></code></a> unless
<a class="reference external" href="https://docs.pytorch.org/docs/stable/future_mod.html#torch.__future__.get_swap_module_params_on_conversion" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_swap_module_params_on_conversion()</span></code></a> is <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  whether to strictly enforce that the keys
in <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this modules
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><strong>assign</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  When set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, the properties of the tensors
in the current module are preserved whereas setting it to <code class="docutils literal notranslate"><span class="pre">True</span></code> preserves
properties of the Tensors in the state dict. The only
exception is the <code class="docutils literal notranslate"><span class="pre">requires_grad</span></code> field of <code class="xref py py-class docutils literal notranslate"><span class="pre">Parameter</span></code>
for which the value from the module is preserved. Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> is a list of str containing any keys that are expected</dt><dd><p>by this module but missing from the provided <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt><code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> is a list of str containing the keys that are not</dt><dd><p>expected by this module but present in the provided <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>.</p>
</dd>
</dl>
</li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a parameter or buffer is registered as <code class="docutils literal notranslate"><span class="pre">None</span></code> and its corresponding key
exists in <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>, <a class="reference internal" href="#stable_pretraining.Module.load_state_dict" title="stable_pretraining.Module.load_state_dict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> will raise a
<code class="docutils literal notranslate"><span class="pre">RuntimeError</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.log">
<span class="sig-name descname"><span class="pre">log</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">value</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Metric</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric_attribute</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.log" title="Link to this definition">#</a></dt>
<dd><p>Log a key, value pair.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s1">&#39;train_loss&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
</pre></div>
</div>
<p>The default behavior per hook is documented here: <span class="xref std std-ref">extensions/logging:Automatic Logging</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong>  key to log. Must be identical across all processes if using DDP or any other distributed strategy.</p></li>
<li><p><strong>value</strong>  value to log. Can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, or a <code class="docutils literal notranslate"><span class="pre">Metric</span></code>.</p></li>
<li><p><strong>prog_bar</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress bar.</p></li>
<li><p><strong>logger</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><strong>on_step</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step. The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>on_epoch</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics. The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>reduce_fx</strong>  reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto detach the graph.</p></li>
<li><p><strong>sync_dist</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across devices. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong>  the DDP group to sync across.</p></li>
<li><p><strong>add_dataloader_idx</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple dataloaders). If False, user needs to give unique names for
each dataloader to not mix the values.</p></li>
<li><p><strong>batch_size</strong>  Current batch_size. This will be directly inferred from the loaded batch,
but for some data structures you might need to explicitly provide it.</p></li>
<li><p><strong>metric_attribute</strong>  To restore the metric state, Lightning requires the reference of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">torchmetrics.Metric</span></code> in your model. This is found automatically if it is a model attribute.</p></li>
<li><p><strong>rank_zero_only</strong>  Tells Lightning if you are calling <code class="docutils literal notranslate"><span class="pre">self.log</span></code> from every process (default) or only from
rank 0. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, you wont be able to use this metric as a monitor in callbacks
(e.g., early stopping). Warning: Improper use can lead to deadlocks! See
<span class="xref std std-ref">Advanced Logging</span> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.log_dict">
<span class="sig-name descname"><span class="pre">log_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dictionary</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">Metric</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">MetricCollection</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prog_bar</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_step</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">on_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">reduce_fx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mean'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">enable_graph</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">sync_dist_group</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">add_dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rank_zero_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.log_dict" title="Link to this definition">#</a></dt>
<dd><p>Log a dictionary of values at once.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">values</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;acc&#39;</span><span class="p">:</span> <span class="n">acc</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="s1">&#39;metric_n&#39;</span><span class="p">:</span> <span class="n">metric_n</span><span class="p">}</span>
<span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>dictionary</strong>  key value pairs.
Keys must be identical across all processes if using DDP or any other distributed strategy.
The values can be a <code class="docutils literal notranslate"><span class="pre">float</span></code>, <code class="docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">Metric</span></code>, or <code class="docutils literal notranslate"><span class="pre">MetricCollection</span></code>.</p></li>
<li><p><strong>prog_bar</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the progress base.</p></li>
<li><p><strong>logger</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs to the logger.</p></li>
<li><p><strong>on_step</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs at this step.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for training_step but not validation/test_step.
The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>on_epoch</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code> logs epoch accumulated metrics.
<code class="docutils literal notranslate"><span class="pre">None</span></code> auto-logs for val/test step but not <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.
The default value is determined by the hook.
See <span class="xref std std-ref">extensions/logging:Automatic Logging</span> for details.</p></li>
<li><p><strong>reduce_fx</strong>  reduction function over step values for end of epoch. <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.mean()</span></code> by default.</p></li>
<li><p><strong>enable_graph</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, will not auto-detach the graph</p></li>
<li><p><strong>sync_dist</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, reduces the metric across GPUs/TPUs. Use with care as this may lead to a significant
communication overhead.</p></li>
<li><p><strong>sync_dist_group</strong>  the ddp group to sync across.</p></li>
<li><p><strong>add_dataloader_idx</strong>  if <code class="docutils literal notranslate"><span class="pre">True</span></code>, appends the index of the current dataloader to
the name (when using multiple). If <code class="docutils literal notranslate"><span class="pre">False</span></code>, user needs to give unique names for
each dataloader to not mix values.</p></li>
<li><p><strong>batch_size</strong>  Current batch size. This will be directly inferred from the loaded batch,
but some data structures might need to explicitly provide it.</p></li>
<li><p><strong>rank_zero_only</strong>  Tells Lightning if you are calling <code class="docutils literal notranslate"><span class="pre">self.log</span></code> from every process (default) or only from
rank 0. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, you wont be able to use this metric as a monitor in callbacks
(e.g., early stopping). Warning: Improper use can lead to deadlocks! See
<span class="xref std std-ref">Advanced Logging</span> for more details.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.lr_scheduler_step">
<span class="sig-name descname"><span class="pre">lr_scheduler_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.lr_scheduler_step" title="Link to this definition">#</a></dt>
<dd><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls
each scheduler. By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and as shown in the example for each scheduler based on
its <code class="docutils literal notranslate"><span class="pre">interval</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scheduler</strong>  Learning rate scheduler.</p></li>
<li><p><strong>metric</strong>  Value of the monitor used for schedulers like <code class="docutils literal notranslate"><span class="pre">ReduceLROnPlateau</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lr_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">metric</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">metric</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">metric</span><span class="p">)</span>

<span class="c1"># Alternative way to update schedulers if it requires an epoch value</span>
<span class="k">def</span><span class="w"> </span><span class="nf">lr_scheduler_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">metric</span><span class="p">):</span>
    <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">epoch</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.lr_schedulers">
<span class="sig-name descname"><span class="pre">lr_schedulers</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.ReduceLROnPlateau.html#torch.optim.lr_scheduler.ReduceLROnPlateau" title="(in PyTorch v2.9)"><span class="pre">ReduceLROnPlateau</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.lr_schedulers" title="Link to this definition">#</a></dt>
<dd><p>Returns the learning rate scheduler(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A single scheduler, or a list of schedulers in case multiple ones are present, or <code class="docutils literal notranslate"><span class="pre">None</span></code> if no
schedulers were returned in <code class="xref py py-meth docutils literal notranslate"><span class="pre">configure_optimizers()</span></code>.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.manual_backward">
<span class="sig-name descname"><span class="pre">manual_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.manual_backward" title="Link to this definition">#</a></dt>
<dd><p>Call this directly from your <a class="reference internal" href="#stable_pretraining.Module.training_step" title="stable_pretraining.Module.training_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">training_step()</span></code></a> when doing optimizations manually. By using this,
Lightning can ensure that all the proper scaling gets applied when using mixed precision.</p>
<p>See <span class="xref std std-ref">manual optimization</span> for more examples.</p>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="c1"># automatically applies scaling, etc...</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>loss</strong>  The tensor on which to compute gradients. Must have a graph attached.</p></li>
<li><p><strong>*args</strong>  Additional positional arguments to be forwarded to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
<li><p><strong>**kwargs</strong>  Additional keyword arguments to be forwarded to <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">backward()</span></code></a></p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.modules">
<span class="sig-name descname"><span class="pre">modules</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.modules" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over all modules in the network.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>Module</em>  a module in the network</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">modules</span><span class="p">()):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="go">0 -&gt; Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">)</span>
<span class="go">1 -&gt; Linear(in_features=2, out_features=2, bias=True)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.mtia">
<span class="sig-name descname"><span class="pre">mtia</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.mtia"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.mtia" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the MTIA.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing the optimizer if the module will
live on MTIA while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.named_buffers">
<span class="sig-name descname"><span class="pre">named_buffers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_buffers"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.named_buffers" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  prefix to prepend to all buffer names.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  if True, then yields buffers of this module
and all submodules. Otherwise, yields only buffers that
are direct members of this module. Defaults to True.</p></li>
<li><p><strong>remove_duplicate</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  whether to remove the duplicated buffers in the result. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>(str, torch.Tensor)</em>  Tuple containing the name and buffer</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">buf</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_buffers</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;running_var&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">buf</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.named_children">
<span class="sig-name descname"><span class="pre">named_children</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterator" title="(in Python v3.14)"><span class="pre">Iterator</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_children"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.named_children" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over immediate children modules, yielding both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Yields<span class="colon">:</span></dt>
<dd class="field-odd"><p><em>(str, Module)</em>  Tuple containing a name and child module</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">named_children</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;conv4&#39;</span><span class="p">,</span> <span class="s1">&#39;conv5&#39;</span><span class="p">]:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="nb">print</span><span class="p">(</span><span class="n">module</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.named_modules">
<span class="sig-name descname"><span class="pre">named_modules</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">memo</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#set" title="(in Python v3.14)"><span class="pre">set</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">remove_duplicate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.named_modules"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.named_modules" title="Link to this definition">#</a></dt>
<dd><p>Return an iterator over all modules in the network, yielding both the name of the module as well as the module itself.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>memo</strong>  a memo to store the set of modules already added to the result</p></li>
<li><p><strong>prefix</strong>  a prefix that will be added to the name of the module</p></li>
<li><p><strong>remove_duplicate</strong>  whether to remove the duplicated module instances in the result
or not</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>(str, Module)</em>  Tuple of name and module</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Duplicate modules are returned only once. In the following
example, <code class="docutils literal notranslate"><span class="pre">l</span></code> will be returned only once.</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">named_modules</span><span class="p">()):</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="s1">&#39;-&gt;&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>

<span class="go">0 -&gt; (&#39;&#39;, Sequential(</span>
<span class="go">  (0): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">  (1): Linear(in_features=2, out_features=2, bias=True)</span>
<span class="go">))</span>
<span class="go">1 -&gt; (&#39;0&#39;, Linear(in_features=2, out_features=2, bias=True))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.named_parameters">
<span class="sig-name descname"><span class="pre">named_parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.named_parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.named_parameters" title="Link to this definition">#</a></dt>
<dd><p>Override to globally exclude callback-related parameters.</p>
<p>Excludes parameters that belong to <code class="docutils literal notranslate"><span class="pre">self.callbacks_modules</span></code> or <code class="docutils literal notranslate"><span class="pre">self.callbacks_metrics</span></code>.
This prevents accidental optimization of callback/metric internals, even if external code
calls <code class="docutils literal notranslate"><span class="pre">self.parameters()</span></code> or <code class="docutils literal notranslate"><span class="pre">self.named_parameters()</span></code> directly.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>with_callbacks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If False, excludes callback parameters. Defaults to True.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  Prefix to prepend to parameter names. Defaults to .</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If True, yields parameters of this module and all submodules.
If False, yields only direct parameters. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>tuple[str, torch.nn.Parameter]</em>  Name and parameter pairs.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_after_backward" title="Link to this definition">#</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code> and before optimizers are stepped.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If using native AMP, the gradients will not be unscaled at this point.
Use the <code class="docutils literal notranslate"><span class="pre">on_before_optimizer_step</span></code> if you need the unscaled gradients.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_after_batch_transfer">
<span class="sig-name descname"><span class="pre">on_after_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_after_batch_transfer" title="Link to this definition">#</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch after it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_after_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">gpu_transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#stable_pretraining.Module.on_before_batch_transfer" title="stable_pretraining.Module.on_before_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_before_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.transfer_batch_to_device" title="stable_pretraining.Module.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_before_backward">
<span class="sig-name descname"><span class="pre">on_before_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_before_backward" title="Link to this definition">#</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">loss.backward()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>loss</strong>  Loss divided by number of batches for gradient accumulation and scaled if using AMP.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_before_batch_transfer">
<span class="sig-name descname"><span class="pre">on_before_batch_transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_before_batch_transfer" title="Link to this definition">#</a></dt>
<dd><p>Override to alter or apply batch augmentations to your batch before it is transferred to the device.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be altered or augmented.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A batch of data</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_before_batch_transfer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s1">&#39;x&#39;</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="#stable_pretraining.Module.on_after_batch_transfer" title="stable_pretraining.Module.on_after_batch_transfer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_after_batch_transfer()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.transfer_batch_to_device" title="stable_pretraining.Module.transfer_batch_to_device"><code class="xref py py-meth docutils literal notranslate"><span class="pre">transfer_batch_to_device()</span></code></a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_before_optimizer_step">
<span class="sig-name descname"><span class="pre">on_before_optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_before_optimizer_step" title="Link to this definition">#</a></dt>
<dd><p>Called before <code class="docutils literal notranslate"><span class="pre">optimizer.step()</span></code>.</p>
<p>If using gradient accumulation, the hook is called once the gradients have been accumulated.
See: <a href="#id13"><span class="problematic" id="id14">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.accumulate_grad_batches`</span></a>.</p>
<p>If using AMP, the loss will be unscaled before calling this hook.
See these <a class="reference external" href="https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-unscaled-gradients">docs</a>
for more information on the scaling of gradients.</p>
<p>If clipping gradients, the gradients will not have been clipped yet.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  Current optimizer being used.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_before_optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="c1"># example to inspect gradient information in tensorboard</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span> <span class="o">%</span> <span class="mi">25</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># don&#39;t make the tf file huge</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_histogram</span><span class="p">(</span>
                <span class="n">tag</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">values</span><span class="o">=</span><span class="n">v</span><span class="o">.</span><span class="n">grad</span><span class="p">,</span> <span class="n">global_step</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">global_step</span>
            <span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_before_zero_grad">
<span class="sig-name descname"><span class="pre">on_before_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_before_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Called after <code class="docutils literal notranslate"><span class="pre">training_step()</span></code> and before <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<p>Called in the training loop after taking an optimizer step and before zeroing grads.
Good place to inspect weight information with weights updated.</p>
<p>This is where it is called:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">optimizer</span> <span class="ow">in</span> <span class="n">optimizers</span><span class="p">:</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

    <span class="n">model</span><span class="o">.</span><span class="n">on_before_zero_grad</span><span class="p">(</span><span class="n">optimizer</span><span class="p">)</span> <span class="c1"># &lt; ---- called here</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">backward</span><span class="p">()</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer for which grads should be zeroed.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_fit_end">
<span class="sig-name descname"><span class="pre">on_fit_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_fit_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the very end of fit.</p>
<p>If on DDP it is called on every process</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_fit_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the very beginning of fit.</p>
<p>If on DDP it is called on every process</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_load_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called by Lightning to restore your model. If you saved something with <a class="reference internal" href="#stable_pretraining.Module.on_save_checkpoint" title="stable_pretraining.Module.on_save_checkpoint"><code class="xref py py-meth docutils literal notranslate"><span class="pre">on_save_checkpoint()</span></code></a> this is
your chance to restore this.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong>  Loaded checkpoint</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">on_load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">checkpoint</span><span class="p">):</span>
    <span class="c1"># 99% of the time you don&#39;t need to implement this method</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">something_cool_i_want_to_save</span> <span class="o">=</span> <span class="n">checkpoint</span><span class="p">[</span><span class="s1">&#39;something_cool_i_want_to_save&#39;</span><span class="p">]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning auto-restores global step, epoch, and train state including amp scaling.
There is no need for you to restore anything regarding training.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the predict loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of predict_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the prediction DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_batch_start">
<span class="sig-name descname"><span class="pre">on_predict_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the predict loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_end">
<span class="sig-name descname"><span class="pre">on_predict_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_epoch_end">
<span class="sig-name descname"><span class="pre">on_predict_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_epoch_start">
<span class="sig-name descname"><span class="pre">on_predict_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_model_eval">
<span class="sig-name descname"><span class="pre">on_predict_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the predict loop starts.</p>
<p>The predict loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_predict_start">
<span class="sig-name descname"><span class="pre">on_predict_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_predict_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of predicting.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.on_save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.on_save_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Offload checkpoint tensors to CPU to reduce GPU memory usage during save.</p>
<p>This method intercepts the checkpoint saving process and recursively moves all
PyTorch tensors (model weights, optimizer states, scheduler states) from GPU
to CPU before writing to disk. This prevents GPU OOM issues when checkpointing
large models (e.g., 2B+ parameters with optimizer states).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>checkpoint</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  Lightning checkpoint dictionary containing:
- state_dict: Model parameters (moved to CPU)
- optimizer_states: Optimizer state dicts (moved to CPU)
- lr_schedulers: LR scheduler states (moved to CPU)
- Other keys: Custom objects, metadata (left unchanged)</p>
</dd>
</dl>
<dl class="simple">
<dt>Behavior:</dt><dd><ul class="simple">
<li><p>Processes standard Lightning checkpoint keys (state_dict, optimizer_states, lr_schedulers)</p></li>
<li><p>Recursively traverses dicts, lists, and tuples to find tensors</p></li>
<li><p>Moves all torch.Tensor objects to CPU</p></li>
<li><p>Skips custom objects (returns unchanged)</p></li>
<li><p>Logs GPU memory freed and processing time</p></li>
<li><p>Non-destructive: Checkpoint loading/resuming works normally</p></li>
</ul>
</dd>
<dt>Side Effects:</dt><dd><ul class="simple">
<li><p>Modifies checkpoint dict in-place (tensors moved to CPU)</p></li>
<li><p>Temporarily increases CPU memory during offload</p></li>
<li><p>Adds ~2-5 seconds to checkpoint save time for 2B models</p></li>
<li><p>Frees ~8-12GB GPU memory for 2B model + optimizer states</p></li>
</ul>
</dd>
<dt>Custom Objects:</dt><dd><p>Custom objects in the checkpoint are NOT modified and will be logged as
warnings. These include: custom classes, numpy arrays, primitives, etc.
They are safely skipped and preserved in the checkpoint.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.14)"><strong>Exception</strong></a>  If tensor offload fails for any checkpoint key, logs error
    but allows checkpoint save to proceed (non-fatal).</p>
</dd>
</dl>
<p class="rubric">Example</p>
<p>For a 2B parameter model with AdamW optimizer:
- Before: ~12GB GPU memory spike on rank 0 during checkpoint save
- After: ~0.2GB GPU memory spike, ~10-12GB freed
- Checkpoint save time: +2-3 seconds
- Resume from checkpoint: Works normally, tensors auto-loaded to GPU</p>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>Only rank 0 saves checkpoints in DDP, so only rank 0 sees memory benefit</p></li>
<li><p>Does not affect checkpoint contents or ability to resume training</p></li>
<li><p>Safe for standard PyTorch/Lightning use cases</p></li>
<li><p>If using FSDP/DeepSpeed, consider strategy-specific checkpointing instead</p></li>
</ul>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p>PyTorch Lightning ModelCheckpoint callback</p></li>
<li><p>torch.Tensor.cpu() for device transfer behavior</p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of test_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_batch_start">
<span class="sig-name descname"><span class="pre">on_test_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the test DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_end">
<span class="sig-name descname"><span class="pre">on_test_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_epoch_end">
<span class="sig-name descname"><span class="pre">on_test_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_epoch_start">
<span class="sig-name descname"><span class="pre">on_test_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the test loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_model_eval">
<span class="sig-name descname"><span class="pre">on_test_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the test loop starts.</p>
<p>The test loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior. See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_model_train()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_model_train">
<span class="sig-name descname"><span class="pre">on_test_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_model_train" title="Link to this definition">#</a></dt>
<dd><p>Called when the test loop ends.</p>
<p>The test loop by default restores the <cite>training</cite> mode of the LightningModule to what it was before
starting testing. Override this hook to change the behavior. See also
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_test_model_eval()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_test_start">
<span class="sig-name descname"><span class="pre">on_test_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_test_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of testing.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_train_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of training_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">outputs[&quot;loss&quot;]</span></code> here will be the normalized value w.r.t <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> of the
loss returned from <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_batch_start">
<span class="sig-name descname"><span class="pre">on_train_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_train_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop before anything happens for that batch.</p>
<p>If you return -1 here, you will skip training for the rest of the current epoch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the training DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_end">
<span class="sig-name descname"><span class="pre">on_train_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_train_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of training before logger experiment is closed.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_epoch_end">
<span class="sig-name descname"><span class="pre">on_train_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_train_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop at the very end of the epoch.</p>
<p>To access all batch outputs at the end of the epoch, you can cache step outputs as an attribute of the
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> and access them in this hook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyLightningModule</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">on_train_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># do something with all training_step outputs, for example:</span>
        <span class="n">epoch_mean</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="s2">&quot;training_epoch_mean&quot;</span><span class="p">,</span> <span class="n">epoch_mean</span><span class="p">)</span>
        <span class="c1"># free up the memory</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">training_step_outputs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_epoch_start">
<span class="sig-name descname"><span class="pre">on_train_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_train_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the training loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_train_start">
<span class="sig-name descname"><span class="pre">on_train_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.on_train_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.on_train_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of training after sanity check.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Mapping" title="(in Python v3.14)"><span class="pre">Mapping</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop after the batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>outputs</strong>  The outputs of validation_step(x)</p></li>
<li><p><strong>batch</strong>  The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_batch_start">
<span class="sig-name descname"><span class="pre">on_validation_batch_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_batch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop before anything happens for that batch.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The batched data as it is returned by the validation DataLoader.</p></li>
<li><p><strong>batch_idx</strong>  the index of the batch</p></li>
<li><p><strong>dataloader_idx</strong>  the index of the dataloader</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_end">
<span class="sig-name descname"><span class="pre">on_validation_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_end" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of validation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_epoch_end">
<span class="sig-name descname"><span class="pre">on_validation_epoch_end</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_epoch_end" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop at the very end of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_epoch_start">
<span class="sig-name descname"><span class="pre">on_validation_epoch_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_epoch_start" title="Link to this definition">#</a></dt>
<dd><p>Called in the validation loop at the very beginning of the epoch.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_model_eval">
<span class="sig-name descname"><span class="pre">on_validation_model_eval</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_model_eval" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation loop starts.</p>
<p>The validation loop by default calls <code class="docutils literal notranslate"><span class="pre">.eval()</span></code> on the LightningModule before it starts. Override this hook
to change the behavior. See also <code class="xref py py-meth docutils literal notranslate"><span class="pre">on_validation_model_train()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_model_train">
<span class="sig-name descname"><span class="pre">on_validation_model_train</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_model_train" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation loop ends.</p>
<p>The validation loop by default restores the <cite>training</cite> mode of the LightningModule to what it was before
starting validation. Override this hook to change the behavior. See also
<code class="xref py py-meth docutils literal notranslate"><span class="pre">on_validation_model_eval()</span></code>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_model_zero_grad">
<span class="sig-name descname"><span class="pre">on_validation_model_zero_grad</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_model_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Called by the training loop to release gradients before entering the validation loop.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.on_validation_start">
<span class="sig-name descname"><span class="pre">on_validation_start</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.on_validation_start" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of validation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.optimizer_step">
<span class="sig-name descname"><span class="pre">optimizer_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer_closure</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.optimizer_step" title="Link to this definition">#</a></dt>
<dd><p>Override this method to adjust the default way the <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> calls
the optimizer.</p>
<p>By default, Lightning calls <code class="docutils literal notranslate"><span class="pre">step()</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code> as shown in the example.
This method (and <code class="docutils literal notranslate"><span class="pre">zero_grad()</span></code>) wont be called during the accumulation phase when
<code class="docutils literal notranslate"><span class="pre">Trainer(accumulate_grad_batches</span> <span class="pre">!=</span> <span class="pre">1)</span></code>. Overriding this hook has no benefit with manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong>  Current epoch</p></li>
<li><p><strong>batch_idx</strong>  Index of current batch</p></li>
<li><p><strong>optimizer</strong>  A PyTorch optimizer</p></li>
<li><p><strong>optimizer_closure</strong>  The optimizer closure. This closure must be executed as it includes the
calls to <code class="docutils literal notranslate"><span class="pre">training_step()</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>, and <code class="docutils literal notranslate"><span class="pre">backward()</span></code>.</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">optimizer_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">optimizer_closure</span><span class="p">):</span>
    <span class="c1"># Add your custom logic to run directly before `optimizer.step()`</span>

    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">closure</span><span class="o">=</span><span class="n">optimizer_closure</span><span class="p">)</span>

    <span class="c1"># Add your custom logic to run directly after `optimizer.step()`</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.optimizer_zero_grad">
<span class="sig-name descname"><span class="pre">optimizer_zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.optimizer_zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Override this method to change the default behaviour of <code class="docutils literal notranslate"><span class="pre">optimizer.zero_grad()</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong>  Current epoch</p></li>
<li><p><strong>batch_idx</strong>  Index of current batch</p></li>
<li><p><strong>optimizer</strong>  A PyTorch optimizer</p></li>
</ul>
</dd>
</dl>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>

<span class="c1"># Set gradients to `None` instead of zero to improve performance (not required on `torch&gt;=2.0.0`).</span>
<span class="k">def</span><span class="w"> </span><span class="nf">optimizer_zero_grad</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code></a> for the explanation of the above example.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.optimizers">
<span class="sig-name descname"><span class="pre">optimizers</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">use_pl_optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">_FabricOptimizer</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><span class="pre">LightningOptimizer</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><span class="pre">_FabricOptimizer</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#stable_pretraining.Module.optimizers" title="Link to this definition">#</a></dt>
<dd><p>Returns the optimizer(s) that are being used during training. Useful for manual optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>use_pl_optimizer</strong>  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, will wrap the optimizer(s) in a
<code class="xref py py-class docutils literal notranslate"><span class="pre">LightningOptimizer</span></code> for automatic handling of precision,
profiling, and counting of step calls for proper logging and checkpointing. It specifically wraps the
<code class="docutils literal notranslate"><span class="pre">step</span></code> method and custom optimizers that dont have this method are not supported.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A single optimizer, or a list of optimizers in case multiple ones are present.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.parameters">
<span class="sig-name descname"><span class="pre">parameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">with_callbacks</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.parameters"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.parameters" title="Link to this definition">#</a></dt>
<dd><p>Override to route through the filtered <code class="docutils literal notranslate"><span class="pre">named_parameters</span></code> implementation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>with_callbacks</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If False, excludes callback parameters. Defaults to True.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If True, yields parameters of this module and all submodules.
If False, yields only direct parameters. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Yields<span class="colon">:</span></dt>
<dd class="field-even"><p><em>torch.nn.Parameter</em>  Module parameters.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.predict_dataloader">
<span class="sig-name descname"><span class="pre">predict_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.predict_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying prediction samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>Its recommended that all data downloads and preparation happen in <a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>A <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.data.DataLoader</span></code></a> or a sequence of them specifying prediction samples.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.predict_step">
<span class="sig-name descname"><span class="pre">predict_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.predict_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.predict_step" title="Link to this definition">#</a></dt>
<dd><p>Step function called during <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict()</span></code>. By default, it calls
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code>. Override to add any processing logic.</p>
<p>The <code class="xref py py-meth docutils literal notranslate"><span class="pre">predict_step()</span></code> is used
to scale inference on multi-devices.</p>
<p>To prevent an OOM error, it is possible to use <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code>
callback to write the predictions to disk or database after each batch or on epoch end.</p>
<p>The <code class="xref py py-class docutils literal notranslate"><span class="pre">BasePredictionWriter</span></code> should be used while using a spawn
based accelerator. This happens for <code class="docutils literal notranslate"><span class="pre">Trainer(strategy=&quot;ddp_spawn&quot;)</span></code>
or training on 8 TPU cores with <code class="docutils literal notranslate"><span class="pre">Trainer(accelerator=&quot;tpu&quot;,</span> <span class="pre">devices=8)</span></code> as predictions wont be returned.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Predicted output (optional).</p>
</dd>
</dl>
<p>Example</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>

<span class="n">dm</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span>
<span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">accelerator</span><span class="o">=</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">trainer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dm</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.prepare_data">
<span class="sig-name descname"><span class="pre">prepare_data</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.prepare_data" title="Link to this definition">#</a></dt>
<dd><p>Use this to download and prepare data. Downloading and saving data with multiple processes (distributed
settings) will result in corrupted data. Lightning ensures this method is called only within a single process,
so you can safely add your downloading logic within.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>DO NOT set state to the model (use <code class="docutils literal notranslate"><span class="pre">setup</span></code> instead)
since this is NOT called on every device</p>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="c1"># good</span>
    <span class="n">download_data</span><span class="p">()</span>
    <span class="n">tokenize</span><span class="p">()</span>
    <span class="n">etc</span><span class="p">()</span>

    <span class="c1"># bad</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">split</span> <span class="o">=</span> <span class="n">data_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">some_state</span> <span class="o">=</span> <span class="n">some_other_state</span><span class="p">()</span>
</pre></div>
</div>
<p>In a distributed environment, <code class="docutils literal notranslate"><span class="pre">prepare_data</span></code> can be called in two ways
(using <span class="xref std std-ref">prepare_data_per_node</span>)</p>
<ol class="arabic simple">
<li><p>Once per node. This is the default and is only called on LOCAL_RANK=0.</p></li>
<li><p>Once in total. Only called on GLOBAL_RANK=0.</p></li>
</ol>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># DEFAULT</span>
<span class="c1"># called once per node on LOCAL_RANK=0 of that node</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">True</span>


<span class="c1"># call on GLOBAL_RANK=0 (great for shared file systems)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LitDataModule</span><span class="p">(</span><span class="n">LightningDataModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prepare_data_per_node</span> <span class="o">=</span> <span class="kc">False</span>
</pre></div>
</div>
<p>This is called before requesting the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">prepare_data</span><span class="p">()</span>
<span class="n">initialize_distributed</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">setup</span><span class="p">(</span><span class="n">stage</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">train_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">val_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">test_dataloader</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">predict_dataloader</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.print">
<span class="sig-name descname"><span class="pre">print</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.print" title="Link to this definition">#</a></dt>
<dd><p>Prints only from process 0. Use this in any distributed mode to log only once.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong>  The thing to print. The same as for Pythons built-in print function.</p></li>
<li><p><strong>**kwargs</strong>  The same as for Pythons built-in print function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s1">&#39;in forward&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_backward_hook">
<span class="sig-name descname"><span class="pre">register_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_backward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward hook on the module.</p>
<p>This function is deprecated in favor of <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">register_full_backward_hook()</span></code></a> and
the behavior of this function will change in future versions.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_buffer">
<span class="sig-name descname"><span class="pre">register_buffer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">tensor</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">persistent</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_buffer"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_buffer" title="Link to this definition">#</a></dt>
<dd><p>Add a buffer to the module.</p>
<p>This is typically used to register a buffer that should not be
considered a model parameter. For example, BatchNorms <code class="docutils literal notranslate"><span class="pre">running_mean</span></code>
is not a parameter, but is part of the modules state. Buffers, by
default, are persistent and will be saved alongside parameters. This
behavior can be changed by setting <code class="xref py py-attr docutils literal notranslate"><span class="pre">persistent</span></code> to <code class="docutils literal notranslate"><span class="pre">False</span></code>. The
only difference between a persistent buffer and a non-persistent buffer
is that the latter will not be a part of this modules
<a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p>
<p>Buffers can be accessed as attributes using given names.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the buffer. The buffer can be accessed
from this module using the given name</p></li>
<li><p><strong>tensor</strong> (<em>Tensor</em><em> or </em><em>None</em>)  buffer to be registered. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations
that run on buffers, such as <a class="reference internal" href="#stable_pretraining.Module.cuda" title="stable_pretraining.Module.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>, are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>,
the buffer is <strong>not</strong> included in the modules <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
<li><p><strong>persistent</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether the buffer is part of this modules
<a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s1">&#39;running_mean&#39;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_features</span><span class="p">))</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_forward_hook">
<span class="sig-name descname"><span class="pre">register_forward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">always_call</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_forward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_forward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a forward hook on the module.</p>
<p>The hook will be called every time after <a class="reference internal" href="#module-stable_pretraining.forward" title="stable_pretraining.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> has computed an output.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is <code class="docutils literal notranslate"><span class="pre">False</span></code> or not specified, the input contains only
the positional arguments given to the module. Keyword arguments wont be
passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>. The hook can modify the
output. It can modify the input inplace but it will not have effect on
forward since this is called after <a class="reference internal" href="#module-stable_pretraining.forward" title="stable_pretraining.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is called. The hook
should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, the forward hook will be passed the
<code class="docutils literal notranslate"><span class="pre">kwargs</span></code> given to the forward function and be expected to return the
output possibly modified. The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="n">output</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired
before all existing <code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks on
this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">forward</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_hook()</span></code> will fire before all hooks
registered by this method.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>with_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be passed the
kwargs given to the forward function.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>always_call</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If <code class="docutils literal notranslate"><span class="pre">True</span></code> the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be run regardless of
whether an exception is raised while calling the Module.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_forward_pre_hook">
<span class="sig-name descname"><span class="pre">register_forward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">T</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">with_kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_forward_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_forward_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a forward pre-hook on the module.</p>
<p>The hook will be called every time before <a class="reference internal" href="#module-stable_pretraining.forward" title="stable_pretraining.forward"><code class="xref py py-func docutils literal notranslate"><span class="pre">forward()</span></code></a> is invoked.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is false or not specified, the input contains only
the positional arguments given to the module. Keyword arguments wont be
passed to the hooks and only to the <code class="docutils literal notranslate"><span class="pre">forward</span></code>. The hook can modify the
input. User can either return a tuple or a single modified value in the
hook. We will wrap the value into a tuple if a single value is returned
(unless that value is already a tuple). The hook should have the
following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">modified</span> <span class="nb">input</span>
</pre></div>
</div>
<p>If <code class="docutils literal notranslate"><span class="pre">with_kwargs</span></code> is true, the forward pre-hook will be passed the
kwargs given to the forward function. And if the hook modifies the
input, both the args and kwargs should be returned. The hook should have
the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">a</span> <span class="nb">tuple</span> <span class="n">of</span> <span class="n">modified</span> <span class="nb">input</span> <span class="ow">and</span> <span class="n">kwargs</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks
on this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">forward_pre</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_forward_pre_hook()</span></code> will fire before all
hooks registered by this method.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><strong>with_kwargs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be passed the kwargs
given to the forward function.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_full_backward_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_full_backward_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_full_backward_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward hook on the module.</p>
<p>The hook will be called every time the gradients with respect to a module are computed, and its firing rules are as follows:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Ordinarily, the hook fires when the gradients are computed with respect to the module inputs.</p></li>
<li><p>If none of the module inputs require gradients, the hook will fire when the gradients are computed
with respect to module outputs.</p></li>
<li><p>If none of the module outputs require gradients, then the hooks will not fire.</p></li>
</ol>
</div></blockquote>
<p>The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_input</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">Tensor</span><span class="p">)</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> are tuples that contain the gradients
with respect to the inputs and outputs respectively. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the input that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> in
subsequent computations. <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> will only correspond to the inputs given
as positional arguments and all kwarg arguments are ignored. Entries
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_input</span></code> and <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for all non-Tensor
arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Modules forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs or outputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user-defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks on
this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">backward</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_hook()</span></code> will fire before
all hooks registered by this method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_full_backward_pre_hook">
<span class="sig-name descname"><span class="pre">register_full_backward_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.14)"><span class="pre">Callable</span></a><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><span class="pre">Tensor</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prepend</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">RemovableHandle</span></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_full_backward_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_full_backward_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a backward pre-hook on the module.</p>
<p>The hook will be called every time the gradients for the module are computed.
The hook should have the following signature:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">hook</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">Tensor</span><span class="p">]</span> <span class="ow">or</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> is a tuple. The hook should
not modify its arguments, but it can optionally return a new gradient with
respect to the output that will be used in place of <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> in
subsequent computations. Entries in <code class="xref py py-attr docutils literal notranslate"><span class="pre">grad_output</span></code> will be <code class="docutils literal notranslate"><span class="pre">None</span></code> for
all non-Tensor arguments.</p>
<p>For technical reasons, when this hook is applied to a Module, its forward function will
receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
of each Tensor returned by the Modules forward function.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Modifying inputs inplace is not allowed when using backward hooks and
will raise an error.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>hook</strong> (<em>Callable</em>)  The user-defined hook to be registered.</p></li>
<li><p><strong>prepend</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If true, the provided <code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired before
all existing <code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks on this
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Otherwise, the provided
<code class="docutils literal notranslate"><span class="pre">hook</span></code> will be fired after all existing <code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks
on this <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.Module</span></code></a>. Note that global
<code class="docutils literal notranslate"><span class="pre">backward_pre</span></code> hooks registered with
<code class="xref py py-func docutils literal notranslate"><span class="pre">register_module_full_backward_pre_hook()</span></code> will fire before
all hooks registered by this method.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_load_state_dict_post_hook">
<span class="sig-name descname"><span class="pre">register_load_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_load_state_dict_post_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_load_state_dict_post_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a post-hook to be run after modules <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> is called.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, incompatible_keys) -&gt; None</p>
</dd>
</dl>
<p>The <code class="docutils literal notranslate"><span class="pre">module</span></code> argument is the current module that this hook is registered
on, and the <code class="docutils literal notranslate"><span class="pre">incompatible_keys</span></code> argument is a <code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> consisting
of attributes <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>. <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code>
is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the missing keys and
<code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> is a <code class="docutils literal notranslate"><span class="pre">list</span></code> of <code class="docutils literal notranslate"><span class="pre">str</span></code> containing the unexpected keys.</p>
<p>The given incompatible_keys can be modified inplace if needed.</p>
<p>Note that the checks performed when calling <a class="reference internal" href="#stable_pretraining.Module.load_state_dict" title="stable_pretraining.Module.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> with
<code class="docutils literal notranslate"><span class="pre">strict=True</span></code> are affected by modifications the hook makes to
<code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> or <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code>, as expected. Additions to either
set of keys will result in an error being thrown when <code class="docutils literal notranslate"><span class="pre">strict=True</span></code>, and
clearing out both missing and unexpected keys will avoid an error.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>a handle that can be used to remove the added hook by calling
<code class="docutils literal notranslate"><span class="pre">handle.remove()</span></code></p>
</dd>
<dt class="field-even">Return type<span class="colon">:</span></dt>
<dd class="field-even"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.utils.hooks.RemovableHandle</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_load_state_dict_pre_hook">
<span class="sig-name descname"><span class="pre">register_load_state_dict_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_load_state_dict_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_load_state_dict_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a pre-hook to be run before modules <code class="xref py py-meth docutils literal notranslate"><span class="pre">load_state_dict()</span></code> is called.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -&gt; None  # noqa: B950</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>hook</strong> (<em>Callable</em>)  Callable hook that will be invoked before
loading the state dict.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_module">
<span class="sig-name descname"><span class="pre">register_module</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_module"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_module" title="Link to this definition">#</a></dt>
<dd><p>Alias for <a class="reference internal" href="#stable_pretraining.Module.add_module" title="stable_pretraining.Module.add_module"><code class="xref py py-func docutils literal notranslate"><span class="pre">add_module()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_parameter">
<span class="sig-name descname"><span class="pre">register_parameter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">param</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter" title="(in PyTorch v2.9)"><span class="pre">Parameter</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_parameter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_parameter" title="Link to this definition">#</a></dt>
<dd><p>Add a parameter to the module.</p>
<p>The parameter can be accessed as an attribute using given name.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a>)  name of the parameter. The parameter can be accessed
from this module using the given name</p></li>
<li><p><strong>param</strong> (<em>Parameter</em><em> or </em><em>None</em>)  parameter to be added to the module. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, then operations that run on parameters, such as <a class="reference internal" href="#stable_pretraining.Module.cuda" title="stable_pretraining.Module.cuda"><code class="xref py py-attr docutils literal notranslate"><span class="pre">cuda</span></code></a>,
are ignored. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the parameter is <strong>not</strong> included in the
modules <a class="reference internal" href="#stable_pretraining.Module.state_dict" title="stable_pretraining.Module.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_state_dict_post_hook">
<span class="sig-name descname"><span class="pre">register_state_dict_post_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_state_dict_post_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_state_dict_post_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a post-hook for the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> method.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, state_dict, prefix, local_metadata) -&gt; None</p>
</dd>
</dl>
<p>The registered hooks can modify the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> inplace.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.register_state_dict_pre_hook">
<span class="sig-name descname"><span class="pre">register_state_dict_pre_hook</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">hook</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.register_state_dict_pre_hook"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.register_state_dict_pre_hook" title="Link to this definition">#</a></dt>
<dd><p>Register a pre-hook for the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.state_dict" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code></a> method.</p>
<dl class="simple">
<dt>It should have the following signature::</dt><dd><p>hook(module, prefix, keep_vars) -&gt; None</p>
</dd>
</dl>
<p>The registered hooks can be used to perform pre-processing before the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code>
call is made.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.requires_grad_">
<span class="sig-name descname"><span class="pre">requires_grad_</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">requires_grad</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.requires_grad_"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.requires_grad_" title="Link to this definition">#</a></dt>
<dd><p>Change if autograd should record operations on parameters in this module.</p>
<p>This method sets the parameters <code class="xref py py-attr docutils literal notranslate"><span class="pre">requires_grad</span></code> attributes
in-place.</p>
<p>This method is helpful for freezing part of the module for finetuning
or training parts of a model individually (e.g., GAN training).</p>
<p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/notes/autograd.html#locally-disable-grad-doc" title="(in PyTorch v2.9)"><span>Locally disabling gradient computation</span></a> for a comparison between
<cite>.requires_grad_()</cite> and several similar mechanisms that may be confused with it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>requires_grad</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether autograd should record operations on
parameters in this module. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.rescale_loss_for_grad_acc">
<span class="sig-name descname"><span class="pre">rescale_loss_for_grad_acc</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">loss</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.rescale_loss_for_grad_acc"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.rescale_loss_for_grad_acc" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.save_hyperparameters">
<span class="sig-name descname"><span class="pre">save_hyperparameters</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">ignore</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Sequence" title="(in Python v3.14)"><span class="pre">Sequence</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frame</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">FrameType</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">logger</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.save_hyperparameters" title="Link to this definition">#</a></dt>
<dd><p>Save arguments to <code class="docutils literal notranslate"><span class="pre">hparams</span></code> attribute.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>args</strong>  single object of <cite>dict</cite>, <cite>NameSpace</cite> or <cite>OmegaConf</cite>
or string names or arguments from class <code class="docutils literal notranslate"><span class="pre">__init__</span></code></p></li>
<li><p><strong>ignore</strong>  an argument name or a list of argument names from
class <code class="docutils literal notranslate"><span class="pre">__init__</span></code> to be ignored</p></li>
<li><p><strong>frame</strong>  a frame object. Default is None</p></li>
<li><p><strong>logger</strong>  Whether to send the hyperparameters to the logger. Default: True</p></li>
</ul>
</dd>
</dl>
<dl>
<dt>Example::</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign arguments</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="s1">&#39;arg1&#39;</span><span class="p">,</span> <span class="s1">&#39;arg3&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">AutomaticArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># equivalent automatic</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">()</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutomaticArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg2&quot;: abc</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">SingleArgModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># manually assign single argument</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">SingleArgModel</span><span class="p">(</span><span class="n">Namespace</span><span class="p">(</span><span class="n">p1</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p2</span><span class="o">=</span><span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="n">p3</span><span class="o">=</span><span class="mf">3.14</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;p1&quot;: 1</span>
<span class="go">&quot;p2&quot;: abc</span>
<span class="go">&quot;p3&quot;: 3.14</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">lightning.pytorch.core.mixins</span><span class="w"> </span><span class="kn">import</span> <span class="n">HyperparametersMixin</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span><span class="w"> </span><span class="nc">ManuallyArgsModel</span><span class="p">(</span><span class="n">HyperparametersMixin</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arg1</span><span class="p">,</span> <span class="n">arg2</span><span class="p">,</span> <span class="n">arg3</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="gp">... </span>        <span class="c1"># pass argument(s) to ignore as a string or in a list</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">ignore</span><span class="o">=</span><span class="s1">&#39;arg2&#39;</span><span class="p">)</span>
<span class="gp">... </span>    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="gp">... </span>        <span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">ManuallyArgsModel</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;abc&#39;</span><span class="p">,</span> <span class="mf">3.14</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span><span class="o">.</span><span class="n">hparams</span>
<span class="go">&quot;arg1&quot;: 1</span>
<span class="go">&quot;arg3&quot;: 3.14</span>
</pre></div>
</div>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.set_extra_state">
<span class="sig-name descname"><span class="pre">set_extra_state</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">state</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.set_extra_state"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.set_extra_state" title="Link to this definition">#</a></dt>
<dd><p>Set extra state contained in the loaded <cite>state_dict</cite>.</p>
<p>This function is called from <a class="reference internal" href="#stable_pretraining.Module.load_state_dict" title="stable_pretraining.Module.load_state_dict"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_state_dict()</span></code></a> to handle any extra state
found within the <cite>state_dict</cite>. Implement this function and a corresponding
<a class="reference internal" href="#stable_pretraining.Module.get_extra_state" title="stable_pretraining.Module.get_extra_state"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_extra_state()</span></code></a> for your module if you need to store extra state within its
<cite>state_dict</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>state</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a>)  Extra state from the <cite>state_dict</cite></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.set_submodule">
<span class="sig-name descname"><span class="pre">set_submodule</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">strict</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.set_submodule"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.set_submodule" title="Link to this definition">#</a></dt>
<dd><p>Set the submodule given by <code class="docutils literal notranslate"><span class="pre">target</span></code> if it exists, otherwise throw an error.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> (default), the method will replace an existing submodule
or create a new submodule if the parent module exists. If <code class="docutils literal notranslate"><span class="pre">strict</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the method will only attempt to replace an existing submodule and throw an error if
the submodule does not exist.</p>
</div>
<p>For example, lets say you have an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code> that
looks like this:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>A(
    (net_b): Module(
        (net_c): Module(
            (conv): Conv2d(3, 3, 3)
        )
        (linear): Linear(3, 3)
    )
)
</pre></div>
</div>
<p>(The diagram shows an <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code> <code class="docutils literal notranslate"><span class="pre">A</span></code>. <code class="docutils literal notranslate"><span class="pre">A</span></code> has a nested
submodule <code class="docutils literal notranslate"><span class="pre">net_b</span></code>, which itself has two submodules <code class="docutils literal notranslate"><span class="pre">net_c</span></code>
and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. <code class="docutils literal notranslate"><span class="pre">net_c</span></code> then has a submodule <code class="docutils literal notranslate"><span class="pre">conv</span></code>.)</p>
<p>To override the <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> with a new submodule <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, you
could call <code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.net_c.conv&quot;,</span> <span class="pre">nn.Linear(1,</span> <span class="pre">1))</span></code>
where <code class="docutils literal notranslate"><span class="pre">strict</span></code> could be <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code></p>
<p>To add a new submodule <code class="docutils literal notranslate"><span class="pre">Conv2d</span></code> to the existing <code class="docutils literal notranslate"><span class="pre">net_b</span></code> module,
you would call <code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.conv&quot;,</span> <span class="pre">nn.Conv2d(1,</span> <span class="pre">1,</span> <span class="pre">1))</span></code>.</p>
<p>In the above if you set <code class="docutils literal notranslate"><span class="pre">strict=True</span></code> and call
<code class="docutils literal notranslate"><span class="pre">set_submodule(&quot;net_b.conv&quot;,</span> <span class="pre">nn.Conv2d(1,</span> <span class="pre">1,</span> <span class="pre">1),</span> <span class="pre">strict=True)</span></code>, an AttributeError
will be raised because <code class="docutils literal notranslate"><span class="pre">net_b</span></code> does not have a submodule named <code class="docutils literal notranslate"><span class="pre">conv</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>target</strong>  The fully-qualified string name of the submodule
to look for. (See above example for how to specify a
fully-qualified string.)</p></li>
<li><p><strong>module</strong>  The module to set the submodule to.</p></li>
<li><p><strong>strict</strong>  If <code class="docutils literal notranslate"><span class="pre">False</span></code>, the method will replace an existing submodule
or create a new submodule if the parent module exists. If <code class="docutils literal notranslate"><span class="pre">True</span></code>,
the method will only attempt to replace an existing submodule and throw an error
if the submodule doesnt already exist.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a>  If the <code class="docutils literal notranslate"><span class="pre">target</span></code> string is empty or if <code class="docutils literal notranslate"><span class="pre">module</span></code> is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.14)"><strong>AttributeError</strong></a>  If at any point along the path resulting from
    the <code class="docutils literal notranslate"><span class="pre">target</span></code> string the (sub)path resolves to a non-existent
    attribute name or an object that is not an instance of <code class="docutils literal notranslate"><span class="pre">nn.Module</span></code>.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.setup" title="Link to this definition">#</a></dt>
<dd><p>Called at the beginning of fit (train + validate), validate, test, or predict. This is a good hook when you
need to build models dynamically or adjust something about them. This hook is called on every process when
using DDP.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong>  either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LitModel</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">download_data</span><span class="p">()</span>
        <span class="n">tokenize</span><span class="p">()</span>

        <span class="c1"># don&#39;t do this</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">something</span> <span class="o">=</span> <span class="k">else</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">load_data</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.share_memory">
<span class="sig-name descname"><span class="pre">share_memory</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.share_memory"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.share_memory" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.Tensor.share_memory_()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.state_dict">
<span class="sig-name descname"><span class="pre">state_dict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destination</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">prefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keep_vars</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.state_dict"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.state_dict" title="Link to this definition">#</a></dt>
<dd><p>Return a dictionary containing references to the whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.
Parameters and buffers set to <code class="docutils literal notranslate"><span class="pre">None</span></code> are not included.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The returned object is a shallow copy. It contains references
to the modules parameters and buffers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Currently <code class="docutils literal notranslate"><span class="pre">state_dict()</span></code> also accepts positional arguments for
<code class="docutils literal notranslate"><span class="pre">destination</span></code>, <code class="docutils literal notranslate"><span class="pre">prefix</span></code> and <code class="docutils literal notranslate"><span class="pre">keep_vars</span></code> in order. However,
this is being deprecated and keyword arguments will be enforced in
future releases.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please avoid the use of argument <code class="docutils literal notranslate"><span class="pre">destination</span></code> as it is not
designed for end-users.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>destination</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><em>dict</em></a><em>, </em><em>optional</em>)  If provided, the state of module will
be updated into the dict and the same object is returned.
Otherwise, an <code class="docutils literal notranslate"><span class="pre">OrderedDict</span></code> will be created and returned.
Default: <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p><strong>prefix</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><em>str</em></a><em>, </em><em>optional</em>)  a prefix added to parameter and buffer
names to compose the keys in state_dict. Default: <code class="docutils literal notranslate"><span class="pre">''</span></code>.</p></li>
<li><p><strong>keep_vars</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  by default the <a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> s
returned in the state dict are detached from autograd. If its
set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, detaching will not be performed.
Default: <code class="docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)">dict</a></p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined vars&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">[&#39;bias&#39;, &#39;weight&#39;]</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.teardown">
<span class="sig-name descname"><span class="pre">teardown</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.teardown" title="Link to this definition">#</a></dt>
<dd><p>Called at the end of fit (train + validate), validate, test, or predict.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>stage</strong>  either <code class="docutils literal notranslate"><span class="pre">'fit'</span></code>, <code class="docutils literal notranslate"><span class="pre">'validate'</span></code>, <code class="docutils literal notranslate"><span class="pre">'test'</span></code>, or <code class="docutils literal notranslate"><span class="pre">'predict'</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.test_dataloader">
<span class="sig-name descname"><span class="pre">test_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.test_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying test samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">test()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need a test dataset and a <a class="reference internal" href="#stable_pretraining.Module.test_step" title="stable_pretraining.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a>, you dont need to implement
this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.test_step">
<span class="sig-name descname"><span class="pre">test_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.test_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.test_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the test set. In this step youd normally generate examples or
calculate anything of interest such as accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one test dataloader:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple test dataloaders:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single test dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">test_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;test_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;test_acc&#39;</span><span class="p">:</span> <span class="n">test_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple test dataloaders, <a class="reference internal" href="#stable_pretraining.Module.test_step" title="stable_pretraining.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple test dataloaders</span>
<span class="k">def</span><span class="w"> </span><span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;test_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;test_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need to test you dont need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#stable_pretraining.Module.test_step" title="stable_pretraining.Module.test_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">test_step()</span></code></a> is called, the model has been put in eval mode and
PyTorch gradients have been disabled. At the end of the test epoch, the model goes back
to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.to">
<span class="sig-name descname"><span class="pre">to</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.to" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.to" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.to()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.to_empty">
<span class="sig-name descname"><span class="pre">to_empty</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="keyword-only-separator o"><abbr title="Keyword-only parameters separator (PEP 3102)"><span class="pre">*</span></abbr></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">recurse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.to_empty"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.to_empty" title="Link to this definition">#</a></dt>
<dd><p>Move the parameters and buffers to the specified device without copying storage.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>device</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.device</span></code></a>)  The desired device of the parameters
and buffers in this module.</p></li>
<li><p><strong>recurse</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  Whether parameters and buffers of submodules should
be recursively moved to the specified device.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.to_onnx">
<span class="sig-name descname"><span class="pre">to_onnx</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/io.html#io.BytesIO" title="(in Python v3.14)"><span class="pre">BytesIO</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_sample</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ONNXProgram</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.to_onnx" title="Link to this definition">#</a></dt>
<dd><p>Saves the model in ONNX format.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong>  The path of the file the onnx model should be saved to. Default: None (no file saved).</p></li>
<li><p><strong>input_sample</strong>  An input for tracing. Default: None (Use self.example_input_array)</p></li>
<li><p><strong>**kwargs</strong>  Will be passed to torch.onnx.export function.</p></li>
</ul>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">input_sample</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_onnx</span><span class="p">(</span><span class="s2">&quot;export.onnx&quot;</span><span class="p">,</span> <span class="n">input_sample</span><span class="p">,</span> <span class="n">export_params</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.to_torchscript">
<span class="sig-name descname"><span class="pre">to_torchscript</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">file_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'script'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">example_inputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">ScriptModule</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">ScriptModule</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#stable_pretraining.Module.to_torchscript" title="Link to this definition">#</a></dt>
<dd><p>By default compiles the whole model to a <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.ScriptModule.html#torch.jit.ScriptModule" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">ScriptModule</span></code></a>. If you want to use tracing,
please provided the argument <code class="docutils literal notranslate"><span class="pre">method='trace'</span></code> and make sure that either the <cite>example_inputs</cite> argument is
provided, or the model has <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code> set. If you would like to customize the modules that are
scripted you should override this method. In case you want to return multiple modules, we recommend using a
dictionary.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>file_path</strong>  Path where to save the torchscript. Default: None (no file saved).</p></li>
<li><p><strong>method</strong>  Whether to use TorchScripts script or trace method. Default: script</p></li>
<li><p><strong>example_inputs</strong>  An input to be used to do tracing when method is set to trace.
Default: None (uses <code class="xref py py-attr docutils literal notranslate"><span class="pre">example_input_array</span></code>)</p></li>
<li><p><strong>**kwargs</strong>  Additional arguments that will be passed to the <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.script.html#torch.jit.script" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.script()</span></code></a> or
<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.jit.trace.html#torch.jit.trace" title="(in PyTorch v2.9)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.jit.trace()</span></code></a> function.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Requires the implementation of the
<code class="xref py py-meth docutils literal notranslate"><span class="pre">forward()</span></code> method.</p></li>
<li><p>The exported script will be set to evaluation mode.</p></li>
<li><p>It is recommended that you install the latest supported version of PyTorch
to use this feature without limitations. See also the <a class="reference external" href="https://docs.pytorch.org/docs/stable/jit.html#module-torch.jit" title="(in PyTorch v2.9)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">torch.jit</span></code></a>
documentation for supported features.</p></li>
</ul>
</div>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">SimpleModel</span><span class="p">(</span><span class="n">LightningModule</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleModel</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span><span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model.pt&quot;</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">to_torchscript</span><span class="p">(</span>
    <span class="n">file_path</span><span class="o">=</span><span class="s2">&quot;model_trace.pt&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;trace&#39;</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">64</span><span class="p">))</span>
<span class="p">)</span>
</pre></div>
</div>
<dl class="field-list simple">
<dt class="field-odd">Returns<span class="colon">:</span></dt>
<dd class="field-odd"><p>This LightningModule as a torchscript, regardless of whether <cite>file_path</cite> is
defined or not.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.toggle_optimizer">
<span class="sig-name descname"><span class="pre">toggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.toggle_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Makes sure only the gradients of the current optimizers parameters are calculated in the training step to
prevent dangling gradients in multiple-optimizer setup.</p>
<p>It works with <a class="reference internal" href="#stable_pretraining.Module.untoggle_optimizer" title="stable_pretraining.Module.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> to make sure <code class="docutils literal notranslate"><span class="pre">param_requires_grad_state</span></code> is properly reset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to toggle.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.toggled_optimizer">
<span class="sig-name descname"><span class="pre">toggled_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/collections.abc.html#collections.abc.Generator" title="(in Python v3.14)"><span class="pre">Generator</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.toggled_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Makes sure only the gradients of the current optimizers parameters are calculated in the training step to
prevent dangling gradients in multiple-optimizer setup. Combines <a class="reference internal" href="#stable_pretraining.Module.toggle_optimizer" title="stable_pretraining.Module.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a> and
<a class="reference internal" href="#stable_pretraining.Module.untoggle_optimizer" title="stable_pretraining.Module.untoggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untoggle_optimizer()</span></code></a> into context manager.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to toggle.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">training_step</span><span class="p">(</span><span class="o">...</span><span class="p">):</span>
    <span class="n">opt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizers</span><span class="p">()</span>
    <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">toggled_optimizer</span><span class="p">(</span><span class="n">opt</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">...</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">manual_backward</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.train">
<span class="sig-name descname"><span class="pre">train</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.train"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.train" title="Link to this definition">#</a></dt>
<dd><p>Set the module in training mode.</p>
<p>This has an effect only on certain modules. See the documentation of
particular modules for details of their behaviors in training/evaluation
mode, i.e., whether they are affected, e.g. <code class="xref py py-class docutils literal notranslate"><span class="pre">Dropout</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">BatchNorm</span></code>,
etc.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>mode</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  whether to set training mode (<code class="docutils literal notranslate"><span class="pre">True</span></code>) or evaluation
mode (<code class="docutils literal notranslate"><span class="pre">False</span></code>). Default: <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.train_dataloader">
<span class="sig-name descname"><span class="pre">train_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.train_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying training samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id15"><span class="problematic" id="id16">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>For data processing use the following pattern:</p>
<blockquote>
<div><ul class="simple">
<li><p>download in <a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p>process and split in <a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
</div></blockquote>
<p>However, the above are only necessary for distributed processing.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>do not assign state in prepare_data</p>
</div>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware.
There is no need to set it yourself.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.training_step">
<span class="sig-name descname"><span class="pre">training_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.training_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.training_step" title="Link to this definition">#</a></dt>
<dd><p>Manual optimization training step with support for multiple optimizers.</p>
<p>Expected output from forward during training (stage=fit):
- state[loss]: torch.Tensor - Single joint loss for all optimizers</p>
<p>When multiple optimizers are configured, the same loss is used for all of them.
Each optimizer updates its assigned parameters based on gradients from this joint loss.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.transfer_batch_to_device">
<span class="sig-name descname"><span class="pre">transfer_batch_to_device</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.transfer_batch_to_device" title="Link to this definition">#</a></dt>
<dd><p>Override this hook if your <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a> returns tensors wrapped in a custom data
structure.</p>
<p>The data types listed below (and any arbitrary nesting of them) are supported out of the box:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.Tensor</span></code></a> or anything that implements <cite>.to()</cite></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">list</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">dict</span></code></a></p></li>
<li><p><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></a></p></li>
</ul>
<p>For anything else, you need to define how the data is moved to the target device (CPU, GPU, TPU, ).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This hook should only transfer the data and not modify it, nor should it move the data to
any other device than the one passed in as argument (unless you know what you are doing).
To check the current state of execution of this hook you can use
<code class="docutils literal notranslate"><span class="pre">self.trainer.training/testing/validating/predicting</span></code> so that you can
add different logic as per your requirement.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  A batch of data that needs to be transferred to a new device.</p></li>
<li><p><strong>device</strong>  The target device as defined in PyTorch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader to which the batch belongs.</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>A reference to the data on the new device.</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">transfer_batch_to_device</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">CustomBatch</span><span class="p">):</span>
        <span class="c1"># move all tensors in your custom data structure to the device</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">samples</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">samples</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">batch</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">batch</span><span class="o">.</span><span class="n">targets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># skip device transfer for the first dataloader or anything you wish</span>
        <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">transfer_batch_to_device</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">device</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">batch</span>
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">move_data_to_device()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_to_collection()</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.type">
<span class="sig-name descname"><span class="pre">type</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">dst_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.dtype" title="(in PyTorch v2.9)"><span class="pre">dtype</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.type" title="Link to this definition">#</a></dt>
<dd><p>See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.type" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.nn.Module.type()</span></code></a>.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.unfreeze">
<span class="sig-name descname"><span class="pre">unfreeze</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.unfreeze" title="Link to this definition">#</a></dt>
<dd><p>Unfreeze all parameters for training.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">MyLightningModule</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">unfreeze</span><span class="p">()</span>
</pre></div>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.untoggle_optimizer">
<span class="sig-name descname"><span class="pre">untoggle_optimizer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">LightningOptimizer</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.untoggle_optimizer" title="Link to this definition">#</a></dt>
<dd><p>Resets the state of required gradients that were toggled with <a class="reference internal" href="#stable_pretraining.Module.toggle_optimizer" title="stable_pretraining.Module.toggle_optimizer"><code class="xref py py-meth docutils literal notranslate"><span class="pre">toggle_optimizer()</span></code></a>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>optimizer</strong>  The optimizer to untoggle.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.val_dataloader">
<span class="sig-name descname"><span class="pre">val_dataloader</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.14)"><span class="pre">Any</span></a></span></span><a class="headerlink" href="#stable_pretraining.Module.val_dataloader" title="Link to this definition">#</a></dt>
<dd><p>An iterable or collection of iterables specifying validation samples.</p>
<p>For more information about multiple dataloaders, see this <span class="xref std std-ref">section</span>.</p>
<p>The dataloader you return will not be reloaded unless you set
<a href="#id17"><span class="problematic" id="id18">:paramref:`~lightning.pytorch.trainer.trainer.Trainer.reload_dataloaders_every_n_epochs`</span></a> to
a positive integer.</p>
<p>Its recommended that all data downloads and preparation happen in <a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a>.</p>
<ul class="simple">
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">fit()</span></code></p></li>
<li><p><code class="xref py py-meth docutils literal notranslate"><span class="pre">validate()</span></code></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.prepare_data" title="stable_pretraining.Module.prepare_data"><code class="xref py py-meth docutils literal notranslate"><span class="pre">prepare_data()</span></code></a></p></li>
<li><p><a class="reference internal" href="#stable_pretraining.Module.setup" title="stable_pretraining.Module.setup"><code class="xref py py-meth docutils literal notranslate"><span class="pre">setup()</span></code></a></p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Lightning tries to add the correct sampler for distributed and arbitrary hardware
There is no need to set it yourself.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need a validation dataset and a <a class="reference internal" href="#stable_pretraining.Module.validation_step" title="stable_pretraining.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a>, you dont need to
implement this method.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.validation_step">
<span class="sig-name descname"><span class="pre">validation_step</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/module/#Module.validation_step"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.validation_step" title="Link to this definition">#</a></dt>
<dd><p>Operates on a single batch of data from the validation set. In this step youd might generate examples or
calculate anything of interest like accuracy.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>batch</strong>  The output of your data iterable, normally a <a class="reference external" href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code></a>.</p></li>
<li><p><strong>batch_idx</strong>  The index of this batch.</p></li>
<li><p><strong>dataloader_idx</strong>  The index of the dataloader that produced this batch.
(only if multiple dataloaders used)</p></li>
</ul>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p><ul class="simple">
<li><p><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensors.html#torch.Tensor" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></a> - The loss tensor</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dict</span></code> - A dictionary. Can include any keys, but must include the key <code class="docutils literal notranslate"><span class="pre">'loss'</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">None</span></code> - Skip to the next batch.</p></li>
</ul>
</p>
</dd>
</dl>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># if you have one val dataloader:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span> <span class="o">...</span>


<span class="c1"># if you have multiple val dataloaders:</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
<p>Examples:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 1: A single validation dataset</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># log 6 example images</span>
    <span class="c1"># or generated text... or whatever</span>
    <span class="n">sample_imgs</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="mi">6</span><span class="p">]</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">torchvision</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">make_grid</span><span class="p">(</span><span class="n">sample_imgs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">add_image</span><span class="p">(</span><span class="s1">&#39;example_images&#39;</span><span class="p">,</span> <span class="n">grid</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs!</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="s1">&#39;val_loss&#39;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;val_acc&#39;</span><span class="p">:</span> <span class="n">val_acc</span><span class="p">})</span>
</pre></div>
</div>
<p>If you pass in multiple val dataloaders, <a class="reference internal" href="#stable_pretraining.Module.validation_step" title="stable_pretraining.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> will have an additional argument. We recommend
setting the default value of 0 so that you can quickly switch between single and multiple dataloaders.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># CASE 2: multiple validation dataloaders</span>
<span class="k">def</span><span class="w"> </span><span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="n">dataloader_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="c1"># dataloader_idx tells you which dataset this is.</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">batch</span>

    <span class="c1"># implement your own</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataloader_idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss0</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss1</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># calculate acc</span>
    <span class="n">labels_hat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">acc</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">y</span> <span class="o">==</span> <span class="n">labels_hat</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mf">1.0</span><span class="p">)</span>

    <span class="c1"># log the outputs separately for each dataloader</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">({</span><span class="sa">f</span><span class="s2">&quot;val_loss_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;val_acc_</span><span class="si">{</span><span class="n">dataloader_idx</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">acc</span><span class="p">})</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you dont need to validate you dont need to implement this method.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When the <a class="reference internal" href="#stable_pretraining.Module.validation_step" title="stable_pretraining.Module.validation_step"><code class="xref py py-meth docutils literal notranslate"><span class="pre">validation_step()</span></code></a> is called, the model has been put in eval mode
and PyTorch gradients have been disabled. At the end of validation,
the model goes back to training mode and gradients are enabled.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.xpu">
<span class="sig-name descname"><span class="pre">xpu</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/tensor_attributes.html#torch.device" title="(in PyTorch v2.9)"><span class="pre">device</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Self" title="(in Python v3.14)"><span class="pre">Self</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.xpu"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.xpu" title="Link to this definition">#</a></dt>
<dd><p>Move all model parameters and buffers to the XPU.</p>
<p>This also makes associated parameters and buffers different objects. So
it should be called before constructing optimizer if the module will
live on XPU while being optimized.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This method modifies the module in-place.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a><em>, </em><em>optional</em>)  if specified, all parameters will be
copied to that device</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>self</p>
</dd>
<dt class="field-odd">Return type<span class="colon">:</span></dt>
<dd class="field-odd"><p><a class="reference internal" href="#stable_pretraining.Module" title="stable_pretraining.Module">Module</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.Module.zero_grad">
<span class="sig-name descname"><span class="pre">zero_grad</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">set_to_none</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/torch/nn/modules/module/#Module.zero_grad"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.Module.zero_grad" title="Link to this definition">#</a></dt>
<dd><p>Reset gradients of all model parameters.</p>
<p>See similar function under <a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code></a> for more context.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>set_to_none</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  instead of setting to zero, set the grads to None.
See <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(in PyTorch v2.9)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.optim.Optimizer.zero_grad()</span></code></a> for details.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.ModuleSummary">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">ModuleSummary</span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#ModuleSummary"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.ModuleSummary" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Logs detailed module parameter statistics in a formatted table.</p>
<p>This callback provides a comprehensive overview of all modules in the model,
showing the number of trainable, non-trainable, uninitialized parameters,
and buffers for each module. This helps understand model architecture and
parameter distribution.</p>
<p>The summary is displayed during the setup phase and includes:
- Module name and hierarchy
- Trainable parameter count
- Non-trainable (frozen) parameter count
- Uninitialized parameter count (for lazy modules)
- Buffer count (non-parameter persistent state)</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.ModuleSummary.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#ModuleSummary.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.ModuleSummary.setup" title="Link to this definition">#</a></dt>
<dd><p>Called when fit, validate, test, predict, or tune begins.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.OnlineKNN">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">OnlineKNN</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.14)"><span class="pre">Dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">input_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Tuple" title="(in Python v3.14)"><span class="pre">Tuple</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="p"><span class="pre">...</span></span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.List" title="(in Python v3.14)"><span class="pre">List</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_dim</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">k</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">temperature</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.07</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">chunk_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">distance_metric</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Literal" title="(in Python v3.14)"><span class="pre">Literal</span></a><span class="p"><span class="pre">[</span></span><span class="s"><span class="pre">'euclidean'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'squared_euclidean'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'cosine'</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="s"><span class="pre">'manhattan'</span></span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'euclidean'</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/knn/#OnlineKNN"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineKNN" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Weighted K-Nearest Neighbors online evaluator using queue discovery.</p>
<p>This callback implements a weighted KNN classifier that evaluates the quality of
learned representations during training. It automatically discovers or creates
OnlineQueue callbacks to maintain circular buffers of features and labels, then
uses this cached data to compute KNN predictions during validation.</p>
<p>The KNN evaluation is performed by:
1. Finding k nearest neighbors in the feature space
2. Weighting neighbors by inverse distance with temperature scaling
3. Using weighted voting to produce class predictions
4. Computing specified metrics on the predictions</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong>  Unique identifier for this callback instance. Used for logging and
storing metrics.</p></li>
<li><p><strong>input</strong>  Key in batch dict containing input features to evaluate.</p></li>
<li><p><strong>target</strong>  Key in batch dict containing ground truth target labels.</p></li>
<li><p><strong>queue_length</strong>  Size of the circular buffer for caching features and labels.
Larger values provide more representative samples but use more memory.</p></li>
<li><p><strong>metrics</strong>  Dictionary of metrics to compute during validation. Keys are metric
names, values are metric instances (e.g., torchmetrics.Accuracy).</p></li>
<li><p><strong>input_dim</strong>  Expected dimensionality of input features. Can be int, tuple/list
(will be flattened to product), or None to accept any dimension.</p></li>
<li><p><strong>target_dim</strong>  Expected dimensionality of targets. None accepts any dimension.</p></li>
<li><p><strong>k</strong>  Number of nearest neighbors to consider for voting. Default is 5.</p></li>
<li><p><strong>temperature</strong>  Temperature parameter for distance weighting. Lower values give
more weight to closer neighbors. Default is 0.07.</p></li>
<li><p><strong>chunk_size</strong>  Batch size for memory-efficient distance computation. Set to -1
to compute all distances at once. Default is -1.</p></li>
<li><p><strong>distance_metric</strong>  Distance metric for finding nearest neighbors. Options are
euclidean, squared_euclidean, cosine, manhattan. Default is euclidean.</p></li>
</ul>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><a class="reference external" href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.14)"><strong>ValueError</strong></a>  If k &lt;= 0, temperature &lt;= 0, or chunk_size is invalid.</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The callback automatically handles distributed training by gathering data</p></li>
<li><p>Mixed precision is supported through automatic dtype conversion</p></li>
<li><p>Predictions are stored in batch dict with key {name}_preds</p></li>
<li><p>Metrics are logged with prefix eval/{name}_</p></li>
</ul>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineKNN.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.14)"><span class="pre">Dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.14)"><span class="pre">Dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/knn/#OnlineKNN.on_validation_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineKNN.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Compute KNN predictions during validation.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineKNN.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/knn/#OnlineKNN.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineKNN.setup" title="Link to this definition">#</a></dt>
<dd><p>Find or create queue callbacks and setup metrics.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.OnlineKNN.state_key">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">state_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#stable_pretraining.OnlineKNN.state_key" title="Link to this definition">#</a></dt>
<dd><p>Unique identifier for this callbacks state during checkpointing.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.OnlineProbe">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">OnlineProbe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">input</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">probe</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss_fn</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">callable</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">optimizer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">partial</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/optim.html#torch.optim.Optimizer" title="(in PyTorch v2.9)"><span class="pre">Optimizer</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scheduler</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">partial</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.LRScheduler.html#torch.optim.lr_scheduler.LRScheduler" title="(in PyTorch v2.9)"><span class="pre">LRScheduler</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">accumulate_grad_batches</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_val</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_clip_algorithm</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'norm'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">metrics</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.14)"><span class="pre">tuple</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Metric</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/probe/#OnlineProbe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineProbe" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference internal" href="../stable_pretraining.callbacks/#stable_pretraining.callbacks.utils.TrainableCallback" title="stable_pretraining.callbacks.utils.TrainableCallback"><code class="xref py py-class docutils literal notranslate"><span class="pre">TrainableCallback</span></code></a></p>
<p>Online probe for evaluating learned representations during self-supervised training.</p>
<p>This callback implements the standard linear evaluation protocol by training a probe
(typically a linear classifier) on top of frozen features from the main model. The probe
is trained simultaneously with the main model but maintains its own optimizer, scheduler,
and training loop. This allows monitoring representation quality throughout training
without modifying the base model.</p>
<p>Key features:
- Automatic gradient detachment to prevent probe gradients affecting the main model
- Independent optimizer and scheduler management
- Support for gradient accumulation
- Mixed precision training compatibility through automatic dtype conversion
- Metric tracking and logging</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong>  The spt.LightningModule to probe.</p></li>
<li><p><strong>name</strong>  Unique identifier for this probe instance. Used for logging and storing
metrics/modules.</p></li>
<li><p><strong>input</strong>  Key in batch dict or outputs dict containing input features to probe.</p></li>
<li><p><strong>target</strong>  Key in batch dict containing ground truth target labels.</p></li>
<li><p><strong>probe</strong>  The probe module to train. Can be a nn.Module instance, callable that
returns a module, or Hydra config to instantiate.</p></li>
<li><p><strong>loss_fn</strong>  Loss function for probe training (e.g., nn.CrossEntropyLoss()).</p></li>
<li><p><strong>optimizer</strong>  <p>Optimizer configuration for the probe. Can be:
- str: optimizer name (e.g., AdamW, SGD, LARS)
- dict: {type: AdamW, lr: 1e-3, }
- partial: pre-configured optimizer factory
- optimizer instance or callable
- None: uses LARS(lr=0.1, clip_lr=True, eta=0.02, exclude_bias_n_norm=True,</p>
<blockquote>
<div><p>weight_decay=0), which is the standard for SSL linear probes (default)</p>
</div></blockquote>
</p></li>
<li><p><strong>scheduler</strong>  Learning rate scheduler configuration. Can be:
- str: scheduler name (e.g., CosineAnnealingLR, StepLR)
- dict: {type: CosineAnnealingLR, T_max: 1000, }
- partial: pre-configured scheduler factory
- scheduler instance or callable
- None: uses ConstantLR(factor=1.0), maintaining constant learning rate (default)</p></li>
<li><p><strong>accumulate_grad_batches</strong>  Number of batches to accumulate gradients before
optimizer step. Default is 1 (no accumulation).</p></li>
<li><p><strong>metrics</strong>  Metrics to track during training/validation. Can be dict, list, tuple,
or single metric instance.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The probe module is stored in pl_module.callbacks_modules[name]</p></li>
<li><p>Metrics are stored in pl_module.callbacks_metrics[name]</p></li>
<li><p>Predictions are stored in batch dict with key {name}_preds</p></li>
<li><p>Loss is logged as train/{name}_loss</p></li>
<li><p>Metrics are logged with prefix train/{name}_ and eval/{name}_</p></li>
</ul>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineProbe.configure_model">
<span class="sig-name descname"><span class="pre">configure_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/probe/#OnlineProbe.configure_model"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineProbe.configure_model" title="Link to this definition">#</a></dt>
<dd><p>Initialize the probe module from configuration.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineProbe.wrap_forward">
<span class="sig-name descname"><span class="pre">wrap_forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/probe/#OnlineProbe.wrap_forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineProbe.wrap_forward" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">OnlineWriter</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.14)"><span class="pre">Path</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">during</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.14)"><span class="pre">list</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">every_k_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">-1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_last_epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_sanity_check</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">all_gather</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Writes specified batch data to disk during training and validation.</p>
<p>This callback enables selective saving of batch data (e.g., features, predictions,
embeddings) to disk at specified intervals during training. Its useful for
debugging, visualization, and analysis of model behavior during training.</p>
<p>Features:
- Flexible saving schedule (every k epochs, last epoch, sanity check)
- Support for distributed training with optional all_gather
- Automatic directory creation
- Configurable for different training phases (train, val, test)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>names</strong>  Name(s) of the batch keys to save. Can be string or list of strings.</p></li>
<li><p><strong>path</strong>  Directory path where files will be saved.</p></li>
<li><p><strong>during</strong>  Training phase(s) when to save (train, val, test, or list).</p></li>
<li><p><strong>every_k_epochs</strong>  Save every k epochs. -1 means every epoch.</p></li>
<li><p><strong>save_last_epoch</strong>  Whether to save on the last training epoch.</p></li>
<li><p><strong>save_sanity_check</strong>  Whether to save during sanity check phase.</p></li>
<li><p><strong>all_gather</strong>  Whether to gather data across all distributed processes.</p></li>
</ul>
</dd>
</dl>
<p>Files are saved with naming pattern: {phase}_{name}_epoch{epoch}_batch{batch}.pt</p>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.dump">
<span class="sig-name descname"><span class="pre">dump</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.dump"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.dump" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.is_writing_epoch">
<span class="sig-name descname"><span class="pre">is_writing_epoch</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.is_writing_epoch"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.is_writing_epoch" title="Link to this definition">#</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_predict_batch_end">
<span class="sig-name descname"><span class="pre">on_predict_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_predict_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_predict_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the predict batch ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_sanity_check_end">
<span class="sig-name descname"><span class="pre">on_sanity_check_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_sanity_check_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_sanity_check_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation sanity check ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_sanity_check_start">
<span class="sig-name descname"><span class="pre">on_sanity_check_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_sanity_check_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_sanity_check_start" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation sanity check starts.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_test_batch_end">
<span class="sig-name descname"><span class="pre">on_test_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_test_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_test_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the test batch ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_train_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_train_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the train batch ends.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The value <code class="docutils literal notranslate"><span class="pre">outputs[&quot;loss&quot;]</span></code> here will be the normalized value w.r.t <code class="docutils literal notranslate"><span class="pre">accumulate_grad_batches</span></code> of the
loss returned from <code class="docutils literal notranslate"><span class="pre">training_step</span></code>.</p>
</div>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.on_validation_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Called when the validation batch ends.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.OnlineWriter.write_at_phase">
<span class="sig-name descname"><span class="pre">write_at_phase</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">phase_name</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/writer/#OnlineWriter.write_at_phase"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.OnlineWriter.write_at_phase" title="Link to this definition">#</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.RankMe">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">RankMe</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">queue_length</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">target_shape</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.14)"><span class="pre">Iterable</span></a><span class="p"><span class="pre">[</span></span><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a><span class="p"><span class="pre">]</span></span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/rankme/#RankMe"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.RankMe" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>RankMe (effective rank) monitor using queue discovery.</p>
<p>RankMe measures the effective rank of feature representations by computing
the exponential of the entropy of normalized singular values. This metric
helps detect dimensional collapse in self-supervised learning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong>  Unique name for this callback instance</p></li>
<li><p><strong>target</strong>  Key in batch dict containing the feature embeddings to monitor</p></li>
<li><p><strong>queue_length</strong>  Required queue length</p></li>
<li><p><strong>target_shape</strong>  Shape of the target embeddings (e.g., 768 for 768-dim features)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.RankMe.on_validation_batch_end">
<span class="sig-name descname"><span class="pre">on_validation_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.14)"><span class="pre">dict</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">dataloader_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/rankme/#RankMe.on_validation_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.RankMe.on_validation_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Compute RankMe metric on the first validation batch only.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.RankMe.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/rankme/#RankMe.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.RankMe.setup" title="Link to this definition">#</a></dt>
<dd><p>Find or create the queue callback for target features.</p>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="stable_pretraining.RankMe.state_key">
<em class="property"><span class="k"><span class="pre">property</span></span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">state_key</span></span><em class="property"><span class="p"><span class="pre">:</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a></em><a class="headerlink" href="#stable_pretraining.RankMe.state_key" title="Link to this definition">#</a></dt>
<dd><p>Unique identifier for this callbacks state during checkpointing.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.SklearnCheckpoint">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">SklearnCheckpoint</span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/checkpoint_sklearn/#SklearnCheckpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.SklearnCheckpoint" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Callback for saving and loading sklearn models in PyTorch Lightning checkpoints.</p>
<p>This callback automatically detects sklearn models (Regressors and Classifiers)
attached to the Lightning module and handles their serialization/deserialization
during checkpoint save/load operations. This is necessary because sklearn models
are not natively supported by PyTorchs checkpoint system.</p>
<p>The callback will:
1. Automatically discover sklearn models attached to the Lightning module
2. Save them to the checkpoint dictionary during checkpoint saving
3. Restore them from the checkpoint during checkpoint loading
4. Log information about discovered sklearn modules during setup</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Only attributes that are sklearn RegressorMixin or ClassifierMixin instances are saved</p></li>
<li><p>Private attributes (starting with _) are ignored</p></li>
<li><p>The callback will raise an error if a sklearn model name conflicts with existing checkpoint keys</p></li>
</ul>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.SklearnCheckpoint.on_load_checkpoint">
<span class="sig-name descname"><span class="pre">on_load_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/checkpoint_sklearn/#SklearnCheckpoint.on_load_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.SklearnCheckpoint.on_load_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called when loading a model checkpoint, use to reload state.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong>  the current <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> instance.</p></li>
<li><p><strong>pl_module</strong>  the current <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance.</p></li>
<li><p><strong>checkpoint</strong>  the full checkpoint dictionary that got loaded by the Trainer.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.SklearnCheckpoint.on_save_checkpoint">
<span class="sig-name descname"><span class="pre">on_save_checkpoint</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">checkpoint</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/checkpoint_sklearn/#SklearnCheckpoint.on_save_checkpoint"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.SklearnCheckpoint.on_save_checkpoint" title="Link to this definition">#</a></dt>
<dd><p>Called when saving a checkpoint to give you a chance to store anything else you might want to save.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>trainer</strong>  the current <code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code> instance.</p></li>
<li><p><strong>pl_module</strong>  the current <code class="xref py py-class docutils literal notranslate"><span class="pre">LightningModule</span></code> instance.</p></li>
<li><p><strong>checkpoint</strong>  the checkpoint dictionary that will be saved.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.SklearnCheckpoint.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.14)"><span class="pre">str</span></a><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/checkpoint_sklearn/#SklearnCheckpoint.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.SklearnCheckpoint.setup" title="Link to this definition">#</a></dt>
<dd><p>Called when fit, validate, test, predict, or tune begins.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentCallback">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">TeacherStudentCallback</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">update_frequency</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">update_after_backward</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/teacher_student/#TeacherStudentCallback"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentCallback" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Automatically updates TeacherStudentWrapper instances during training.</p>
<p>This callback handles the EMA (Exponential Moving Average) updates for any
TeacherStudentWrapper instances found in the model. It updates both the teacher
parameters and the EMA coefficient schedule.</p>
<p>The callback automatically detects all TeacherStudentWrapper instances in the
model hierarchy and updates them at the appropriate times during training.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>update_frequency</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  How often to update the teacher (in batches).
Default is 1 (every batch).</p></li>
<li><p><strong>update_after_backward</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a>)  If True, updates happen after backward pass.
If False, updates happen after optimizer step. Default is True.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">backbone</span> <span class="o">=</span> <span class="n">ResNet18</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">wrapped_backbone</span> <span class="o">=</span> <span class="n">TeacherStudentWrapper</span><span class="p">(</span><span class="n">backbone</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">module</span> <span class="o">=</span> <span class="n">ssl</span><span class="o">.</span><span class="n">Module</span><span class="p">(</span><span class="n">backbone</span><span class="o">=</span><span class="n">wrapped_backbone</span><span class="p">,</span> <span class="o">...</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">TeacherStudentCallback</span><span class="p">()])</span>
</pre></div>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentCallback.on_after_backward">
<span class="sig-name descname"><span class="pre">on_after_backward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/teacher_student/#TeacherStudentCallback.on_after_backward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentCallback.on_after_backward" title="Link to this definition">#</a></dt>
<dd><p>Update teacher models after backward pass if update_after_backward is True.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentCallback.on_fit_start">
<span class="sig-name descname"><span class="pre">on_fit_start</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/teacher_student/#TeacherStudentCallback.on_fit_start"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentCallback.on_fit_start" title="Link to this definition">#</a></dt>
<dd><p>Log if TeacherStudentWrapper instances are found.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentCallback.on_train_batch_end">
<span class="sig-name descname"><span class="pre">on_train_batch_end</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">LightningModule</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_idx</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><a class="reference external" href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.14)"><span class="pre">None</span></a></span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/teacher_student/#TeacherStudentCallback.on_train_batch_end"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentCallback.on_train_batch_end" title="Link to this definition">#</a></dt>
<dd><p>Update teacher models after training batch if update_after_backward is False.</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">TeacherStudentWrapper</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">student</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><span class="pre">Module</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">warm_init</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><span class="pre">bool</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">base_ema_coefficient</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.996</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">final_ema_coefficient</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><span class="pre">float</span></a></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper" title="Link to this definition">#</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code></a></p>
<p>Backbone wrapper that implements teacher-student distillation via EMA.</p>
<p>This is a wrapper for backbones that creates a teacher model as an exponential moving average (EMA) of the student model.
It should be passed as the backbone to stable_pretraining.Module and accessed via
forward_student() and forward_teacher() methods in your custom forward function.</p>
<p>The teacher model is updated by taking a running average of the students
parameters and buffers. When <cite>ema_coefficient == 0.0</cite>, the teacher and student
are literally the same object, saving memory but forward passes through the teacher
will not produce any gradients.</p>
<dl>
<dt>Usage example:</dt><dd><p>backbone = ResNet18()
wrapped_backbone = TeacherStudentWrapper(backbone)
module = ssl.Module(</p>
<blockquote>
<div><p>backbone=wrapped_backbone,
projector=projector,
forward=forward_with_teacher_student,
</p>
</div></blockquote>
<p>)</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>student</strong> (<a class="reference external" href="https://docs.pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module" title="(in PyTorch v2.9)"><em>torch.nn.Module</em></a>)  The student model whose parameters will be tracked.</p></li>
<li><p><strong>warm_init</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.14)"><em>bool</em></a><em>, </em><em>optional</em>)  If True, performs an initialization step to match the students parameters
immediately. Default is True.</p></li>
<li><p><strong>base_ema_coefficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>)  EMA decay factor at the start of training.
This value will be updated following a cosine schedule.
Should be in [0, 1]. A value of 0.0 means the teacher is fully
updated to the students parameters on every step, while a value of 1.0 means
the teacher remains unchanged.
Default is 0.996.</p></li>
<li><p><strong>final_ema_coefficient</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.14)"><em>float</em></a><em>, </em><em>optional</em>)  EMA decay factor at the end of training.
Default is 1.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper.forward">
<span class="sig-name descname"><span class="pre">forward</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper.forward"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper.forward" title="Link to this definition">#</a></dt>
<dd><p>Forward pass through either the student or teacher network.</p>
<p>You can choose which model to run in the default forward.
Commonly the teacher is evaluated, so we default to that.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper.forward_student">
<span class="sig-name descname"><span class="pre">forward_student</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper.forward_student"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper.forward_student" title="Link to this definition">#</a></dt>
<dd><p>Forward pass through the student network. Gradients will flow normally.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper.forward_teacher">
<span class="sig-name descname"><span class="pre">forward_teacher</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper.forward_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper.forward_teacher" title="Link to this definition">#</a></dt>
<dd><p>Forward pass through the teacher network.</p>
<p>By default, the teacher network does not require grad.
If ema_coefficient == 0, then teacher==student,
so we wrap in torch.no_grad() to ensure no gradients flow.</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper.update_ema_coefficient">
<span class="sig-name descname"><span class="pre">update_ema_coefficient</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">epoch</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em>, <em class="sig-param"><span class="n"><span class="pre">total_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><span class="pre">int</span></a></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper.update_ema_coefficient"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper.update_ema_coefficient" title="Link to this definition">#</a></dt>
<dd><p>Update the EMA coefficient following a cosine schedule.</p>
<dl class="simple">
<dt>The EMA coefficient is updated following a cosine schedule:</dt><dd><p>ema_coefficient = final_ema_coefficient -
0.5 * (final_ema_coefficient - base_ema_coefficient)
* (1 + cos(epoch / total_epochs * pi))</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>epoch</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  Current epoch in the training loop.</p></li>
<li><p><strong>total_epochs</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.14)"><em>int</em></a>)  Total number of epochs in the training loop.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TeacherStudentWrapper.update_teacher">
<span class="sig-name descname"><span class="pre">update_teacher</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/backbone/utils/#TeacherStudentWrapper.update_teacher"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TeacherStudentWrapper.update_teacher" title="Link to this definition">#</a></dt>
<dd><p>Perform one EMA update step on the teachers parameters.</p>
<dl class="simple">
<dt>The update rule is:</dt><dd><p>teacher_param = ema_coefficient * teacher_param
+ (1 - ema_coefficient) * student_param</p>
</dd>
</dl>
<p>This is done in a <cite>no_grad</cite> context to ensure the teachers parameters do
not accumulate gradients, but the student remains fully trainable.</p>
<p>Everything is updated, including buffers (e.g. batch norm running averages).</p>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="stable_pretraining.TrainerInfo">
<em class="property"><span class="k"><span class="pre">class</span></span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">stable_pretraining.</span></span><span class="sig-name descname"><span class="pre">TrainerInfo</span></span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#TrainerInfo"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TrainerInfo" title="Link to this definition">#</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">Callback</span></code></p>
<p>Links the trainer to the DataModule for enhanced functionality.</p>
<p>This callback establishes a bidirectional connection between the trainer
and DataModule, enabling the DataModule to access trainer information
such as device placement, distributed training state, and other runtime
configurations.</p>
<p>This is particularly useful for DataModules that need to adapt their
behavior based on trainer configuration (e.g., device-aware data loading,
distributed sampling adjustments).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Only works with DataModule instances that have a set_pl_trainer method.
A warning is logged if using a custom DataModule without this method.</p>
</div>
<dl class="py method">
<dt class="sig sig-object py" id="stable_pretraining.TrainerInfo.setup">
<span class="sig-name descname"><span class="pre">setup</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">trainer</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pl_module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">stage</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/stable_pretraining/callbacks/trainer_info/#TrainerInfo.setup"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#stable_pretraining.TrainerInfo.setup" title="Link to this definition">#</a></dt>
<dd><p>Called when fit, validate, test, predict, or tune begins.</p>
</dd></dl>

</dd></dl>

</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#subpackages">Subpackages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#submodules">Submodules</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.cli">stable_pretraining.cli module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.cli.dump_csv_logs"><code class="docutils literal notranslate"><span class="pre">dump_csv_logs()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.cli.run"><code class="docutils literal notranslate"><span class="pre">run()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.config">stable_pretraining.config module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.collapse_nested_dict"><code class="docutils literal notranslate"><span class="pre">collapse_nested_dict()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.instantiate_from_config"><code class="docutils literal notranslate"><span class="pre">instantiate_from_config()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.config.recursive_instantiate"><code class="docutils literal notranslate"><span class="pre">recursive_instantiate()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.forward">stable_pretraining.forward module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.barlow_twins_forward"><code class="docutils literal notranslate"><span class="pre">barlow_twins_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.byol_forward"><code class="docutils literal notranslate"><span class="pre">byol_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.dino_forward"><code class="docutils literal notranslate"><span class="pre">dino_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.dinov2_forward"><code class="docutils literal notranslate"><span class="pre">dinov2_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.nnclr_forward"><code class="docutils literal notranslate"><span class="pre">nnclr_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.simclr_forward"><code class="docutils literal notranslate"><span class="pre">simclr_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.supervised_forward"><code class="docutils literal notranslate"><span class="pre">supervised_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.swav_forward"><code class="docutils literal notranslate"><span class="pre">swav_forward()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.forward.vicreg_forward"><code class="docutils literal notranslate"><span class="pre">vicreg_forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.manager">stable_pretraining.manager module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager"><code class="docutils literal notranslate"><span class="pre">Manager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.init_and_sync_wandb"><code class="docutils literal notranslate"><span class="pre">Manager.init_and_sync_wandb()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.instantiated_data"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_data</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.instantiated_module"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_module</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.predict"><code class="docutils literal notranslate"><span class="pre">Manager.predict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Manager.save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.test"><code class="docutils literal notranslate"><span class="pre">Manager.test()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.Manager.validate"><code class="docutils literal notranslate"><span class="pre">Manager.validate()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.print_logger_info"><code class="docutils literal notranslate"><span class="pre">print_logger_info()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.manager.print_signal_info"><code class="docutils literal notranslate"><span class="pre">print_signal_info()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.module">stable_pretraining.module module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.add_module"><code class="docutils literal notranslate"><span class="pre">Module.add_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.all_gather"><code class="docutils literal notranslate"><span class="pre">Module.all_gather()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.apply"><code class="docutils literal notranslate"><span class="pre">Module.apply()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.backward"><code class="docutils literal notranslate"><span class="pre">Module.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.bfloat16"><code class="docutils literal notranslate"><span class="pre">Module.bfloat16()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.buffers"><code class="docutils literal notranslate"><span class="pre">Module.buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.children"><code class="docutils literal notranslate"><span class="pre">Module.children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.clip_gradients"><code class="docutils literal notranslate"><span class="pre">Module.clip_gradients()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.compile"><code class="docutils literal notranslate"><span class="pre">Module.compile()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_callbacks"><code class="docutils literal notranslate"><span class="pre">Module.configure_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_gradient_clipping"><code class="docutils literal notranslate"><span class="pre">Module.configure_gradient_clipping()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">Module.configure_optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.configure_sharded_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_sharded_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.cpu"><code class="docutils literal notranslate"><span class="pre">Module.cpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.cuda"><code class="docutils literal notranslate"><span class="pre">Module.cuda()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.double"><code class="docutils literal notranslate"><span class="pre">Module.double()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.eval"><code class="docutils literal notranslate"><span class="pre">Module.eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.extra_repr"><code class="docutils literal notranslate"><span class="pre">Module.extra_repr()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.float"><code class="docutils literal notranslate"><span class="pre">Module.float()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.freeze"><code class="docutils literal notranslate"><span class="pre">Module.freeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_buffer"><code class="docutils literal notranslate"><span class="pre">Module.get_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.get_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_parameter"><code class="docutils literal notranslate"><span class="pre">Module.get_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.get_submodule"><code class="docutils literal notranslate"><span class="pre">Module.get_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.half"><code class="docutils literal notranslate"><span class="pre">Module.half()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.ipu"><code class="docutils literal notranslate"><span class="pre">Module.ipu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.load_from_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.load_from_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.load_state_dict"><code class="docutils literal notranslate"><span class="pre">Module.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.log"><code class="docutils literal notranslate"><span class="pre">Module.log()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.log_dict"><code class="docutils literal notranslate"><span class="pre">Module.log_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.lr_scheduler_step"><code class="docutils literal notranslate"><span class="pre">Module.lr_scheduler_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.lr_schedulers"><code class="docutils literal notranslate"><span class="pre">Module.lr_schedulers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.manual_backward"><code class="docutils literal notranslate"><span class="pre">Module.manual_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.modules"><code class="docutils literal notranslate"><span class="pre">Module.modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.mtia"><code class="docutils literal notranslate"><span class="pre">Module.mtia()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_buffers"><code class="docutils literal notranslate"><span class="pre">Module.named_buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_children"><code class="docutils literal notranslate"><span class="pre">Module.named_children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_modules"><code class="docutils literal notranslate"><span class="pre">Module.named_modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.named_parameters"><code class="docutils literal notranslate"><span class="pre">Module.named_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_after_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_after_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_after_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_before_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_before_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.on_before_optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_before_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_before_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_fit_end"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_fit_start"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_predict_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_test_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_train_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_model_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.on_validation_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizer_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.optimizers"><code class="docutils literal notranslate"><span class="pre">Module.optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.parameters"><code class="docutils literal notranslate"><span class="pre">Module.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.predict_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.predict_step"><code class="docutils literal notranslate"><span class="pre">Module.predict_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.prepare_data"><code class="docutils literal notranslate"><span class="pre">Module.prepare_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.print"><code class="docutils literal notranslate"><span class="pre">Module.print()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_buffer"><code class="docutils literal notranslate"><span class="pre">Module.register_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_forward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_forward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_full_backward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_load_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_load_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_module"><code class="docutils literal notranslate"><span class="pre">Module.register_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_parameter"><code class="docutils literal notranslate"><span class="pre">Module.register_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.register_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.requires_grad_"><code class="docutils literal notranslate"><span class="pre">Module.requires_grad_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.rescale_loss_for_grad_acc"><code class="docutils literal notranslate"><span class="pre">Module.rescale_loss_for_grad_acc()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.save_hyperparameters"><code class="docutils literal notranslate"><span class="pre">Module.save_hyperparameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.set_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.set_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.set_submodule"><code class="docutils literal notranslate"><span class="pre">Module.set_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.setup"><code class="docutils literal notranslate"><span class="pre">Module.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.share_memory"><code class="docutils literal notranslate"><span class="pre">Module.share_memory()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.state_dict"><code class="docutils literal notranslate"><span class="pre">Module.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.teardown"><code class="docutils literal notranslate"><span class="pre">Module.teardown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.test_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.test_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.test_step"><code class="docutils literal notranslate"><span class="pre">Module.test_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to"><code class="docutils literal notranslate"><span class="pre">Module.to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_empty"><code class="docutils literal notranslate"><span class="pre">Module.to_empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_onnx"><code class="docutils literal notranslate"><span class="pre">Module.to_onnx()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.to_torchscript"><code class="docutils literal notranslate"><span class="pre">Module.to_torchscript()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.toggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.toggled_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggled_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.train"><code class="docutils literal notranslate"><span class="pre">Module.train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.train_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.train_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.training_step"><code class="docutils literal notranslate"><span class="pre">Module.training_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.transfer_batch_to_device"><code class="docutils literal notranslate"><span class="pre">Module.transfer_batch_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.type"><code class="docutils literal notranslate"><span class="pre">Module.type()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.unfreeze"><code class="docutils literal notranslate"><span class="pre">Module.unfreeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.untoggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.untoggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.val_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.val_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.validation_step"><code class="docutils literal notranslate"><span class="pre">Module.validation_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.xpu"><code class="docutils literal notranslate"><span class="pre">Module.xpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.module.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.run">stable_pretraining.run module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.run.main"><code class="docutils literal notranslate"><span class="pre">main()</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.run.print_config"><code class="docutils literal notranslate"><span class="pre">print_config()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining.static">stable_pretraining.static module</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic"><code class="docutils literal notranslate"><span class="pre">MetaStatic</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.clear"><code class="docutils literal notranslate"><span class="pre">MetaStatic.clear()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.get"><code class="docutils literal notranslate"><span class="pre">MetaStatic.get()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.items"><code class="docutils literal notranslate"><span class="pre">MetaStatic.items()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.keys"><code class="docutils literal notranslate"><span class="pre">MetaStatic.keys()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.update"><code class="docutils literal notranslate"><span class="pre">MetaStatic.update()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.MetaStatic.values"><code class="docutils literal notranslate"><span class="pre">MetaStatic.values()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_EMBEDDINGS"><code class="docutils literal notranslate"><span class="pre">TIMM_EMBEDDINGS</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_EMBEDDINGS.data"><code class="docutils literal notranslate"><span class="pre">TIMM_EMBEDDINGS.data</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_PARAMETERS"><code class="docutils literal notranslate"><span class="pre">TIMM_PARAMETERS</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.TIMM_PARAMETERS.data"><code class="docutils literal notranslate"><span class="pre">TIMM_PARAMETERS.data</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.static.count_parameters"><code class="docutils literal notranslate"><span class="pre">count_parameters()</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#module-stable_pretraining">Module contents</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.EarlyStopping"><code class="docutils literal notranslate"><span class="pre">EarlyStopping</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.EarlyStopping.should_stop"><code class="docutils literal notranslate"><span class="pre">EarlyStopping.should_stop()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.NAME"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.NAME</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ImageRetrieval.state_key"><code class="docutils literal notranslate"><span class="pre">ImageRetrieval.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR"><code class="docutils literal notranslate"><span class="pre">LiDAR</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">LiDAR.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.setup"><code class="docutils literal notranslate"><span class="pre">LiDAR.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LiDAR.state_key"><code class="docutils literal notranslate"><span class="pre">LiDAR.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LoggingCallback"><code class="docutils literal notranslate"><span class="pre">LoggingCallback</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.LoggingCallback.on_validation_end"><code class="docutils literal notranslate"><span class="pre">LoggingCallback.on_validation_end()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager"><code class="docutils literal notranslate"><span class="pre">Manager</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.init_and_sync_wandb"><code class="docutils literal notranslate"><span class="pre">Manager.init_and_sync_wandb()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.instantiated_data"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_data</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.instantiated_module"><code class="docutils literal notranslate"><span class="pre">Manager.instantiated_module</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.predict"><code class="docutils literal notranslate"><span class="pre">Manager.predict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Manager.save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.test"><code class="docutils literal notranslate"><span class="pre">Manager.test()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Manager.validate"><code class="docutils literal notranslate"><span class="pre">Manager.validate()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module"><code class="docutils literal notranslate"><span class="pre">Module</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.add_module"><code class="docutils literal notranslate"><span class="pre">Module.add_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.all_gather"><code class="docutils literal notranslate"><span class="pre">Module.all_gather()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.apply"><code class="docutils literal notranslate"><span class="pre">Module.apply()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.backward"><code class="docutils literal notranslate"><span class="pre">Module.backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.bfloat16"><code class="docutils literal notranslate"><span class="pre">Module.bfloat16()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.buffers"><code class="docutils literal notranslate"><span class="pre">Module.buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.children"><code class="docutils literal notranslate"><span class="pre">Module.children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.clip_gradients"><code class="docutils literal notranslate"><span class="pre">Module.clip_gradients()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.compile"><code class="docutils literal notranslate"><span class="pre">Module.compile()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_callbacks"><code class="docutils literal notranslate"><span class="pre">Module.configure_callbacks()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_gradient_clipping"><code class="docutils literal notranslate"><span class="pre">Module.configure_gradient_clipping()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_optimizers"><code class="docutils literal notranslate"><span class="pre">Module.configure_optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.configure_sharded_model"><code class="docutils literal notranslate"><span class="pre">Module.configure_sharded_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.cpu"><code class="docutils literal notranslate"><span class="pre">Module.cpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.cuda"><code class="docutils literal notranslate"><span class="pre">Module.cuda()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.double"><code class="docutils literal notranslate"><span class="pre">Module.double()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.eval"><code class="docutils literal notranslate"><span class="pre">Module.eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.extra_repr"><code class="docutils literal notranslate"><span class="pre">Module.extra_repr()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.float"><code class="docutils literal notranslate"><span class="pre">Module.float()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.forward"><code class="docutils literal notranslate"><span class="pre">Module.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.freeze"><code class="docutils literal notranslate"><span class="pre">Module.freeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_buffer"><code class="docutils literal notranslate"><span class="pre">Module.get_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.get_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_parameter"><code class="docutils literal notranslate"><span class="pre">Module.get_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.get_submodule"><code class="docutils literal notranslate"><span class="pre">Module.get_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.half"><code class="docutils literal notranslate"><span class="pre">Module.half()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.ipu"><code class="docutils literal notranslate"><span class="pre">Module.ipu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.load_from_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.load_from_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.load_state_dict"><code class="docutils literal notranslate"><span class="pre">Module.load_state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.log"><code class="docutils literal notranslate"><span class="pre">Module.log()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.log_dict"><code class="docutils literal notranslate"><span class="pre">Module.log_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.lr_scheduler_step"><code class="docutils literal notranslate"><span class="pre">Module.lr_scheduler_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.lr_schedulers"><code class="docutils literal notranslate"><span class="pre">Module.lr_schedulers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.manual_backward"><code class="docutils literal notranslate"><span class="pre">Module.manual_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.modules"><code class="docutils literal notranslate"><span class="pre">Module.modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.mtia"><code class="docutils literal notranslate"><span class="pre">Module.mtia()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_buffers"><code class="docutils literal notranslate"><span class="pre">Module.named_buffers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_children"><code class="docutils literal notranslate"><span class="pre">Module.named_children()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_modules"><code class="docutils literal notranslate"><span class="pre">Module.named_modules()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.named_parameters"><code class="docutils literal notranslate"><span class="pre">Module.named_parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_after_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_after_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_after_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_backward"><code class="docutils literal notranslate"><span class="pre">Module.on_before_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_batch_transfer"><code class="docutils literal notranslate"><span class="pre">Module.on_before_batch_transfer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.on_before_optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_before_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_before_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_fit_end"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_fit_start"><code class="docutils literal notranslate"><span class="pre">Module.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_predict_start"><code class="docutils literal notranslate"><span class="pre">Module.on_predict_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">Module.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_test_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_test_start"><code class="docutils literal notranslate"><span class="pre">Module.on_test_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_train_start"><code class="docutils literal notranslate"><span class="pre">Module.on_train_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_batch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_batch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_epoch_end"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_epoch_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_epoch_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_eval"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_eval()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_train"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_model_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_model_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.on_validation_start"><code class="docutils literal notranslate"><span class="pre">Module.on_validation_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizer_step"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizer_zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.optimizer_zero_grad()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.optimizers"><code class="docutils literal notranslate"><span class="pre">Module.optimizers()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.parameters"><code class="docutils literal notranslate"><span class="pre">Module.parameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.predict_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.predict_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.predict_step"><code class="docutils literal notranslate"><span class="pre">Module.predict_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.prepare_data"><code class="docutils literal notranslate"><span class="pre">Module.prepare_data()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.print"><code class="docutils literal notranslate"><span class="pre">Module.print()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_buffer"><code class="docutils literal notranslate"><span class="pre">Module.register_buffer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_forward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_forward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_forward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_full_backward_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_full_backward_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_full_backward_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_load_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_load_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_load_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_module"><code class="docutils literal notranslate"><span class="pre">Module.register_module()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_parameter"><code class="docutils literal notranslate"><span class="pre">Module.register_parameter()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_state_dict_post_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_post_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.register_state_dict_pre_hook"><code class="docutils literal notranslate"><span class="pre">Module.register_state_dict_pre_hook()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.requires_grad_"><code class="docutils literal notranslate"><span class="pre">Module.requires_grad_()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.rescale_loss_for_grad_acc"><code class="docutils literal notranslate"><span class="pre">Module.rescale_loss_for_grad_acc()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.save_hyperparameters"><code class="docutils literal notranslate"><span class="pre">Module.save_hyperparameters()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.set_extra_state"><code class="docutils literal notranslate"><span class="pre">Module.set_extra_state()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.set_submodule"><code class="docutils literal notranslate"><span class="pre">Module.set_submodule()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.setup"><code class="docutils literal notranslate"><span class="pre">Module.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.share_memory"><code class="docutils literal notranslate"><span class="pre">Module.share_memory()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.state_dict"><code class="docutils literal notranslate"><span class="pre">Module.state_dict()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.teardown"><code class="docutils literal notranslate"><span class="pre">Module.teardown()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.test_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.test_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.test_step"><code class="docutils literal notranslate"><span class="pre">Module.test_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to"><code class="docutils literal notranslate"><span class="pre">Module.to()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_empty"><code class="docutils literal notranslate"><span class="pre">Module.to_empty()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_onnx"><code class="docutils literal notranslate"><span class="pre">Module.to_onnx()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.to_torchscript"><code class="docutils literal notranslate"><span class="pre">Module.to_torchscript()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.toggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.toggled_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.toggled_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.train"><code class="docutils literal notranslate"><span class="pre">Module.train()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.train_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.train_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.training_step"><code class="docutils literal notranslate"><span class="pre">Module.training_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.transfer_batch_to_device"><code class="docutils literal notranslate"><span class="pre">Module.transfer_batch_to_device()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.type"><code class="docutils literal notranslate"><span class="pre">Module.type()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.unfreeze"><code class="docutils literal notranslate"><span class="pre">Module.unfreeze()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.untoggle_optimizer"><code class="docutils literal notranslate"><span class="pre">Module.untoggle_optimizer()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.val_dataloader"><code class="docutils literal notranslate"><span class="pre">Module.val_dataloader()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.validation_step"><code class="docutils literal notranslate"><span class="pre">Module.validation_step()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.xpu"><code class="docutils literal notranslate"><span class="pre">Module.xpu()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.Module.zero_grad"><code class="docutils literal notranslate"><span class="pre">Module.zero_grad()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ModuleSummary"><code class="docutils literal notranslate"><span class="pre">ModuleSummary</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.ModuleSummary.setup"><code class="docutils literal notranslate"><span class="pre">ModuleSummary.setup()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN"><code class="docutils literal notranslate"><span class="pre">OnlineKNN</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.setup"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineKNN.state_key"><code class="docutils literal notranslate"><span class="pre">OnlineKNN.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe"><code class="docutils literal notranslate"><span class="pre">OnlineProbe</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe.configure_model"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.configure_model()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineProbe.wrap_forward"><code class="docutils literal notranslate"><span class="pre">OnlineProbe.wrap_forward()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter"><code class="docutils literal notranslate"><span class="pre">OnlineWriter</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.dump"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.dump()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.is_writing_epoch"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.is_writing_epoch()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_predict_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_predict_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_sanity_check_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_sanity_check_start"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_sanity_check_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_test_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_test_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_train_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.OnlineWriter.write_at_phase"><code class="docutils literal notranslate"><span class="pre">OnlineWriter.write_at_phase()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe"><code class="docutils literal notranslate"><span class="pre">RankMe</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.on_validation_batch_end"><code class="docutils literal notranslate"><span class="pre">RankMe.on_validation_batch_end()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.setup"><code class="docutils literal notranslate"><span class="pre">RankMe.setup()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.RankMe.state_key"><code class="docutils literal notranslate"><span class="pre">RankMe.state_key</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.on_load_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_load_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.on_save_checkpoint"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.on_save_checkpoint()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.SklearnCheckpoint.setup"><code class="docutils literal notranslate"><span class="pre">SklearnCheckpoint.setup()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_after_backward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_after_backward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_fit_start"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_fit_start()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentCallback.on_train_batch_end"><code class="docutils literal notranslate"><span class="pre">TeacherStudentCallback.on_train_batch_end()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward_student"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_student()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.forward_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.forward_teacher()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.update_ema_coefficient"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_ema_coefficient()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TeacherStudentWrapper.update_teacher"><code class="docutils literal notranslate"><span class="pre">TeacherStudentWrapper.update_teacher()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TrainerInfo"><code class="docutils literal notranslate"><span class="pre">TrainerInfo</span></code></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#stable_pretraining.TrainerInfo.setup"><code class="docutils literal notranslate"><span class="pre">TrainerInfo.setup()</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By stable-pretraining team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
       Copyright 2024, stable-pretraining team.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>